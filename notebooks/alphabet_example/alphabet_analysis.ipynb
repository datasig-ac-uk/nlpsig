{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c43e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    RobertaConfig,\n",
    "    RobertaForMaskedLM,\n",
    "    RobertaTokenizerFast,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "import tokenizers\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nlpsig\n",
    "\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from nlpsig import set_seed\n",
    "seed = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5ea439d-1821-4ecc-a88b-e207decae0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5bd9238-bcbc-45c0-9bab-d3d09fbbaf83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d3859",
   "metadata": {},
   "source": [
    "## Language dataset\n",
    "\n",
    "In the `data/` folder, we have several text folders of words from different languages:\n",
    "- `wordlist_de.txt`: German words\n",
    "- `wordlist_en.txt`: English words\n",
    "- `wordlist_fr.txt`: French words\n",
    "- `wordlist_it.txt`: Italian words\n",
    "- `wordlist_pl.txt`: Polish words\n",
    "- `wordlist_sv.txt`: Swedish words\n",
    "\n",
    "We additionally have a `alphabet.txt` file which just stores the alphabet characters ('a', 'b', 'c', ...).\n",
    "\n",
    "The task is to split the words into its individual characters and to obtain an embedding for each of them. We can represent a word by a path of its character embeddings and compute its path signature to use as features in predicting the language for which the word belongs.\n",
    "\n",
    "Here we look at obtaining embeddings using a Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a6aaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "ALPHABET_FILE = 'data/alphabet.txt'\n",
    "with open(ALPHABET_FILE) as f:\n",
    "    alphabet = f.read().splitlines()\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b2270d",
   "metadata": {},
   "source": [
    "## Set up Tokenizer for word corpora\n",
    "\n",
    "If we were to fine-tune an existing pretrained transformer, we could use the same tokenizer that the model was pretrained with. However, here we want to train a model from stratch, and so using a tokenizer that was pretrained on a corpus that looks quite different to ours is suboptimal.\n",
    "\n",
    "Here, we need to use the `tokenizers` library to set up and train a new tokenizer for our text.\n",
    "\n",
    "In particular, we're going to start off with a character-based tokenizer (as we're going to split up our words into characters), and train it to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99c39ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAR_BERT/vocab.json', 'CHAR_BERT/merges.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialise character based tokenizer\n",
    "tokenizer = tokenizers.CharBPETokenizer()\n",
    "tokenizer.train(files=[ALPHABET_FILE],\n",
    "                show_progress=False,\n",
    "                special_tokens=['<s>', '</s>', '<unk>', '<pad>', '<mask>'])\n",
    "\n",
    "# save the tokenizer to \"CHAR_BERT/\" folder\n",
    "if not os.path.exists(\"CHAR_BERT\"):\n",
    "    os.makedirs(\"CHAR_BERT\")\n",
    "\n",
    "tokenizer.save_model(\"CHAR_BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "941eb204",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist_files = [\"data/wordlist_de.txt\",\n",
    "                  \"data/wordlist_en.txt\",\n",
    "                  \"data/wordlist_fr.txt\",\n",
    "                  \"data/wordlist_it.txt\",\n",
    "                  \"data/wordlist_pl.txt\",\n",
    "                  \"data/wordlist_sv.txt\"]\n",
    "\n",
    "wordlist_dfs = []\n",
    "for filename in wordlist_files:\n",
    "    with open(filename, \"r\") as f:\n",
    "        words = f.read().splitlines()\n",
    "        words_df = pd.DataFrame({\"word\": words})\n",
    "        words_df[\"language\"] = filename.split(\"_\")[1][0:2]\n",
    "        wordlist_dfs.append(words_df)\n",
    "\n",
    "corpus_df = pd.concat(wordlist_dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f88916a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aal</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aale</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aalen</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aalend</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922530</th>\n",
       "      <td>zons</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922531</th>\n",
       "      <td>zoo</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922532</th>\n",
       "      <td>zoologisk</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922533</th>\n",
       "      <td>zoologiska</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922534</th>\n",
       "      <td>zoologiskt</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3922535 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               word language\n",
       "0                 a       de\n",
       "1               aal       de\n",
       "2              aale       de\n",
       "3             aalen       de\n",
       "4            aalend       de\n",
       "...             ...      ...\n",
       "3922530        zons       sv\n",
       "3922531         zoo       sv\n",
       "3922532   zoologisk       sv\n",
       "3922533  zoologiska       sv\n",
       "3922534  zoologiskt       sv\n",
       "\n",
       "[3922535 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a7a38f",
   "metadata": {},
   "source": [
    "We can see that there are relatively fewer English words than the other languages..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee3c6f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "it    1862929\n",
       "pl    1517274\n",
       "fr     198538\n",
       "de     186027\n",
       "en      80641\n",
       "sv      77126\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f4c498",
   "metadata": {},
   "source": [
    "We are going to train our language model on the English words, so taking out a sample of English words from the corpus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "066d9241",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train_pickle_file = \"data/english_train.pkl\"\n",
    "if os.path.isfile(english_train_pickle_file):\n",
    "    english_train = pd.read_pickle(english_train_pickle_file)\n",
    "else:    \n",
    "    # set seed for sampling\n",
    "    random.seed(seed)\n",
    "    n_words = 70000\n",
    "    \n",
    "    # sample english words from the corpus\n",
    "    english_train = corpus_df[corpus_df[\"language\"]==\"en\"].sample(n_words)\n",
    "    english_train = english_train.reset_index(drop=True)\n",
    "    \n",
    "    # save data for later\n",
    "    english_train.to_pickle(english_train_pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "739f58df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sensitised</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>signifying</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wholesomeness</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adware</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chasm</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>entourages</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>axe</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>disdained</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>calibers</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>perambulation</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word language\n",
       "0         sensitised       en\n",
       "1         signifying       en\n",
       "2      wholesomeness       en\n",
       "3             adware       en\n",
       "4              chasm       en\n",
       "...              ...      ...\n",
       "69995     entourages       en\n",
       "69996            axe       en\n",
       "69997      disdained       en\n",
       "69998       calibers       en\n",
       "69999  perambulation       en\n",
       "\n",
       "[70000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b0681e",
   "metadata": {},
   "source": [
    "To make the dataset bit more manageable, I'll just take a sample of each of the languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccac4f00-e15d-4e1f-9a84-bc899ef6cea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "it    1860112\n",
       "pl    1513877\n",
       "fr     190682\n",
       "de     185275\n",
       "sv      74994\n",
       "en      10641\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the words that we use to train language model from the corpus\n",
    "cond = corpus_df[\"word\"].isin(english_train[\"word\"])\n",
    "corpus_df = corpus_df.drop(corpus_df[cond].index)\n",
    "corpus_df = corpus_df.reset_index(drop=True)\n",
    "corpus_df[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e60fabc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_sample_pickle_file = \"data/corpus_sample.pkl\"\n",
    "if os.path.isfile(corpus_sample_pickle_file):\n",
    "    corpus_sample_df = pd.read_pickle(corpus_sample_pickle_file)\n",
    "else:\n",
    "    # set seed for sampling\n",
    "    set_seed(seed)\n",
    "    balanced = True\n",
    "\n",
    "    # take a sample from the rest of the remaining words\n",
    "    if balanced:\n",
    "        n_english = 10000\n",
    "        n_remaining = 2000\n",
    "        # sampling non-english words\n",
    "        languages = corpus_df[\"language\"].unique()\n",
    "        words_per_language = math.floor(n_remaining / (len(languages)-1))\n",
    "        non_english_df = pd.concat(\n",
    "            [corpus_df[corpus_df[\"language\"]==lang].sample(words_per_language, random_state=seed)\n",
    "             for lang in languages if lang != \"en\"]\n",
    "        )\n",
    "        # sampling english words\n",
    "        english_df = corpus_df[corpus_df[\"language\"]==\"en\"].sample(n_english, random_state=seed)\n",
    "        corpus_sample_df = pd.concat([non_english_df, english_df]).reset_index(drop=True)\n",
    "    else:\n",
    "        n_words = 12000\n",
    "        corpus_sample_df = corpus_df.iloc[random.sample(range(len(corpus_df)), n_words)]\n",
    "        corpus_sample_df = corpus_sample_df.reset_index(drop=True)\n",
    "\n",
    "    # save data for later\n",
    "    corpus_sample_df.to_pickle(corpus_sample_pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "902cbbe4-9b4b-4237-83e4-57984722f254",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>getrippelte</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ordnungsliebenderes</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vermutetem</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beizumischende</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scholastischer</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>swapping</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>pruners</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>teatimes</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>bonk</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>plummet</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      word language\n",
       "0              getrippelte       de\n",
       "1      ordnungsliebenderes       de\n",
       "2               vermutetem       de\n",
       "3           beizumischende       de\n",
       "4           scholastischer       de\n",
       "...                    ...      ...\n",
       "11995             swapping       en\n",
       "11996              pruners       en\n",
       "11997             teatimes       en\n",
       "11998                 bonk       en\n",
       "11999              plummet       en\n",
       "\n",
       "[12000 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9e53354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    10000\n",
       "de      400\n",
       "fr      400\n",
       "it      400\n",
       "pl      400\n",
       "sv      400\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample_df[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa40272c",
   "metadata": {},
   "source": [
    "## Training a language model\n",
    "\n",
    "We want to train a masked language model for our corpus of English words. In particular, we mask out particular letters and ask our model to try predict the masked letter.\n",
    "\n",
    "Here, we initialise our tokenizer (here we tokenize by character), data collator (with padding) and set up our transformer model by specifying the config (we use the RoBERTa here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5008adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "# load in tokenizer for architecture\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('CHAR_BERT/',\n",
    "                                                 max_len=max_length)\n",
    "\n",
    "# set up data_collator to use (intially just one that adds padding)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# initialise transformer architecture (random weights)\n",
    "config_args = {\"vocab_size\": tokenizer.backend_tokenizer.get_vocab_size(),\n",
    "               \"hidden_size\": 768,\n",
    "               \"max_length\": max_length,\n",
    "               \"max_position_embeddings\": max_length + 2,\n",
    "               \"hidden_dropout_prob\": 0.1,\n",
    "               \"num_attention_heads\": 12,\n",
    "               \"num_hidden_layers\": 6,\n",
    "               \"type_vocab_size\": 1}\n",
    "\n",
    "config = RobertaConfig(**config_args)\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312e8ba",
   "metadata": {},
   "source": [
    "## Using the `TextEncoder` class\n",
    "\n",
    "The `TextEncoder` class in the `nlpsig` package is able to take a dataframe with a column of text. We can use this class to obtain embeddings for the input text, or to train the model with the input text.\n",
    "\n",
    "Note: In the initial writing of the package, the idea was that we can fit our transformer to some data (some text), and then we can obtain embeddings for them. But in our setting, we actually want to fit our data to our sample of English words (which we call `english_train`), but then obtain embeddings for our sample of the remaining words (which we call `corpus_sample_df`) - noting that this also contains some English words.\n",
    "\n",
    "So we will actually use two instances of `TextEncoder` - one to pass in `corpus_df` and train the model. And then another to obtain embeddings for the words in `corpus_sample_df`. This is not optimal and not clean, but some adjustment to `TextEncoder` will be able. In particular, we can perhaps make changes to the `.tokenize_text()` method (which we will see how to use later) which can take in some external text data. \n",
    "\n",
    "I envisage that we can pass in different data to train the model rather than training it on the data that is passed...\n",
    "\n",
    "But for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d79eb814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sensitised</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>signifying</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wholesomeness</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adware</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chasm</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word language\n",
       "0     sensitised       en\n",
       "1     signifying       en\n",
       "2  wholesomeness       en\n",
       "3         adware       en\n",
       "4          chasm       en"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "059975c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = nlpsig.TextEncoder(df=english_train,\n",
    "                                  feature_name=\"word\",\n",
    "                                  model=model,\n",
    "                                  config=config,\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9201959b",
   "metadata": {},
   "source": [
    "We can tokenize the text with the `.tokenize_text()` method, which tokenizes each of the items in the column of the dataframe that we passed in. So in the above, we tokenise the `word` column of the `english_train` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d7c5e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting return_special_tokens_mask=True\n",
      "[INFO] Tokenizing the datatset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:01<00:00, 50.95ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving the tokenized text for each sentence into `.df['tokens']`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:03<00:00, 18.33ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating tokenized dataframe and setting in `.tokenized_df` attribute...\n",
      "[INFO] Note: 'text_id' is the column name for denoting the corresponding text id\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['word', 'language', 'input_ids', 'attention_mask', 'special_tokens_mask', 'tokens'],\n",
       "    num_rows: 70000\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokenize_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3396bd0d",
   "metadata": {},
   "source": [
    "Note that the `text_encoder` object (instance of `TextEncoder`) also keeps the data as a Huggingface Dataset object too which is stored in the `.dataset` attribute of the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec831c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['word', 'language', 'input_ids', 'attention_mask', 'special_tokens_mask', 'tokens'],\n",
       "    num_rows: 70000\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f19f545",
   "metadata": {},
   "source": [
    "We can see that we have tokenized this as there are `input_ids`, `attention_mask`, `special_tokens_mask`, and `tokens` features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57359738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sensitised'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.dataset[\"word\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "376415cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 23, 9, 18, 23, 13, 24, 13, 23, 9, 8, 1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.dataset[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ea34f6",
   "metadata": {},
   "source": [
    "We can see that this word has been tokenized by character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d717a3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'e', 'n', 's', 'i', 't', 'i', 's', 'e', 'd']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.dataset[\"tokens\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d11263",
   "metadata": {},
   "source": [
    "We can also see that we have saved the tokenized text in the `'token'` column of the dataframe stored in `.df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8be65324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sensitised</td>\n",
       "      <td>en</td>\n",
       "      <td>[s, e, n, s, i, t, i, s, e, d]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>signifying</td>\n",
       "      <td>en</td>\n",
       "      <td>[s, i, g, n, i, f, y, i, n, g]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wholesomeness</td>\n",
       "      <td>en</td>\n",
       "      <td>[w, h, o, l, e, s, o, m, e, n, e, s, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adware</td>\n",
       "      <td>en</td>\n",
       "      <td>[a, d, w, a, r, e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chasm</td>\n",
       "      <td>en</td>\n",
       "      <td>[c, h, a, s, m]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>entourages</td>\n",
       "      <td>en</td>\n",
       "      <td>[e, n, t, o, u, r, a, g, e, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>axe</td>\n",
       "      <td>en</td>\n",
       "      <td>[a, x, e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>disdained</td>\n",
       "      <td>en</td>\n",
       "      <td>[d, i, s, d, a, i, n, e, d]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>calibers</td>\n",
       "      <td>en</td>\n",
       "      <td>[c, a, l, i, b, e, r, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>perambulation</td>\n",
       "      <td>en</td>\n",
       "      <td>[p, e, r, a, m, b, u, l, a, t, i, o, n]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word language                                   tokens\n",
       "0         sensitised       en           [s, e, n, s, i, t, i, s, e, d]\n",
       "1         signifying       en           [s, i, g, n, i, f, y, i, n, g]\n",
       "2      wholesomeness       en  [w, h, o, l, e, s, o, m, e, n, e, s, s]\n",
       "3             adware       en                       [a, d, w, a, r, e]\n",
       "4              chasm       en                          [c, h, a, s, m]\n",
       "...              ...      ...                                      ...\n",
       "69995     entourages       en           [e, n, t, o, u, r, a, g, e, s]\n",
       "69996            axe       en                                [a, x, e]\n",
       "69997      disdained       en              [d, i, s, d, a, i, n, e, d]\n",
       "69998       calibers       en                 [c, a, l, i, b, e, r, s]\n",
       "69999  perambulation       en  [p, e, r, a, m, b, u, l, a, t, i, o, n]\n",
       "\n",
       "[70000 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3077c5",
   "metadata": {},
   "source": [
    "We also store the tokens in `.tokens` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e935d45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 70000\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6048cae",
   "metadata": {},
   "source": [
    "After applying the `.tokenize_text()` method, we store a tokenized dataframe in the `.tokenized_df` attribue. Here, we have each token in our corpus and their associated `'text_id'` (which is just the index they were given in the original dataframe that we pass):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d527e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596689</th>\n",
       "      <td>69999</td>\n",
       "      <td>en</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596690</th>\n",
       "      <td>69999</td>\n",
       "      <td>en</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596691</th>\n",
       "      <td>69999</td>\n",
       "      <td>en</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596692</th>\n",
       "      <td>69999</td>\n",
       "      <td>en</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596693</th>\n",
       "      <td>69999</td>\n",
       "      <td>en</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>596694 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id language tokens\n",
       "0             0       en      s\n",
       "1             0       en      e\n",
       "2             0       en      n\n",
       "3             0       en      s\n",
       "4             0       en      i\n",
       "...         ...      ...    ...\n",
       "596689    69999       en      a\n",
       "596690    69999       en      t\n",
       "596691    69999       en      i\n",
       "596692    69999       en      o\n",
       "596693    69999       en      n\n",
       "\n",
       "[596694 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokenized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df9b7e",
   "metadata": {},
   "source": [
    "So if we looked at `text_id==0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a23c2895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_id language tokens\n",
       "0        0       en      s\n",
       "1        0       en      e\n",
       "2        0       en      n\n",
       "3        0       en      s\n",
       "4        0       en      i\n",
       "5        0       en      t\n",
       "6        0       en      i\n",
       "7        0       en      s\n",
       "8        0       en      e\n",
       "9        0       en      d"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokenized_df[text_encoder.tokenized_df[\"text_id\"]==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e147f",
   "metadata": {},
   "source": [
    "If we had passed in a pre-trained model (remember above, we just initialised one with a config and so have random weight), we can obtain token embeddings by the `.obtain_embeddings()` method. \n",
    "\n",
    "There are many ways in which one can get embeddings from the transformer network, as the output is the layers for the full network. A few ways are:\n",
    "\n",
    "- Returning the output of a particular hidden layer\n",
    "    - use `.obtain_embeddings(method = \"hidden_layer\", layers = l)` where `l` is the layer you want\n",
    "    - If no layer is requested, it will just give you the second-to-last hidden layer of the transformer network.\n",
    "- Concatenate the output of several hidden layers\n",
    "    - use `.obtain_embeddings(method = \"concatenate\", layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of layers you want to concatenate\n",
    "- Element-wise sum the output of several hidden layers\n",
    "    - use `.obtain_embeddings(method = \"sum\" , layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of layers you want to sum\n",
    "- Mean the output of several hidden layers\n",
    "    - use `.obtain_embeddings(method = \"mean\" , layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of layers you want to mean\n",
    "\n",
    "If a more custom way to obtain embeddings from the hidden layers, you can specify what layers you want, and it will return them (i.e. using `.obtain_embeddings(method = \"hidden_layer\", layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of hidden layers you want) and so the output will be a 3-dimensional array with dimensions `[layer, token, embedding]` for which you would need to combine in such a way that you would have an embedding for each token. The above methods would return a 2-dimensional array with dimensions `[token, embedding]`.\n",
    "\n",
    "In the below, we just obtain the second-to-last hidden layer of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c068bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/700 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 700/700 [01:18<00:00,  8.89it/s]\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = text_encoder.obtain_embeddings(method = \"hidden_layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd92e0d9",
   "metadata": {},
   "source": [
    "By inspecting the shape of this, we can see that we have a 2-dimensional array with dimensions `[token, embedding]` where the embeddings are 768 dimensional in this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1851492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(596694, 768)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de43a2",
   "metadata": {},
   "source": [
    "Now that we have token embeddings for each text, it is possible to pool these embeddings to obtain an embedding for the full text (for this case, this embedding would represent the word itself. We can use the `.pool_token_embeddings()` method for doing this.\n",
    "\n",
    "Again, there are several methods and full details can be found in the documentation, but a few are:\n",
    "\n",
    "- take the mean of the token embeddings\n",
    "    - use `.pool_token_embeddings(method = \"mean\")`\n",
    "- take the element-wise max of the token embeddings\n",
    "    - use `.pool_token_embeddings(method = \"max\")`\n",
    "- take the element-wise sum of the token embeddings\n",
    "    - use `.pool_token_embeddings(method = \"sum\")`\n",
    "- take the token-embedding for the CLS token\n",
    "    - this is a special token that is used in some transformers like BERT\n",
    "    - but this is only available to us if we set `skip_special_tokens=False` when tokenizing the text (note by default, this is set to `True`)\n",
    "    - use `.pool_token_embeddings(method = \"sum\")`\n",
    "        - this will produce an error if the CLS token is not available..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cffd88db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:27<00:00, 2543.93it/s]\n"
     ]
    }
   ],
   "source": [
    "pooled_embeddings = text_encoder.pool_token_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b07365a",
   "metadata": {},
   "source": [
    "Again, we can inspect the shape and we can see that we have embeddings for each of our words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41e4f4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 768)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927da67c",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "The above embeddings will not be good for any downstream task as the model itself has not been trained to the text. For this we will use other methods in the `TextEncoder` class which allows us to do this by using the Huggingface trainer API.\n",
    "\n",
    "First, we need to set up a data collator for training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5677cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data_collator for language modelling (has dynamic padding)\n",
    "data_collator_for_LM = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "                                                       mlm=True,\n",
    "                                                       mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96e897f",
   "metadata": {},
   "source": [
    "To train our dataset, we will split it into a train, validation and test set with the `.split_dataset()` method.\n",
    "\n",
    "We can set up the trainer's arguments with `.set_up_training_args()` which sets up a `TrainingArguments` object (from the `transformers` package) and stores it in the `.training_args` attribute. And lastly, we set up a `Trainer` object (from the `transformers` package) and store it in the `.trainer` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "490c2393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Splitting up dataset into train / validation / test sets, and saving to `.dataset_split`.\n",
      "[INFO] Setting up TrainingArguments object and saving to `.training_args`.\n",
      "[INFO] Setting up Trainer object, and saving to `.trainer`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x14ece0621a30>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.split_dataset()\n",
    "text_encoder.set_up_training_args(output_dir=\"CHAR_BERT_trained\",\n",
    "                                  num_train_epochs=300,\n",
    "                                  per_device_train_batch_size=128,\n",
    "                                  seed=seed)\n",
    "text_encoder.set_up_trainer(data_collator=data_collator_for_LM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cee6a3",
   "metadata": {},
   "source": [
    "Once everything is set up, we just train our model by calling `.fit_transformer_with_trainer_api()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8e64fcf-4868-4f72-a9ac-20c0e0f2765d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8bbeee74-76d7-446a-ac19-413f9d39d91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e05dd2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/bask/homes/l/lsch4509/.conda/envs/nlpsig/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 56000\n",
      "  Num Epochs = 300\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 131400\n",
      "  Number of trainable parameters = 43560249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training model with 43560249 parameters...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='131400' max='131400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [131400/131400 1:59:15, Epoch 300/300]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.012410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.327600</td>\n",
       "      <td>1.774706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.869500</td>\n",
       "      <td>1.683440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.719600</td>\n",
       "      <td>1.620389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.644500</td>\n",
       "      <td>1.569127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.576000</td>\n",
       "      <td>1.531044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.546200</td>\n",
       "      <td>1.458539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.494000</td>\n",
       "      <td>1.441001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.494000</td>\n",
       "      <td>1.402635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.475300</td>\n",
       "      <td>1.397702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.442300</td>\n",
       "      <td>1.384191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.405300</td>\n",
       "      <td>1.348121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.390700</td>\n",
       "      <td>1.347734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.358300</td>\n",
       "      <td>1.306634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.343700</td>\n",
       "      <td>1.339921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.323700</td>\n",
       "      <td>1.279981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.323700</td>\n",
       "      <td>1.311857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.300700</td>\n",
       "      <td>1.288232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.286400</td>\n",
       "      <td>1.246553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.274800</td>\n",
       "      <td>1.253606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.249200</td>\n",
       "      <td>1.226753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.232300</td>\n",
       "      <td>1.211829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.221900</td>\n",
       "      <td>1.193955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.204600</td>\n",
       "      <td>1.203375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.204600</td>\n",
       "      <td>1.177286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.188100</td>\n",
       "      <td>1.163661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.171800</td>\n",
       "      <td>1.161202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.175500</td>\n",
       "      <td>1.173225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.153300</td>\n",
       "      <td>1.142134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.142500</td>\n",
       "      <td>1.139323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.138900</td>\n",
       "      <td>1.131003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.118000</td>\n",
       "      <td>1.105030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.118000</td>\n",
       "      <td>1.124007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.103400</td>\n",
       "      <td>1.106117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.094900</td>\n",
       "      <td>1.101562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.096700</td>\n",
       "      <td>1.095435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.077400</td>\n",
       "      <td>1.095435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.061700</td>\n",
       "      <td>1.084097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.055100</td>\n",
       "      <td>1.096782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.045600</td>\n",
       "      <td>1.078638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.045600</td>\n",
       "      <td>1.062013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.035900</td>\n",
       "      <td>1.041809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.024400</td>\n",
       "      <td>1.036177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.021100</td>\n",
       "      <td>1.017618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.005000</td>\n",
       "      <td>1.057837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.998600</td>\n",
       "      <td>1.013100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.989200</td>\n",
       "      <td>1.011960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.979500</td>\n",
       "      <td>1.033673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.979500</td>\n",
       "      <td>1.033058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.976500</td>\n",
       "      <td>1.003936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.967600</td>\n",
       "      <td>1.027193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.953200</td>\n",
       "      <td>0.982792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.955400</td>\n",
       "      <td>0.995764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.943400</td>\n",
       "      <td>0.991869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.939700</td>\n",
       "      <td>0.968120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.935400</td>\n",
       "      <td>0.971090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.935400</td>\n",
       "      <td>0.990530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.919000</td>\n",
       "      <td>0.968255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.909000</td>\n",
       "      <td>0.958052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.907700</td>\n",
       "      <td>0.979585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.905300</td>\n",
       "      <td>0.965957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>0.988365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.883600</td>\n",
       "      <td>0.981441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.878600</td>\n",
       "      <td>0.948160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.878600</td>\n",
       "      <td>0.956218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.880100</td>\n",
       "      <td>0.959659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.876800</td>\n",
       "      <td>0.929866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.869800</td>\n",
       "      <td>0.969649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.858800</td>\n",
       "      <td>0.936651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.856100</td>\n",
       "      <td>0.923320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.842900</td>\n",
       "      <td>0.928078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.844400</td>\n",
       "      <td>0.963757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.844400</td>\n",
       "      <td>0.921548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.831400</td>\n",
       "      <td>0.941161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.829600</td>\n",
       "      <td>0.903004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.830100</td>\n",
       "      <td>0.903679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.816600</td>\n",
       "      <td>0.898262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.824200</td>\n",
       "      <td>0.927827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.820300</td>\n",
       "      <td>0.894749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.809300</td>\n",
       "      <td>0.866453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.809300</td>\n",
       "      <td>0.880232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.806700</td>\n",
       "      <td>0.899675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.804700</td>\n",
       "      <td>0.885168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.793900</td>\n",
       "      <td>0.878545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.786500</td>\n",
       "      <td>0.908106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.864457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.777900</td>\n",
       "      <td>0.883980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.770100</td>\n",
       "      <td>0.856428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.770100</td>\n",
       "      <td>0.875970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.778400</td>\n",
       "      <td>0.852612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.763700</td>\n",
       "      <td>0.878221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.763300</td>\n",
       "      <td>0.891282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.756300</td>\n",
       "      <td>0.853291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.758600</td>\n",
       "      <td>0.875660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.756900</td>\n",
       "      <td>0.853085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.865354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.833938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.746800</td>\n",
       "      <td>0.824114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.731900</td>\n",
       "      <td>0.848299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.742200</td>\n",
       "      <td>0.817356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.734400</td>\n",
       "      <td>0.880905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.728800</td>\n",
       "      <td>0.857121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.859695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.722000</td>\n",
       "      <td>0.834944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.722000</td>\n",
       "      <td>0.871172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.724800</td>\n",
       "      <td>0.840965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.724000</td>\n",
       "      <td>0.858981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.707800</td>\n",
       "      <td>0.839926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.713000</td>\n",
       "      <td>0.826415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.699300</td>\n",
       "      <td>0.829360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.704500</td>\n",
       "      <td>0.810368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.702300</td>\n",
       "      <td>0.826326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.702300</td>\n",
       "      <td>0.806566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.694300</td>\n",
       "      <td>0.823191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.684500</td>\n",
       "      <td>0.827294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.805864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.686200</td>\n",
       "      <td>0.825840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.684400</td>\n",
       "      <td>0.825208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.841036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.676300</td>\n",
       "      <td>0.834576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.676300</td>\n",
       "      <td>0.819793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.682000</td>\n",
       "      <td>0.825023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.673400</td>\n",
       "      <td>0.838544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.670100</td>\n",
       "      <td>0.797555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.670900</td>\n",
       "      <td>0.823311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.660100</td>\n",
       "      <td>0.812915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.658300</td>\n",
       "      <td>0.813070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.660700</td>\n",
       "      <td>0.804592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.661900</td>\n",
       "      <td>0.807517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.661900</td>\n",
       "      <td>0.814886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.653000</td>\n",
       "      <td>0.786704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.654900</td>\n",
       "      <td>0.795770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.650800</td>\n",
       "      <td>0.800151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.644100</td>\n",
       "      <td>0.810715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.639500</td>\n",
       "      <td>0.806666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.636200</td>\n",
       "      <td>0.800871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.639300</td>\n",
       "      <td>0.783825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.639300</td>\n",
       "      <td>0.805148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.633900</td>\n",
       "      <td>0.803854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.631100</td>\n",
       "      <td>0.804640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.627000</td>\n",
       "      <td>0.809673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.626600</td>\n",
       "      <td>0.785544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.626100</td>\n",
       "      <td>0.767482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.628400</td>\n",
       "      <td>0.798354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.622900</td>\n",
       "      <td>0.810946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.622900</td>\n",
       "      <td>0.768865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.614700</td>\n",
       "      <td>0.784588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.611200</td>\n",
       "      <td>0.777013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.608300</td>\n",
       "      <td>0.769956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.612100</td>\n",
       "      <td>0.777283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.612000</td>\n",
       "      <td>0.788817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.608400</td>\n",
       "      <td>0.786280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.606700</td>\n",
       "      <td>0.802138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.606700</td>\n",
       "      <td>0.771542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.605400</td>\n",
       "      <td>0.779534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.595400</td>\n",
       "      <td>0.749574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.597000</td>\n",
       "      <td>0.784862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.601000</td>\n",
       "      <td>0.777359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.590200</td>\n",
       "      <td>0.778019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.595200</td>\n",
       "      <td>0.797649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.596500</td>\n",
       "      <td>0.788037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.596500</td>\n",
       "      <td>0.787933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.598400</td>\n",
       "      <td>0.783778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.588200</td>\n",
       "      <td>0.755815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.578900</td>\n",
       "      <td>0.794217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.583500</td>\n",
       "      <td>0.767667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.581300</td>\n",
       "      <td>0.768181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.577700</td>\n",
       "      <td>0.731959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.575200</td>\n",
       "      <td>0.748760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.575200</td>\n",
       "      <td>0.804682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.572200</td>\n",
       "      <td>0.761424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.573600</td>\n",
       "      <td>0.761336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.561800</td>\n",
       "      <td>0.752105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.562400</td>\n",
       "      <td>0.780878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.561800</td>\n",
       "      <td>0.779602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.569900</td>\n",
       "      <td>0.766704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.561100</td>\n",
       "      <td>0.744590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.561100</td>\n",
       "      <td>0.771017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.564900</td>\n",
       "      <td>0.795673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.555600</td>\n",
       "      <td>0.775241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.566900</td>\n",
       "      <td>0.784991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.561300</td>\n",
       "      <td>0.777566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.553300</td>\n",
       "      <td>0.750027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.557400</td>\n",
       "      <td>0.754693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.548000</td>\n",
       "      <td>0.778467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.548000</td>\n",
       "      <td>0.742019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.761893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.553200</td>\n",
       "      <td>0.748323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.543500</td>\n",
       "      <td>0.754222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.548400</td>\n",
       "      <td>0.739748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.546300</td>\n",
       "      <td>0.735456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.553200</td>\n",
       "      <td>0.746911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.535200</td>\n",
       "      <td>0.782357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.535200</td>\n",
       "      <td>0.744877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.535800</td>\n",
       "      <td>0.758554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.536000</td>\n",
       "      <td>0.738942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.731199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.534600</td>\n",
       "      <td>0.751649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.523900</td>\n",
       "      <td>0.755775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.520200</td>\n",
       "      <td>0.774683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.531400</td>\n",
       "      <td>0.750773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.531400</td>\n",
       "      <td>0.753358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.524500</td>\n",
       "      <td>0.724660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.531300</td>\n",
       "      <td>0.721947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.527300</td>\n",
       "      <td>0.760642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.518200</td>\n",
       "      <td>0.776464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.525300</td>\n",
       "      <td>0.725061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.526700</td>\n",
       "      <td>0.750031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.525800</td>\n",
       "      <td>0.755670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.525800</td>\n",
       "      <td>0.753846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.526100</td>\n",
       "      <td>0.737844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.522000</td>\n",
       "      <td>0.793119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.520300</td>\n",
       "      <td>0.728664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.516900</td>\n",
       "      <td>0.769858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.518600</td>\n",
       "      <td>0.725087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.517700</td>\n",
       "      <td>0.760054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.520700</td>\n",
       "      <td>0.740851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.520700</td>\n",
       "      <td>0.718720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.516400</td>\n",
       "      <td>0.723336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.510200</td>\n",
       "      <td>0.732056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.505900</td>\n",
       "      <td>0.745067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.503600</td>\n",
       "      <td>0.751040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.509500</td>\n",
       "      <td>0.727704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.507600</td>\n",
       "      <td>0.739891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.498300</td>\n",
       "      <td>0.729811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.498300</td>\n",
       "      <td>0.691026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.498400</td>\n",
       "      <td>0.725773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.499700</td>\n",
       "      <td>0.717148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.497800</td>\n",
       "      <td>0.727575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.503800</td>\n",
       "      <td>0.740977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.496200</td>\n",
       "      <td>0.711620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.488100</td>\n",
       "      <td>0.740603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.507100</td>\n",
       "      <td>0.735025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.507100</td>\n",
       "      <td>0.721097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.491200</td>\n",
       "      <td>0.731469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.492200</td>\n",
       "      <td>0.716993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.495700</td>\n",
       "      <td>0.734973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.486700</td>\n",
       "      <td>0.705106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.485100</td>\n",
       "      <td>0.753311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.492400</td>\n",
       "      <td>0.786287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>0.747628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>0.725181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.482000</td>\n",
       "      <td>0.762523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.491800</td>\n",
       "      <td>0.767602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.484600</td>\n",
       "      <td>0.741028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.472400</td>\n",
       "      <td>0.731808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.486200</td>\n",
       "      <td>0.730530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.487900</td>\n",
       "      <td>0.719217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.486100</td>\n",
       "      <td>0.714593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.483600</td>\n",
       "      <td>0.739802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.483600</td>\n",
       "      <td>0.748515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.479300</td>\n",
       "      <td>0.721689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.479800</td>\n",
       "      <td>0.708643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.471300</td>\n",
       "      <td>0.713833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.484600</td>\n",
       "      <td>0.716635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.470800</td>\n",
       "      <td>0.764678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>0.478400</td>\n",
       "      <td>0.714442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>0.478000</td>\n",
       "      <td>0.746063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>0.478000</td>\n",
       "      <td>0.702093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.469900</td>\n",
       "      <td>0.720005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.701944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>0.469200</td>\n",
       "      <td>0.722403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>0.478100</td>\n",
       "      <td>0.751887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.465600</td>\n",
       "      <td>0.724469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.467900</td>\n",
       "      <td>0.714752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>0.464300</td>\n",
       "      <td>0.708309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>0.464300</td>\n",
       "      <td>0.761410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.471900</td>\n",
       "      <td>0.736204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>0.471100</td>\n",
       "      <td>0.756455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.465700</td>\n",
       "      <td>0.733976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>0.462600</td>\n",
       "      <td>0.737599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.467900</td>\n",
       "      <td>0.716531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.474500</td>\n",
       "      <td>0.755128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>0.463500</td>\n",
       "      <td>0.708236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.463500</td>\n",
       "      <td>0.757348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.459100</td>\n",
       "      <td>0.712973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.465600</td>\n",
       "      <td>0.709909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>0.457100</td>\n",
       "      <td>0.752733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>0.470700</td>\n",
       "      <td>0.720914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.455000</td>\n",
       "      <td>0.745305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>0.461900</td>\n",
       "      <td>0.697208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>0.467200</td>\n",
       "      <td>0.706845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.467200</td>\n",
       "      <td>0.739224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>0.725703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.456900</td>\n",
       "      <td>0.702913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.712200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>0.456600</td>\n",
       "      <td>0.738895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.457900</td>\n",
       "      <td>0.692056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>0.458700</td>\n",
       "      <td>0.709872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.456300</td>\n",
       "      <td>0.705061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>0.456300</td>\n",
       "      <td>0.735962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>0.451700</td>\n",
       "      <td>0.733084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>0.723806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>0.457600</td>\n",
       "      <td>0.724105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.460800</td>\n",
       "      <td>0.699730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.453800</td>\n",
       "      <td>0.734357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>0.451800</td>\n",
       "      <td>0.709971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>0.449100</td>\n",
       "      <td>0.731839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>0.449100</td>\n",
       "      <td>0.727432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.448900</td>\n",
       "      <td>0.726188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-1000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-1000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-1500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-1500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-2000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-2000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-2500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-2500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-3000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-3000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-3500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-3500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-4000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-4000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-4500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-4500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-5000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-5000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-5500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-5500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-5500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-6000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-6000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-6500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-6500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-6500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-7000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-7000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-7000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-7500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-7500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-7500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-8000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-8000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-8000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-8500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-8500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-8500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-9000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-9000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-9000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-9500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-9500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-9500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-10000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-10000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-10000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-10500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-10500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-10500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-11000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-11000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-11000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-11500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-11500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-11500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-12000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-12000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-12000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-12500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-12500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-12500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-13000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-13000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-13000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-13500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-13500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-13500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-14000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-14000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-14000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-14500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-14500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-14500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-15000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-15000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-15000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-15500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-15500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-15500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-16000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-16000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-16000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-16500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-16500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-16500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-17000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-17000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-17000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-17500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-17500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-17500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-18000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-18000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-18000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-18500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-18500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-18500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-19000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-19000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-19000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-19500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-19500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-19500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-20000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-20000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-20000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-20500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-20500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-20500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-21000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-21000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-21000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-21500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-21500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-21500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-22000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-22000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-22000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-22500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-22500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-22500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-23000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-23000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-23000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-23500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-23500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-23500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-24000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-24000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-24000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-24500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-24500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-24500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-25000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-25000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-25000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-25500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-25500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-25500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-25500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-26000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-26000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-26000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-26500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-26500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-26500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-26500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-27000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-27000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-27000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-27500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-27500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-27500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-28000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-28000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-28000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-28500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-28500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-28500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-29000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-29000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-29000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-29500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-29500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-29500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-30000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-30000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-30000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-30500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-30500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-30500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-31000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-31000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-31000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-31500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-31500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-31500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-32000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-32000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-32000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-32500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-32500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-32500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-32500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-33000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-33000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-33000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-33500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-33500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-33500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-33500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-33500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-34000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-34000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-34000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-34500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-34500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-34500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-34500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-34500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-35000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-35000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-35000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-35500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-35500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-35500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-35500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-35500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-36000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-36000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-36000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-36000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-36500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-36500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-36500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-36500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-36500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-37000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-37000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-37000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-37000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-37000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-37500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-37500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-37500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-37500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-37500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-38000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-38000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-38000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-38000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-38000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-38500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-38500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-38500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-38500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-38500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-39000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-39000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-39000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-39000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-39000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-39500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-39500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-39500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-39500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-39500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-40000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-40000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-40000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-40000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-40000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-40500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-40500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-40500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-40500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-40500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-41000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-41000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-41000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-41000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-41000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-41500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-41500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-41500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-41500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-41500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-42000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-42000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-42000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-42000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-42000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-42500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-42500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-42500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-42500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-42500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-43000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-43000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-43000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-43000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-43000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-43500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-43500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-43500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-43500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-43500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-44000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-44000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-44000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-44000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-44000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-44500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-44500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-44500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-44500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-44500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-45000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-45000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-45000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-45000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-45000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-45500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-45500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-45500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-45500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-45500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-46000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-46000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-46000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-46000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-46000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-46500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-46500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-46500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-46500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-46500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-47000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-47000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-47000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-47000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-47000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-47500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-47500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-47500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-47500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-47500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-48000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-48000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-48000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-48000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-48000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-48500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-48500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-48500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-48500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-48500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-49000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-49000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-49000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-49000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-49000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-49500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-49500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-49500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-49500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-49500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-50000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-50000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-50000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-50000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-50000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-50500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-50500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-50500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-50500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-50500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-51000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-51000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-51000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-51000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-51000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-51500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-51500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-51500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-51500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-51500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-52000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-52000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-52000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-52000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-52000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-52500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-52500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-52500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-52500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-52500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-53000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-53000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-53000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-53000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-53000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-53500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-53500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-53500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-53500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-53500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-54000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-54000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-54000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-54000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-54000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-54500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-54500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-54500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-54500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-54500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-55000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-55000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-55000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-55000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-55000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-55500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-55500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-55500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-55500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-55500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-56000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-56000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-56000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-56000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-56000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-56500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-56500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-56500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-56500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-56500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-57000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-57000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-57000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-57000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-57000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-57500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-57500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-57500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-57500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-57500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-58000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-58000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-58000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-58000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-58000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-58500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-58500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-58500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-58500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-58500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-59000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-59000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-59000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-59000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-59000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-59500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-59500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-59500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-59500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-59500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-60000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-60000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-60000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-60000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-60000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-60500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-60500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-60500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-60500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-60500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-61000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-61000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-61000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-61000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-61000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-61500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-61500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-61500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-61500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-61500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-62000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-62000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-62000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-62000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-62000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-62500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-62500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-62500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-62500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-62500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-63000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-63000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-63000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-63000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-63000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-63500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-63500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-63500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-63500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-63500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-64000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-64000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-64000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-64000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-64000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-64500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-64500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-64500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-64500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-64500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-65000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-65000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-65000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-65000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-65000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-65500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-65500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-65500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-65500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-65500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-66000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-66000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-66000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-66000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-66000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-66500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-66500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-66500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-66500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-66500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-67000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-67000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-67000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-67000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-67000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-67500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-67500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-67500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-67500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-67500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-68000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-68000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-68000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-68000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-68000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-68500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-68500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-68500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-68500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-68500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-69000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-69000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-69000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-69000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-69000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-69500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-69500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-69500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-69500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-69500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-70000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-70000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-70000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-70000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-70000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-70500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-70500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-70500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-70500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-70500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-71000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-71000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-71000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-71000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-71000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-71500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-71500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-71500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-71500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-71500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-72000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-72000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-72000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-72000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-72000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-72500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-72500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-72500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-72500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-72500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-73000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-73000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-73000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-73000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-73000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-73500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-73500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-73500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-73500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-73500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-74000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-74000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-74000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-74000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-74000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-74500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-74500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-74500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-74500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-74500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-75000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-75000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-75000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-75000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-75000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-75500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-75500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-75500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-75500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-75500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-76000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-76000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-76000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-76000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-76000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-76500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-76500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-76500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-76500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-76500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-77000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-77000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-77000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-77000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-77000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-77500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-77500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-77500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-77500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-77500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-78000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-78000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-78000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-78000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-78000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-78500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-78500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-78500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-78500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-78500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-79000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-79000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-79000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-79000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-79000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-79500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-79500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-79500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-79500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-79500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-80000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-80000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-80000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-80000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-80000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-80500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-80500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-80500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-80500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-80500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-81000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-81000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-81000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-81000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-81000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-81500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-81500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-81500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-81500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-81500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-82000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-82000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-82000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-82000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-82000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-82500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-82500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-82500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-82500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-82500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-83000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-83000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-83000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-83000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-83000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-83500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-83500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-83500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-83500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-83500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-84000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-84000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-84000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-84000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-84000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-84500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-84500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-84500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-84500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-84500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-85000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-85000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-85000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-85000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-85000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-85500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-85500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-85500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-85500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-85500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-86000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-86000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-86000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-86000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-86000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-86500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-86500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-86500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-86500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-86500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-87000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-87000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-87000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-87000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-87000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-87500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-87500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-87500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-87500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-87500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-88000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-88000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-88000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-88000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-88000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-88500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-88500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-88500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-88500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-88500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-89000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-89000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-89000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-89000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-89000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-89500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-89500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-89500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-89500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-89500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-90000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-90000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-90000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-90000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-90000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-90500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-90500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-90500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-90500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-90500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-91000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-91000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-91000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-91000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-91000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-91500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-91500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-91500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-91500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-91500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-92000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-92000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-92000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-92000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-92000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-92500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-92500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-92500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-92500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-92500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-93000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-93000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-93000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-93000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-93000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-93500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-93500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-93500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-93500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-93500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-94000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-94000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-94000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-94000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-94000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-94500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-94500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-94500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-94500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-94500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-95000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-95000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-95000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-95000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-95000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-95500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-95500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-95500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-95500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-95500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-96000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-96000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-96000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-96000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-96000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-96500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-96500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-96500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-96500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-96500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-97000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-97000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-97000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-97000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-97000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-97500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-97500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-97500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-97500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-97500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-98000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-98000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-98000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-98000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-98000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-98500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-98500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-98500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-98500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-98500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-99000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-99000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-99000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-99000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-99000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-99500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-99500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-99500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-99500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-99500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-100000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-100000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-100000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-100000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-100000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-100500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-100500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-100500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-100500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-100500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-101000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-101000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-101000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-101000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-101000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-101500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-101500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-101500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-101500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-101500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-102000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-102000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-102000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-102000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-102000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-102500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-102500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-102500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-102500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-102500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-103000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-103000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-103000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-103000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-103000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-103500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-103500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-103500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-103500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-103500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-104000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-104000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-104000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-104000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-104000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-104500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-104500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-104500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-104500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-104500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-105000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-105000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-105000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-105000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-105000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-105500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-105500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-105500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-105500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-105500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-106000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-106000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-106000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-106000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-106000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-106500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-106500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-106500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-106500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-106500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-107000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-107000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-107000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-107000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-107000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-107500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-107500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-107500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-107500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-107500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-108000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-108000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-108000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-108000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-108000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-108500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-108500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-108500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-108500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-108500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-109000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-109000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-109000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-109000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-109000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-109500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-109500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-109500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-109500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-109500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-110000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-110000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-110000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-110000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-110000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-110500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-110500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-110500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-110500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-110500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-111000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-111000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-111000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-111000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-111000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-111500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-111500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-111500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-111500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-111500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-112000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-112000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-112000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-112000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-112000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-112500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-112500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-112500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-112500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-112500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-113000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-113000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-113000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-113000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-113000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-113500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-113500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-113500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-113500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-113500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-114000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-114000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-114000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-114000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-114000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-114500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-114500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-114500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-114500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-114500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-115000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-115000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-115000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-115000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-115000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-115500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-115500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-115500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-115500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-115500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-116000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-116000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-116000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-116000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-116000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-116500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-116500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-116500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-116500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-116500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-117000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-117000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-117000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-117000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-117000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-117500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-117500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-117500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-117500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-117500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-118000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-118000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-118000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-118000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-118000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-118500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-118500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-118500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-118500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-118500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-119000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-119000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-119000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-119000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-119000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-119500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-119500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-119500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-119500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-119500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-120000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-120000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-120000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-120000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-120000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-120500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-120500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-120500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-120500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-120500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-121000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-121000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-121000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-121000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-121000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-121500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-121500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-121500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-121500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-121500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-122000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-122000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-122000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-122000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-122000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-122500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-122500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-122500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-122500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-122500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-123000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-123000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-123000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-123000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-123000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-123500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-123500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-123500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-123500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-123500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-124000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-124000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-124000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-124000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-124000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-124500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-124500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-124500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-124500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-124500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-125000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-125000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-125000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-125000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-125000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-125500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-125500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-125500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-125500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-125500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-126000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-126000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-126000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-126000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-126000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-126500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-126500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-126500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-126500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-126500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-127000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-127000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-127000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-127000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-127000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-127500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-127500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-127500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-127500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-127500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-128000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-128000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-128000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-128000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-128000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-128500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-128500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-128500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-128500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-128500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-129000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-129000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-129000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-129000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-129000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-129500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-129500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-129500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-129500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-129500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-130000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-130000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-130000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-130000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-130000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-130500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-130500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-130500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-130500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-130500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-131000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-131000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-131000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-131000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-131000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: language, tokens, word, special_tokens_mask. If language, tokens, word, special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training completed!\n"
     ]
    }
   ],
   "source": [
    "text_encoder.fit_transformer_with_trainer_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8497e3",
   "metadata": {},
   "source": [
    "Saving our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2330b558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to CHAR_BERT_trained/\n",
      "Configuration saved in CHAR_BERT_trained/config.json\n",
      "Model weights saved in CHAR_BERT_trained/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "text_encoder.trainer.save_model(\"CHAR_BERT_trained/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6978236",
   "metadata": {},
   "source": [
    "## Evaluating trained model\n",
    "\n",
    "Evaluating the performance on predicting the masked letter for the test dataset. To do this, for each word in our test dataset, we will mask each letter on its own and ask the model to predict the masked letter. So for a 5 letter word, we have 5 predictions to make - one for each letter given the other letters.\n",
    "\n",
    "For our tokenizer, we see that `\\<mask>` is used as the mask token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ccfacdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6fdb04bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file CHAR_BERT_trained/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"CHAR_BERT_trained\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 57\n",
      "}\n",
      "\n",
      "loading configuration file CHAR_BERT_trained/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"CHAR_BERT_trained\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 57\n",
      "}\n",
      "\n",
      "loading weights file CHAR_BERT_trained/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at CHAR_BERT_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with 7000 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [14:04<00:00,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8405469243820486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8405469243820486"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_masked_character_accuracy(fill_mask, words):\n",
    "    was_correct = []\n",
    "    print(f\"Evaluating with {len(words)} words\")\n",
    "    for word in tqdm(words):\n",
    "        masked_strings = [word[:i] + '<mask>' + word[i+1:] for i in range(len(word))]\n",
    "        predictions = [fill_mask(word)[0]['sequence'] for word in masked_strings]\n",
    "        was_correct += [pred == word for pred in predictions]\n",
    "    \n",
    "    acc = np.sum(was_correct) / len(was_correct)\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    return acc\n",
    "\n",
    "fill_mask = pipeline(\"fill-mask\",\n",
    "                     model=\"CHAR_BERT_trained\",\n",
    "                     tokenizer=\"CHAR_BERT_trained\")\n",
    "\n",
    "compute_masked_character_accuracy(fill_mask, \n",
    "                                  text_encoder.dataset_split[\"test\"][\"word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1ad55",
   "metadata": {},
   "source": [
    "## Obtaining a path for each word\n",
    "\n",
    "Now that we have trained our model, we want to obtain embeddings for the words in `corpus_df`. Currently, `TextEncoder` only works with the data that is passed into the function and stored in `.df` and `.dataset`, so we need to initialise a new `TextEncoder` object with the `corpus_df` dataframe and also the trained model.\n",
    "\n",
    "We can then obtain embeddings easily (recall from above we first need to tokenize the text, and then use the `.obtain_embeddings()` and `.pool_token_embeddings()` methods to do this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "015309f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_2 = nlpsig.TextEncoder(df=corpus_sample_df,\n",
    "                                    feature_name=\"word\",\n",
    "                                    model=text_encoder.model,\n",
    "                                    config=text_encoder.config,\n",
    "                                    tokenizer=text_encoder.tokenizer,\n",
    "                                    data_collator=text_encoder.data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d0d9042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting return_special_tokens_mask=True\n",
      "[INFO] Tokenizing the datatset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/12 [00:00<?, ?ba/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/120 [05:33<?, ?it/s].20ba/s]\u001b[A\u001b[A\n",
      "100%|██████████| 12/12 [00:00<00:00, 43.05ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving the tokenized text for each sentence into `.df['tokens']`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 23.78ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating tokenized dataframe and setting in `.tokenized_df` attribute...\n",
      "[INFO] Note: 'text_id' is the column name for denoting the corresponding text id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:13<00:00,  8.80it/s]\n"
     ]
    }
   ],
   "source": [
    "text_encoder_2.tokenize_text()\n",
    "token_embeddings = text_encoder_2.obtain_embeddings(method = \"hidden_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ed8f78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108399</th>\n",
       "      <td>11999</td>\n",
       "      <td>en</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108400</th>\n",
       "      <td>11999</td>\n",
       "      <td>en</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108401</th>\n",
       "      <td>11999</td>\n",
       "      <td>en</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108402</th>\n",
       "      <td>11999</td>\n",
       "      <td>en</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108403</th>\n",
       "      <td>11999</td>\n",
       "      <td>en</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108404 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id language tokens\n",
       "0             0       de      g\n",
       "1             0       de      e\n",
       "2             0       de      t\n",
       "3             0       de      r\n",
       "4             0       de      i\n",
       "...         ...      ...    ...\n",
       "108399    11999       en      u\n",
       "108400    11999       en      m\n",
       "108401    11999       en      m\n",
       "108402    11999       en      e\n",
       "108403    11999       en      t\n",
       "\n",
       "[108404 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_2.tokenized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c1885be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108404, 768)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1c6b3152-cdf3-4226-ac49-23587fb2d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus_sample_token_embeddings.pkl','wb') as f:\n",
    "    pickle.dump(token_embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56cf725",
   "metadata": {},
   "source": [
    "## Dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8024d056",
   "metadata": {},
   "source": [
    "We can perform dimension reduction with `nlpsig` using the `DimReduce` class. Here, we will use UMAP (implemented using the [`umap-learn`](https://umap-learn.readthedocs.io/en/latest/api.html) package, but there are other standard methods available:\n",
    "- PCA (implemented using [`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html))\n",
    "    - `method=\"pca\"`\n",
    "- TSNE (implemented using [`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html))\n",
    "    - `method=\"tsne\"`\n",
    "- Post Processing Algorithm (PPA) with PCA (PPA-PCA)\n",
    "    - `method=\"ppapca\"`\n",
    "    - see _Mu, J., Bhat, S., and Viswanath, P. (2017). All-but-the-top: Simple and effective postprocessing for word representations. arXiv preprint arXiv:1702.01417._\n",
    "- PPA-PCA-PPA\n",
    "    - `method=\"ppapacppa\"`\n",
    "    - see _Raunak, V., Gupta, V., and Metze, F. (2019). Effective dimensionality reduction for word embeddings. In Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP- 2019), pages 235–243._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5233fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = nlpsig.DimReduce(method=\"umap\",\n",
    "                             n_components=10,\n",
    "                             dim_reduction_kwargs={\n",
    "                                 \"metric\": \"cosine\",\n",
    "                             })\n",
    "embeddings_reduced = reduction.fit_transform(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1b3853cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108404, 10)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ca02b749-6283-48f4-a3a7-1ef6ee5897c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus_sample_reduced_token_embeddings.pkl','wb') as f:\n",
    "    pickle.dump(embeddings_reduced, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de892785",
   "metadata": {},
   "source": [
    "As we have embeddings for each token, we can obtain a path for each word by constructing a path of the token embeddings. To do this, we can use the `PrepareData` class and pass in our tokenized dataframe (the dataframe where we have each token in our data and we also have the corresponding id for each word which is saved in the `text_id` column of the tokenized dataframe.\n",
    "\n",
    "We pass in the column which defines the ids, `text_id`, the column which defines the labels, `language`, the token embeddings and the pooled embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83bd0e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Note 'datetime' is not a column in `.df`, so only 'timeline_index' is added.\n",
      "[INFO] As 'datetime' is not a column in `.df`, we assume that the data is ordered by time with respect to the id.\n",
      "[INFO] Adding 'timeline_index' feature...\n"
     ]
    }
   ],
   "source": [
    "dataset = nlpsig.PrepareData(text_encoder_2.tokenized_df,\n",
    "                             id_column=\"text_id\",\n",
    "                             labels_column=\"language\",\n",
    "                             embeddings=token_embeddings,\n",
    "                             embeddings_reduced=embeddings_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd9f55",
   "metadata": {},
   "source": [
    "We can construct a path by using the `.pad()` method, and result of this is a multi-dimensional array or tensor (in particular a numpy array or PyTorch tensor) which can be then used in some downstream task. It is called \"pad\" because arrays and tensors are rectangular and if there are cases where there isn't enough data (e.g. if a word only has 3 letters/tokens and we want to make paths of length 4), we \"pad\" with either the last token embedding (set `zero_padding=False`) or with zeros (set `zero_padding=True`).\n",
    "\n",
    "Here, we construct paths by setting a length of the paths (we call this method `k_last` in the code and we have to specify the length with `k=10`). We alternatively can construct to the longest word possible (by setting `method=\"max\"`). The `time_feature` argument allows us to specify what time features we want to keep. Here we don't have any besides the index in which the word is, which is given by `timeline_index` and we choose not to standardise that by specifying `standardise_time_feature=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "acebfe66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12000/12000 [00:15<00:00, 791.47it/s]\n"
     ]
    }
   ],
   "source": [
    "word_path = dataset.pad(pad_by=\"id\",\n",
    "                        zero_padding=True,\n",
    "                        method=\"k_last\",\n",
    "                        k=10,\n",
    "                        time_feature=[\"timeline_index\"],\n",
    "                        standardise_time_feature=False,\n",
    "                        embeddings=\"dim_reduced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746a5e6f",
   "metadata": {},
   "source": [
    "By inspecting the shape of `word_path`, we see that we have a path for each word and the dimension of the array is `[batch, length of path, channels]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99428872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 10, 13)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "523b4cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.df[\"text_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24792e7d",
   "metadata": {},
   "source": [
    "We store this array as a dataframe in `.df_padded` so that you can see what the columns correspond to, where columns beginning with `e` denote the dimensions of embeddings obtained from the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "70089c53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeline_index</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>d10</th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.642436</td>\n",
       "      <td>0.347501</td>\n",
       "      <td>0.633622</td>\n",
       "      <td>9.498485</td>\n",
       "      <td>-0.536742</td>\n",
       "      <td>7.236354</td>\n",
       "      <td>0.349428</td>\n",
       "      <td>8.476377</td>\n",
       "      <td>10.284984</td>\n",
       "      <td>0.212375</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.097917</td>\n",
       "      <td>1.159596</td>\n",
       "      <td>-0.558051</td>\n",
       "      <td>10.146338</td>\n",
       "      <td>0.965994</td>\n",
       "      <td>4.594119</td>\n",
       "      <td>0.839234</td>\n",
       "      <td>1.718462</td>\n",
       "      <td>8.335831</td>\n",
       "      <td>2.011386</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.660696</td>\n",
       "      <td>-1.474226</td>\n",
       "      <td>0.324929</td>\n",
       "      <td>8.996325</td>\n",
       "      <td>0.147167</td>\n",
       "      <td>5.626800</td>\n",
       "      <td>-0.326708</td>\n",
       "      <td>2.957045</td>\n",
       "      <td>7.357754</td>\n",
       "      <td>0.119781</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.066091</td>\n",
       "      <td>0.934841</td>\n",
       "      <td>1.984142</td>\n",
       "      <td>9.897273</td>\n",
       "      <td>-0.445418</td>\n",
       "      <td>5.485973</td>\n",
       "      <td>0.820208</td>\n",
       "      <td>7.064891</td>\n",
       "      <td>10.352209</td>\n",
       "      <td>0.308647</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.018166</td>\n",
       "      <td>2.327596</td>\n",
       "      <td>-0.396088</td>\n",
       "      <td>10.137131</td>\n",
       "      <td>0.667989</td>\n",
       "      <td>3.218513</td>\n",
       "      <td>1.892539</td>\n",
       "      <td>2.235790</td>\n",
       "      <td>8.186775</td>\n",
       "      <td>1.986284</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>5</td>\n",
       "      <td>0.705757</td>\n",
       "      <td>0.505991</td>\n",
       "      <td>0.561136</td>\n",
       "      <td>9.654332</td>\n",
       "      <td>-0.295937</td>\n",
       "      <td>5.772221</td>\n",
       "      <td>0.606102</td>\n",
       "      <td>8.362031</td>\n",
       "      <td>9.257602</td>\n",
       "      <td>0.483077</td>\n",
       "      <td>11999</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>6</td>\n",
       "      <td>0.281582</td>\n",
       "      <td>0.431672</td>\n",
       "      <td>0.039087</td>\n",
       "      <td>9.760633</td>\n",
       "      <td>0.802404</td>\n",
       "      <td>0.440786</td>\n",
       "      <td>1.116959</td>\n",
       "      <td>1.490295</td>\n",
       "      <td>7.960192</td>\n",
       "      <td>0.338932</td>\n",
       "      <td>11999</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11999</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11999</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11999</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        timeline_index        d1        d2        d3         d4        d5  \\\n",
       "0                    1  0.642436  0.347501  0.633622   9.498485 -0.536742   \n",
       "1                    2  1.097917  1.159596 -0.558051  10.146338  0.965994   \n",
       "2                    3  0.660696 -1.474226  0.324929   8.996325  0.147167   \n",
       "3                    4  0.066091  0.934841  1.984142   9.897273 -0.445418   \n",
       "4                    5  0.018166  2.327596 -0.396088  10.137131  0.667989   \n",
       "...                ...       ...       ...       ...        ...       ...   \n",
       "119995               5  0.705757  0.505991  0.561136   9.654332 -0.295937   \n",
       "119996               6  0.281582  0.431672  0.039087   9.760633  0.802404   \n",
       "119997               0  0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "119998               0  0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "119999               0  0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "\n",
       "              d6        d7        d8         d9       d10  text_id language  \n",
       "0       7.236354  0.349428  8.476377  10.284984  0.212375        0       de  \n",
       "1       4.594119  0.839234  1.718462   8.335831  2.011386        0       de  \n",
       "2       5.626800 -0.326708  2.957045   7.357754  0.119781        0       de  \n",
       "3       5.485973  0.820208  7.064891  10.352209  0.308647        0       de  \n",
       "4       3.218513  1.892539  2.235790   8.186775  1.986284        0       de  \n",
       "...          ...       ...       ...        ...       ...      ...      ...  \n",
       "119995  5.772221  0.606102  8.362031   9.257602  0.483077    11999       en  \n",
       "119996  0.440786  1.116959  1.490295   7.960192  0.338932    11999       en  \n",
       "119997  0.000000  0.000000  0.000000   0.000000  0.000000    11999       -1  \n",
       "119998  0.000000  0.000000  0.000000   0.000000  0.000000    11999       -1  \n",
       "119999  0.000000  0.000000  0.000000   0.000000  0.000000    11999       -1  \n",
       "\n",
       "[120000 rows x 13 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b2f343",
   "metadata": {},
   "source": [
    "We can see that the first column corresponds to the index, the columns beginning with `d` correspond to the dimension-reduced embeddings (which were 10 dimensional), and we also have the corresponding text-id and language (which we passed in the label above). If we look at the first word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "69ca9451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeline_index</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>d10</th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.642436</td>\n",
       "      <td>0.347501</td>\n",
       "      <td>0.633622</td>\n",
       "      <td>9.498485</td>\n",
       "      <td>-0.536742</td>\n",
       "      <td>7.236354</td>\n",
       "      <td>0.349428</td>\n",
       "      <td>8.476377</td>\n",
       "      <td>10.284984</td>\n",
       "      <td>0.212375</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.097917</td>\n",
       "      <td>1.159596</td>\n",
       "      <td>-0.558051</td>\n",
       "      <td>10.146338</td>\n",
       "      <td>0.965994</td>\n",
       "      <td>4.594119</td>\n",
       "      <td>0.839234</td>\n",
       "      <td>1.718462</td>\n",
       "      <td>8.335831</td>\n",
       "      <td>2.011386</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.660696</td>\n",
       "      <td>-1.474226</td>\n",
       "      <td>0.324929</td>\n",
       "      <td>8.996325</td>\n",
       "      <td>0.147167</td>\n",
       "      <td>5.626800</td>\n",
       "      <td>-0.326708</td>\n",
       "      <td>2.957045</td>\n",
       "      <td>7.357754</td>\n",
       "      <td>0.119781</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.066091</td>\n",
       "      <td>0.934841</td>\n",
       "      <td>1.984142</td>\n",
       "      <td>9.897273</td>\n",
       "      <td>-0.445418</td>\n",
       "      <td>5.485973</td>\n",
       "      <td>0.820208</td>\n",
       "      <td>7.064891</td>\n",
       "      <td>10.352209</td>\n",
       "      <td>0.308647</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.018166</td>\n",
       "      <td>2.327596</td>\n",
       "      <td>-0.396088</td>\n",
       "      <td>10.137131</td>\n",
       "      <td>0.667989</td>\n",
       "      <td>3.218513</td>\n",
       "      <td>1.892539</td>\n",
       "      <td>2.235790</td>\n",
       "      <td>8.186775</td>\n",
       "      <td>1.986284</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1.022772</td>\n",
       "      <td>-0.541542</td>\n",
       "      <td>0.324480</td>\n",
       "      <td>9.801931</td>\n",
       "      <td>0.200290</td>\n",
       "      <td>5.401011</td>\n",
       "      <td>-0.242706</td>\n",
       "      <td>3.182425</td>\n",
       "      <td>7.249306</td>\n",
       "      <td>1.414376</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.425700</td>\n",
       "      <td>0.360153</td>\n",
       "      <td>0.315016</td>\n",
       "      <td>9.999400</td>\n",
       "      <td>-0.250613</td>\n",
       "      <td>6.296197</td>\n",
       "      <td>0.384874</td>\n",
       "      <td>8.175232</td>\n",
       "      <td>9.863592</td>\n",
       "      <td>0.369476</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.361710</td>\n",
       "      <td>2.222073</td>\n",
       "      <td>-0.221039</td>\n",
       "      <td>10.026139</td>\n",
       "      <td>0.520481</td>\n",
       "      <td>2.371255</td>\n",
       "      <td>2.405247</td>\n",
       "      <td>1.766772</td>\n",
       "      <td>8.619617</td>\n",
       "      <td>1.612400</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1.573137</td>\n",
       "      <td>-0.281566</td>\n",
       "      <td>0.601034</td>\n",
       "      <td>9.571445</td>\n",
       "      <td>1.029437</td>\n",
       "      <td>4.327699</td>\n",
       "      <td>-0.008659</td>\n",
       "      <td>3.540253</td>\n",
       "      <td>6.914563</td>\n",
       "      <td>1.514759</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.497214</td>\n",
       "      <td>-0.226626</td>\n",
       "      <td>-0.582444</td>\n",
       "      <td>9.962260</td>\n",
       "      <td>1.021137</td>\n",
       "      <td>3.804570</td>\n",
       "      <td>-0.194161</td>\n",
       "      <td>9.596270</td>\n",
       "      <td>7.277706</td>\n",
       "      <td>1.990388</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timeline_index        d1        d2        d3         d4        d5  \\\n",
       "0               1  0.642436  0.347501  0.633622   9.498485 -0.536742   \n",
       "1               2  1.097917  1.159596 -0.558051  10.146338  0.965994   \n",
       "2               3  0.660696 -1.474226  0.324929   8.996325  0.147167   \n",
       "3               4  0.066091  0.934841  1.984142   9.897273 -0.445418   \n",
       "4               5  0.018166  2.327596 -0.396088  10.137131  0.667989   \n",
       "5               6  1.022772 -0.541542  0.324480   9.801931  0.200290   \n",
       "6               7  0.425700  0.360153  0.315016   9.999400 -0.250613   \n",
       "7               8 -0.361710  2.222073 -0.221039  10.026139  0.520481   \n",
       "8               9  1.573137 -0.281566  0.601034   9.571445  1.029437   \n",
       "9              10  0.497214 -0.226626 -0.582444   9.962260  1.021137   \n",
       "\n",
       "         d6        d7        d8         d9       d10  text_id language  \n",
       "0  7.236354  0.349428  8.476377  10.284984  0.212375        0       de  \n",
       "1  4.594119  0.839234  1.718462   8.335831  2.011386        0       de  \n",
       "2  5.626800 -0.326708  2.957045   7.357754  0.119781        0       de  \n",
       "3  5.485973  0.820208  7.064891  10.352209  0.308647        0       de  \n",
       "4  3.218513  1.892539  2.235790   8.186775  1.986284        0       de  \n",
       "5  5.401011 -0.242706  3.182425   7.249306  1.414376        0       de  \n",
       "6  6.296197  0.384874  8.175232   9.863592  0.369476        0       de  \n",
       "7  2.371255  2.405247  1.766772   8.619617  1.612400        0       de  \n",
       "8  4.327699 -0.008659  3.540253   6.914563  1.514759        0       de  \n",
       "9  3.804570 -0.194161  9.596270   7.277706  1.990388        0       de  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# still has the labels and the ids\n",
    "dataset.df_padded[dataset.df_padded[\"text_id\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e308ad4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word                              getrippelte\n",
       "language                                   de\n",
       "tokens      [g, e, t, r, i, p, p, e, l, t, e]\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_2.df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922ba045",
   "metadata": {},
   "source": [
    "We pick out a word which has less than 10 letters, and we can see that the path is padded with zeros and we give these a label `-1` to denote that they have been added.\n",
    "\n",
    "Note that for padding, the method pads from below by default, but we can pad by above by setting `pad_from_below=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e63757f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeline_index</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>d10</th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33200</th>\n",
       "      <td>0</td>\n",
       "      <td>1.353214</td>\n",
       "      <td>-0.676448</td>\n",
       "      <td>0.215638</td>\n",
       "      <td>9.932939</td>\n",
       "      <td>0.650689</td>\n",
       "      <td>5.326757</td>\n",
       "      <td>-0.377507</td>\n",
       "      <td>-1.604548</td>\n",
       "      <td>9.317955</td>\n",
       "      <td>3.325151</td>\n",
       "      <td>3320</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33201</th>\n",
       "      <td>1</td>\n",
       "      <td>0.570237</td>\n",
       "      <td>0.459697</td>\n",
       "      <td>0.609359</td>\n",
       "      <td>9.673721</td>\n",
       "      <td>-0.411311</td>\n",
       "      <td>6.861120</td>\n",
       "      <td>0.438183</td>\n",
       "      <td>8.438325</td>\n",
       "      <td>10.143123</td>\n",
       "      <td>0.247765</td>\n",
       "      <td>3320</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33202</th>\n",
       "      <td>2</td>\n",
       "      <td>0.084992</td>\n",
       "      <td>2.184650</td>\n",
       "      <td>0.310398</td>\n",
       "      <td>10.579418</td>\n",
       "      <td>-0.191729</td>\n",
       "      <td>3.764890</td>\n",
       "      <td>1.148623</td>\n",
       "      <td>1.218172</td>\n",
       "      <td>7.312806</td>\n",
       "      <td>1.726927</td>\n",
       "      <td>3320</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33203</th>\n",
       "      <td>3</td>\n",
       "      <td>1.039667</td>\n",
       "      <td>0.449838</td>\n",
       "      <td>0.181571</td>\n",
       "      <td>10.064098</td>\n",
       "      <td>0.280863</td>\n",
       "      <td>4.666007</td>\n",
       "      <td>0.288457</td>\n",
       "      <td>1.372774</td>\n",
       "      <td>7.274780</td>\n",
       "      <td>1.995180</td>\n",
       "      <td>3320</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33204</th>\n",
       "      <td>4</td>\n",
       "      <td>0.555279</td>\n",
       "      <td>1.213909</td>\n",
       "      <td>1.041155</td>\n",
       "      <td>10.354813</td>\n",
       "      <td>-0.680132</td>\n",
       "      <td>5.347591</td>\n",
       "      <td>0.620669</td>\n",
       "      <td>6.782768</td>\n",
       "      <td>9.622807</td>\n",
       "      <td>-1.069551</td>\n",
       "      <td>3320</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33205</th>\n",
       "      <td>5</td>\n",
       "      <td>0.719879</td>\n",
       "      <td>0.543125</td>\n",
       "      <td>0.123861</td>\n",
       "      <td>9.536921</td>\n",
       "      <td>0.349212</td>\n",
       "      <td>3.724797</td>\n",
       "      <td>0.432820</td>\n",
       "      <td>1.072457</td>\n",
       "      <td>6.548460</td>\n",
       "      <td>0.793852</td>\n",
       "      <td>3320</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33206</th>\n",
       "      <td>6</td>\n",
       "      <td>0.633168</td>\n",
       "      <td>1.132934</td>\n",
       "      <td>1.056927</td>\n",
       "      <td>10.242445</td>\n",
       "      <td>-0.677864</td>\n",
       "      <td>5.246167</td>\n",
       "      <td>0.525881</td>\n",
       "      <td>6.902331</td>\n",
       "      <td>9.556719</td>\n",
       "      <td>-1.307374</td>\n",
       "      <td>3320</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33207</th>\n",
       "      <td>7</td>\n",
       "      <td>1.255061</td>\n",
       "      <td>0.524707</td>\n",
       "      <td>-0.323081</td>\n",
       "      <td>9.800545</td>\n",
       "      <td>0.373513</td>\n",
       "      <td>3.522918</td>\n",
       "      <td>0.788738</td>\n",
       "      <td>0.501832</td>\n",
       "      <td>6.481227</td>\n",
       "      <td>1.250318</td>\n",
       "      <td>3320</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33208</th>\n",
       "      <td>8</td>\n",
       "      <td>1.192596</td>\n",
       "      <td>-0.054308</td>\n",
       "      <td>-0.015530</td>\n",
       "      <td>10.027349</td>\n",
       "      <td>0.571962</td>\n",
       "      <td>3.890334</td>\n",
       "      <td>-0.147352</td>\n",
       "      <td>8.861643</td>\n",
       "      <td>7.323592</td>\n",
       "      <td>1.748386</td>\n",
       "      <td>3320</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33209</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3320</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       timeline_index        d1        d2        d3         d4        d5  \\\n",
       "33200               0  1.353214 -0.676448  0.215638   9.932939  0.650689   \n",
       "33201               1  0.570237  0.459697  0.609359   9.673721 -0.411311   \n",
       "33202               2  0.084992  2.184650  0.310398  10.579418 -0.191729   \n",
       "33203               3  1.039667  0.449838  0.181571  10.064098  0.280863   \n",
       "33204               4  0.555279  1.213909  1.041155  10.354813 -0.680132   \n",
       "33205               5  0.719879  0.543125  0.123861   9.536921  0.349212   \n",
       "33206               6  0.633168  1.132934  1.056927  10.242445 -0.677864   \n",
       "33207               7  1.255061  0.524707 -0.323081   9.800545  0.373513   \n",
       "33208               8  1.192596 -0.054308 -0.015530  10.027349  0.571962   \n",
       "33209               0  0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "\n",
       "             d6        d7        d8         d9       d10  text_id language  \n",
       "33200  5.326757 -0.377507 -1.604548   9.317955  3.325151     3320       en  \n",
       "33201  6.861120  0.438183  8.438325  10.143123  0.247765     3320       en  \n",
       "33202  3.764890  1.148623  1.218172   7.312806  1.726927     3320       en  \n",
       "33203  4.666007  0.288457  1.372774   7.274780  1.995180     3320       en  \n",
       "33204  5.347591  0.620669  6.782768   9.622807 -1.069551     3320       en  \n",
       "33205  3.724797  0.432820  1.072457   6.548460  0.793852     3320       en  \n",
       "33206  5.246167  0.525881  6.902331   9.556719 -1.307374     3320       en  \n",
       "33207  3.522918  0.788738  0.501832   6.481227  1.250318     3320       en  \n",
       "33208  3.890334 -0.147352  8.861643   7.323592  1.748386     3320       en  \n",
       "33209  0.000000  0.000000  0.000000   0.000000  0.000000     3320       -1  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df_padded[dataset.df_padded[\"text_id\"]==3320]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "420f0929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word                          gemmology\n",
       "language                             en\n",
       "tokens      [g, e, m, m, o, l, o, g, y]\n",
       "Name: 3320, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_2.df.iloc[3320]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc44028",
   "metadata": {},
   "source": [
    "For words which are longer than 10 letters, we only take the last 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db8dd09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeline_index</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>d10</th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2</td>\n",
       "      <td>1.317977</td>\n",
       "      <td>-0.560411</td>\n",
       "      <td>0.088102</td>\n",
       "      <td>9.918494</td>\n",
       "      <td>0.652725</td>\n",
       "      <td>5.139175</td>\n",
       "      <td>-0.433229</td>\n",
       "      <td>-1.957868</td>\n",
       "      <td>9.441710</td>\n",
       "      <td>3.286282</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>3</td>\n",
       "      <td>0.638559</td>\n",
       "      <td>0.376517</td>\n",
       "      <td>0.551726</td>\n",
       "      <td>9.560905</td>\n",
       "      <td>-0.479745</td>\n",
       "      <td>7.071331</td>\n",
       "      <td>0.380813</td>\n",
       "      <td>8.455746</td>\n",
       "      <td>10.142075</td>\n",
       "      <td>0.265349</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>4</td>\n",
       "      <td>0.773838</td>\n",
       "      <td>0.858633</td>\n",
       "      <td>-0.193580</td>\n",
       "      <td>9.875707</td>\n",
       "      <td>0.834488</td>\n",
       "      <td>3.804242</td>\n",
       "      <td>0.886892</td>\n",
       "      <td>1.669018</td>\n",
       "      <td>7.961495</td>\n",
       "      <td>1.524800</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>5</td>\n",
       "      <td>0.768352</td>\n",
       "      <td>-1.414051</td>\n",
       "      <td>0.409404</td>\n",
       "      <td>8.982706</td>\n",
       "      <td>0.151192</td>\n",
       "      <td>5.705562</td>\n",
       "      <td>-0.322427</td>\n",
       "      <td>2.893784</td>\n",
       "      <td>7.380110</td>\n",
       "      <td>0.191888</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>6</td>\n",
       "      <td>0.394626</td>\n",
       "      <td>0.569939</td>\n",
       "      <td>0.894882</td>\n",
       "      <td>9.464315</td>\n",
       "      <td>-0.599912</td>\n",
       "      <td>5.978182</td>\n",
       "      <td>0.508771</td>\n",
       "      <td>7.398853</td>\n",
       "      <td>9.959645</td>\n",
       "      <td>-0.185058</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>7</td>\n",
       "      <td>0.331180</td>\n",
       "      <td>0.620971</td>\n",
       "      <td>0.091171</td>\n",
       "      <td>9.666517</td>\n",
       "      <td>0.652566</td>\n",
       "      <td>1.552025</td>\n",
       "      <td>1.014759</td>\n",
       "      <td>1.336801</td>\n",
       "      <td>7.683355</td>\n",
       "      <td>0.604742</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>8</td>\n",
       "      <td>0.215077</td>\n",
       "      <td>1.710426</td>\n",
       "      <td>2.093915</td>\n",
       "      <td>10.198302</td>\n",
       "      <td>0.208076</td>\n",
       "      <td>4.545253</td>\n",
       "      <td>1.807776</td>\n",
       "      <td>7.774212</td>\n",
       "      <td>9.939435</td>\n",
       "      <td>1.417312</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>9</td>\n",
       "      <td>1.838432</td>\n",
       "      <td>-0.503554</td>\n",
       "      <td>0.196605</td>\n",
       "      <td>9.628243</td>\n",
       "      <td>1.677094</td>\n",
       "      <td>4.554884</td>\n",
       "      <td>0.082244</td>\n",
       "      <td>3.169625</td>\n",
       "      <td>6.684532</td>\n",
       "      <td>1.792743</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>10</td>\n",
       "      <td>0.443460</td>\n",
       "      <td>-0.229120</td>\n",
       "      <td>-0.662848</td>\n",
       "      <td>9.978356</td>\n",
       "      <td>1.078365</td>\n",
       "      <td>4.006979</td>\n",
       "      <td>-0.185805</td>\n",
       "      <td>9.632396</td>\n",
       "      <td>7.365420</td>\n",
       "      <td>1.925159</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>11</td>\n",
       "      <td>0.279706</td>\n",
       "      <td>-0.122955</td>\n",
       "      <td>-0.715107</td>\n",
       "      <td>9.911089</td>\n",
       "      <td>1.680002</td>\n",
       "      <td>-3.555078</td>\n",
       "      <td>-0.182654</td>\n",
       "      <td>4.726729</td>\n",
       "      <td>8.821706</td>\n",
       "      <td>1.976509</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     timeline_index        d1        d2        d3         d4        d5  \\\n",
       "100               2  1.317977 -0.560411  0.088102   9.918494  0.652725   \n",
       "101               3  0.638559  0.376517  0.551726   9.560905 -0.479745   \n",
       "102               4  0.773838  0.858633 -0.193580   9.875707  0.834488   \n",
       "103               5  0.768352 -1.414051  0.409404   8.982706  0.151192   \n",
       "104               6  0.394626  0.569939  0.894882   9.464315 -0.599912   \n",
       "105               7  0.331180  0.620971  0.091171   9.666517  0.652566   \n",
       "106               8  0.215077  1.710426  2.093915  10.198302  0.208076   \n",
       "107               9  1.838432 -0.503554  0.196605   9.628243  1.677094   \n",
       "108              10  0.443460 -0.229120 -0.662848   9.978356  1.078365   \n",
       "109              11  0.279706 -0.122955 -0.715107   9.911089  1.680002   \n",
       "\n",
       "           d6        d7        d8         d9       d10  text_id language  \n",
       "100  5.139175 -0.433229 -1.957868   9.441710  3.286282       10       de  \n",
       "101  7.071331  0.380813  8.455746  10.142075  0.265349       10       de  \n",
       "102  3.804242  0.886892  1.669018   7.961495  1.524800       10       de  \n",
       "103  5.705562 -0.322427  2.893784   7.380110  0.191888       10       de  \n",
       "104  5.978182  0.508771  7.398853   9.959645 -0.185058       10       de  \n",
       "105  1.552025  1.014759  1.336801   7.683355  0.604742       10       de  \n",
       "106  4.545253  1.807776  7.774212   9.939435  1.417312       10       de  \n",
       "107  4.554884  0.082244  3.169625   6.684532  1.792743       10       de  \n",
       "108  4.006979 -0.185805  9.632396   7.365420  1.925159       10       de  \n",
       "109 -3.555078 -0.182654  4.726729   8.821706  1.976509       10       de  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df_padded[dataset.df_padded[\"text_id\"]==10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ce40af09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word                                ungetrenntes\n",
       "language                                      de\n",
       "tokens      [u, n, g, e, t, r, e, n, n, t, e, s]\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_2.df.iloc[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caac355",
   "metadata": {},
   "source": [
    "To obtain a path as a torch tensor, we use the `.get_torch_path()` method which by default keeps the time features and will remove the id and label columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be6c23db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12000, 10, 11])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by default keeps the time features\n",
    "torch_word_path = dataset.get_torch_path()\n",
    "torch_word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cbab692c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00,  6.4244e-01,  3.4750e-01,  6.3362e-01,  9.4985e+00,\n",
       "         -5.3674e-01,  7.2364e+00,  3.4943e-01,  8.4764e+00,  1.0285e+01,\n",
       "          2.1238e-01],\n",
       "        [ 2.0000e+00,  1.0979e+00,  1.1596e+00, -5.5805e-01,  1.0146e+01,\n",
       "          9.6599e-01,  4.5941e+00,  8.3923e-01,  1.7185e+00,  8.3358e+00,\n",
       "          2.0114e+00],\n",
       "        [ 3.0000e+00,  6.6070e-01, -1.4742e+00,  3.2493e-01,  8.9963e+00,\n",
       "          1.4717e-01,  5.6268e+00, -3.2671e-01,  2.9570e+00,  7.3578e+00,\n",
       "          1.1978e-01],\n",
       "        [ 4.0000e+00,  6.6091e-02,  9.3484e-01,  1.9841e+00,  9.8973e+00,\n",
       "         -4.4542e-01,  5.4860e+00,  8.2021e-01,  7.0649e+00,  1.0352e+01,\n",
       "          3.0865e-01],\n",
       "        [ 5.0000e+00,  1.8166e-02,  2.3276e+00, -3.9609e-01,  1.0137e+01,\n",
       "          6.6799e-01,  3.2185e+00,  1.8925e+00,  2.2358e+00,  8.1868e+00,\n",
       "          1.9863e+00],\n",
       "        [ 6.0000e+00,  1.0228e+00, -5.4154e-01,  3.2448e-01,  9.8019e+00,\n",
       "          2.0029e-01,  5.4010e+00, -2.4271e-01,  3.1824e+00,  7.2493e+00,\n",
       "          1.4144e+00],\n",
       "        [ 7.0000e+00,  4.2570e-01,  3.6015e-01,  3.1502e-01,  9.9994e+00,\n",
       "         -2.5061e-01,  6.2962e+00,  3.8487e-01,  8.1752e+00,  9.8636e+00,\n",
       "          3.6948e-01],\n",
       "        [ 8.0000e+00, -3.6171e-01,  2.2221e+00, -2.2104e-01,  1.0026e+01,\n",
       "          5.2048e-01,  2.3713e+00,  2.4052e+00,  1.7668e+00,  8.6196e+00,\n",
       "          1.6124e+00],\n",
       "        [ 9.0000e+00,  1.5731e+00, -2.8157e-01,  6.0103e-01,  9.5714e+00,\n",
       "          1.0294e+00,  4.3277e+00, -8.6587e-03,  3.5403e+00,  6.9146e+00,\n",
       "          1.5148e+00],\n",
       "        [ 1.0000e+01,  4.9721e-01, -2.2663e-01, -5.8244e-01,  9.9623e+00,\n",
       "          1.0211e+00,  3.8046e+00, -1.9416e-01,  9.5963e+00,  7.2777e+00,\n",
       "          1.9904e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_word_path[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6010d9ea",
   "metadata": {},
   "source": [
    "We can choose to ignore the time features by setting `include_time_features=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e47efa4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12000, 10, 10])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ingore time features (ignore first column of timeline_index)\n",
    "torch_word_path = dataset.get_torch_path(include_time_features=False)\n",
    "torch_word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "89fc4f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.4244e-01,  3.4750e-01,  6.3362e-01,  9.4985e+00, -5.3674e-01,\n",
       "          7.2364e+00,  3.4943e-01,  8.4764e+00,  1.0285e+01,  2.1238e-01],\n",
       "        [ 1.0979e+00,  1.1596e+00, -5.5805e-01,  1.0146e+01,  9.6599e-01,\n",
       "          4.5941e+00,  8.3923e-01,  1.7185e+00,  8.3358e+00,  2.0114e+00],\n",
       "        [ 6.6070e-01, -1.4742e+00,  3.2493e-01,  8.9963e+00,  1.4717e-01,\n",
       "          5.6268e+00, -3.2671e-01,  2.9570e+00,  7.3578e+00,  1.1978e-01],\n",
       "        [ 6.6091e-02,  9.3484e-01,  1.9841e+00,  9.8973e+00, -4.4542e-01,\n",
       "          5.4860e+00,  8.2021e-01,  7.0649e+00,  1.0352e+01,  3.0865e-01],\n",
       "        [ 1.8166e-02,  2.3276e+00, -3.9609e-01,  1.0137e+01,  6.6799e-01,\n",
       "          3.2185e+00,  1.8925e+00,  2.2358e+00,  8.1868e+00,  1.9863e+00],\n",
       "        [ 1.0228e+00, -5.4154e-01,  3.2448e-01,  9.8019e+00,  2.0029e-01,\n",
       "          5.4010e+00, -2.4271e-01,  3.1824e+00,  7.2493e+00,  1.4144e+00],\n",
       "        [ 4.2570e-01,  3.6015e-01,  3.1502e-01,  9.9994e+00, -2.5061e-01,\n",
       "          6.2962e+00,  3.8487e-01,  8.1752e+00,  9.8636e+00,  3.6948e-01],\n",
       "        [-3.6171e-01,  2.2221e+00, -2.2104e-01,  1.0026e+01,  5.2048e-01,\n",
       "          2.3713e+00,  2.4052e+00,  1.7668e+00,  8.6196e+00,  1.6124e+00],\n",
       "        [ 1.5731e+00, -2.8157e-01,  6.0103e-01,  9.5714e+00,  1.0294e+00,\n",
       "          4.3277e+00, -8.6587e-03,  3.5403e+00,  6.9146e+00,  1.5148e+00],\n",
       "        [ 4.9721e-01, -2.2663e-01, -5.8244e-01,  9.9623e+00,  1.0211e+00,\n",
       "          3.8046e+00, -1.9416e-01,  9.5963e+00,  7.2777e+00,  1.9904e+00]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_word_path[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253964b4-8ccf-4e9e-ad8c-6ae86bc375ce",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "The computations described in this notebook were performed using the Baskerville Tier 2 HPC service (https://www.baskerville.ac.uk/). Baskerville was funded by the EPSRC and UKRI through the World Class Labs scheme (EP/T022221/1) and the Digital Research Infrastructure programme (EP/W032244/1) and is operated by Advanced Research Computing at the University of Birmingham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6e838-51c5-45bf-bf3a-d7fd83246628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66896ed-0b24-40db-9947-88b10035e79f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpsig (Conda)",
   "language": "python",
   "name": "sys_nlpsig"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
