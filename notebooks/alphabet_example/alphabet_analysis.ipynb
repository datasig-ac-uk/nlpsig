{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab9b36c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    RobertaConfig,\n",
    "    RobertaForMaskedLM,\n",
    "    RobertaTokenizerFast,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "import tokenizers\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nlpsig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a138bb3",
   "metadata": {},
   "source": [
    "## Language dataset\n",
    "\n",
    "In the `data/` folder, we have several text folders of words from different languages:\n",
    "- `wordlist_de.txt`: German words\n",
    "- `wordlist_en.txt`: English words\n",
    "- `wordlist_fr.txt`: French words\n",
    "- `wordlist_it.txt`: Italian words\n",
    "- `wordlist_pl.txt`: Polish words\n",
    "- `wordlist_sv.txt`: Swedish words\n",
    "\n",
    "We additionally have a `alphabet.txt` file which just stores the alphabet characters ('a', 'b', 'c', ...).\n",
    "\n",
    "The task is to split the words into its individual characters and to obtain an embedding for each of them. We can represent a word by a path of its character embeddings and compute its path signature to use as features in predicting the language for which the word belongs.\n",
    "\n",
    "Here we look at obtaining embeddings using a Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e86c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "ALPHABET_FILE = 'data/alphabet.txt'\n",
    "with open(ALPHABET_FILE) as f:\n",
    "    alphabet = f.read().splitlines()\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be809c3b",
   "metadata": {},
   "source": [
    "## Set up Tokenizer for word corpora\n",
    "\n",
    "If we were to fine-tune an existing pretrained transformer, we could use the same tokenizer that the model was pretrained with. However, here we want to train a model from stratch, and so using a tokenizer that was pretrained on a corpus that looks quite different to ours is suboptimal.\n",
    "\n",
    "Here, we need to use the `tokenizers` library to set up and train a new tokenizer for our text.\n",
    "\n",
    "In particular, we're going to start off with a character-based tokenizer (as we're going to split up our words into characters), and train it to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4d214a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAR_BERT/vocab.json', 'CHAR_BERT/merges.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialise character based tokenizer\n",
    "tokenizer = tokenizers.CharBPETokenizer()\n",
    "tokenizer.train(files=[ALPHABET_FILE], show_progress=False)\n",
    "\n",
    "# save the tokenizer to \"CHAR_BERT/\" folder\n",
    "if not os.path.exists(\"CHAR_BERT\"):\n",
    "    os.makedirs(\"CHAR_BERT\")\n",
    "\n",
    "tokenizer.save_model(\"CHAR_BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f39b30e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist_files = [\"data/wordlist_de.txt\",\n",
    "                  \"data/wordlist_en.txt\",\n",
    "                  \"data/wordlist_fr.txt\",\n",
    "                  \"data/wordlist_it.txt\",\n",
    "                  \"data/wordlist_pl.txt\",\n",
    "                  \"data/wordlist_sv.txt\"]\n",
    "\n",
    "wordlist_dfs = []\n",
    "for filename in wordlist_files:\n",
    "    with open(filename, \"r\") as f:\n",
    "        words = f.read().splitlines()\n",
    "        words_df = pd.DataFrame({\"word\": words})\n",
    "        words_df[\"language\"] = filename.split(\"_\")[1][0:2]\n",
    "        wordlist_dfs.append(words_df)\n",
    "\n",
    "corpus_df = pd.concat(wordlist_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c22e305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aal</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aale</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aalen</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aalend</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77121</th>\n",
       "      <td>zons</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77122</th>\n",
       "      <td>zoo</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77123</th>\n",
       "      <td>zoologisk</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77124</th>\n",
       "      <td>zoologiska</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77125</th>\n",
       "      <td>zoologiskt</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3922535 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word language\n",
       "0               a       de\n",
       "1             aal       de\n",
       "2            aale       de\n",
       "3           aalen       de\n",
       "4          aalend       de\n",
       "...           ...      ...\n",
       "77121        zons       sv\n",
       "77122         zoo       sv\n",
       "77123   zoologisk       sv\n",
       "77124  zoologiska       sv\n",
       "77125  zoologiskt       sv\n",
       "\n",
       "[3922535 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d2f43e",
   "metadata": {},
   "source": [
    "Question: what do we do with words that appear twice in the data? i.e. different languages have the same word in their vocabularies, e.g. the word \"zoo\" is a valid word in english, french, italian, polish and swedish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "517f028b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80613</th>\n",
       "      <td>zoo</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198413</th>\n",
       "      <td>zoo</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861555</th>\n",
       "      <td>zoo</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508829</th>\n",
       "      <td>zoo</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77122</th>\n",
       "      <td>zoo</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word language\n",
       "80613    zoo       en\n",
       "198413   zoo       fr\n",
       "1861555  zoo       it\n",
       "1508829  zoo       pl\n",
       "77122    zoo       sv"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df[corpus_df[\"word\"]==\"zoo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57561c5b",
   "metadata": {},
   "source": [
    "We take a random sample of the 3.9 million words in the corpora..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b4656a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unbehelligte</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>herauszuschlagender</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anzahlbarer</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unbeseelte</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>imaginativem</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>ifatt</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>formgavs</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>bestods</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>kurirers</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>satsningars</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      word language\n",
       "0             unbehelligte       de\n",
       "1      herauszuschlagender       de\n",
       "2              anzahlbarer       de\n",
       "3               unbeseelte       de\n",
       "4             imaginativem       de\n",
       "...                    ...      ...\n",
       "11995                ifatt       sv\n",
       "11996             formgavs       sv\n",
       "11997              bestods       sv\n",
       "11998             kurirers       sv\n",
       "11999          satsningars       sv\n",
       "\n",
       "[12000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "seed = 2022\n",
    "random.seed(seed)\n",
    "balanced = True\n",
    "\n",
    "# take \n",
    "n_words = 12000\n",
    "if balanced:\n",
    "    languages = corpus_df[\"language\"].unique()\n",
    "    words_per_language = math.floor(n_words / len(languages))\n",
    "    corpus_sample_df = pd.concat(\n",
    "        [corpus_df[corpus_df[\"language\"]==lang].sample(words_per_language, random_state=seed)\n",
    "         for lang in languages]\n",
    "    )\n",
    "    corpus_sample_df = corpus_sample_df.reset_index(drop=True)\n",
    "else:\n",
    "    corpus_sample_df = corpus_df.iloc[random.sample(range(len(corpus_df)), n_words)]\n",
    "    corpus_sample_df = corpus_sample_df.reset_index(drop=True)\n",
    "\n",
    "corpus_sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d853d648",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "de    2000\n",
       "en    2000\n",
       "fr    2000\n",
       "it    2000\n",
       "pl    2000\n",
       "sv    2000\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample_df[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "837941c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "max_length = 512\n",
    "\n",
    "# load in tokenizer for architecture\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('CHAR_BERT/',\n",
    "                                                 max_len=max_length)\n",
    "\n",
    "# set up data_collator to use (intially just one that adds padding)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# initialise transformer architecture (random weights)\n",
    "config_args = {\"vocab_size\": tokenizer.backend_tokenizer.get_vocab_size(),\n",
    "               \"hidden_size\": 768,\n",
    "               \"max_length\": 512,\n",
    "               \"max_position_embeddings\": max_length + 2,\n",
    "               \"hidden_dropout_prob\": 0.1,\n",
    "               \"num_attention_heads\": 12,\n",
    "               \"num_hidden_layers\": 6,\n",
    "               \"type_vocab_size\": 1}\n",
    "\n",
    "config = RobertaConfig(**config_args)\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "059975c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = nlpsig.TextEncoder(df=corpus_sample_df,\n",
    "                                  feature_name=\"word\",\n",
    "                                  model=model,\n",
    "                                  config=config,\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "586a8c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting return_special_tokens_mask=True\n",
      "[INFO] Tokenizing the datatset...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f74faf2a6ac46fb84fbb88f9b75568a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving the tokenized text for each sentence into `.df['tokens']`...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e172c2385b4343ff8aa043bb4c7577a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating tokenized dataframe and setting in `.tokenized_df` attribute...\n",
      "[INFO] Note: 'text_id' is the column name for denoting the corresponding text id\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['word', 'language', 'input_ids', 'attention_mask', 'special_tokens_mask', 'tokens'],\n",
       "    num_rows: 12000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokenize_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b061d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unbehelligte'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.dataset[\"word\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7007324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53, 21, 14, 2, 5, 8, 5, 12, 12, 9, 7, 20, 5, 54]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.dataset[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b011cb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u', 'n', 'b', 'e', 'h', 'e', 'l', 'l', 'i', 'g', 't', 'e']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.dataset[\"tokens\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94207f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130015</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130016</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130017</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130018</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130019</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130020 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id language tokens\n",
       "0             0       de      u\n",
       "1             0       de      n\n",
       "2             0       de      b\n",
       "3             0       de      e\n",
       "4             0       de      h\n",
       "...         ...      ...    ...\n",
       "130015    11999       sv      n\n",
       "130016    11999       sv      g\n",
       "130017    11999       sv      a\n",
       "130018    11999       sv      r\n",
       "130019    11999       sv      s\n",
       "\n",
       "[130020 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokenized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bba147ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                               | 0/120 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:45<00:00,  2.62it/s]\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = text_encoder.obtain_embeddings(method = \"hidden_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77e83d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130020, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "49a30552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    text_id language tokens\n",
       "0         0       de      u\n",
       "1         0       de      n\n",
       "2         0       de      b\n",
       "3         0       de      e\n",
       "4         0       de      h\n",
       "5         0       de      e\n",
       "6         0       de      l\n",
       "7         0       de      l\n",
       "8         0       de      i\n",
       "9         0       de      g\n",
       "10        0       de      t\n",
       "11        0       de      e"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokenized_df[text_encoder.tokenized_df[\"text_id\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c53354b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 12000\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91829082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12000/12000 [00:01<00:00, 6156.56it/s]\n"
     ]
    }
   ],
   "source": [
    "pooled_embeddings = text_encoder.pool_token_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d13c8fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 768)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7f0947",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fbfb14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data_collator for language modelling (has dynamic padding)\n",
    "data_collator_for_LM = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "                                                       mlm=True,\n",
    "                                                       mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32d62984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Splitting up dataset into train / validation / test sets, and saving to `.dataset_split`.\n",
      "[INFO] Setting up TrainingArguments object and saving to `.training_args`.\n",
      "[INFO] Setting up Trainer object, and saving to `.trainer`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x2cf48f6d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.split_dataset()\n",
    "text_encoder.set_up_training_args(output_dir=\"CHAR_BERT_trained\",\n",
    "                                  num_train_epochs=20,\n",
    "                                  per_device_train_batch_size=128,\n",
    "                                  seed=seed)\n",
    "text_encoder.set_up_trainer(data_collator=data_collator_for_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e4e94de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, language, special_tokens_mask, tokens. If word, language, special_tokens_mask, tokens are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/Users/rchan/opt/miniconda3/envs/py38esig/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 120000\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 46900\n",
      "  Number of trainable parameters = 43560249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training model with 43560249 parameters...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='46900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   10/46900 00:09 < 15:15:37, 0.85 it/s, Epoch 0.01/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtext_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transformer_with_trainer_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/rough_paths/path_signatures_bert/nlpsig/encode_text.py:1209\u001b[0m, in \u001b[0;36mTextEncoder.fit_transformer_with_trainer_api\u001b[0;34m(self, output_dir, data_collator, compute_metrics, training_args, trainer_args)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_up_trainer(\n\u001b[1;32m   1204\u001b[0m         data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m   1205\u001b[0m         compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrainer_args,\n\u001b[1;32m   1207\u001b[0m     )\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Training model with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_parameters()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m parameters...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1209\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Training completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py38esig/lib/python3.8/site-packages/transformers/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1498\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py38esig/lib/python3.8/site-packages/transformers/trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1752\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1754\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1755\u001b[0m ):\n\u001b[1;32m   1756\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py38esig/lib/python3.8/site-packages/transformers/trainer.py:2526\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2524\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2526\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py38esig/lib/python3.8/site-packages/torch/_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    248\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    249\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    254\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 255\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py38esig/lib/python3.8/site-packages/torch/autograd/__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 147\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text_encoder.fit_transformer_with_trainer_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8ae8e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to CHAR_BERT_trained/\n",
      "Configuration saved in CHAR_BERT_trained/config.json\n",
      "Model weights saved in CHAR_BERT_trained/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "text_encoder.trainer.save_model(\"CHAR_BERT_trained/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d4447",
   "metadata": {},
   "source": [
    "## Evaluating trained model\n",
    "\n",
    "Evaluating the performance on predicting the masked letter for the test dataset. To do this, for each word in our test dataset, we will mask each letter on its own and ask the model to predict the masked letter. So for a 5 letter word, we have 5 predictions to make - one for each letter given the other letters.\n",
    "\n",
    "For our tokenizer, we see that \"\\<mask>\" is used as the mask token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4354efd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2884230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file CHAR_BERT_trained/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"CHAR_BERT_trained\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 57\n",
      "}\n",
      "\n",
      "loading configuration file CHAR_BERT_trained/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"CHAR_BERT_trained\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 57\n",
      "}\n",
      "\n",
      "loading weights file CHAR_BERT_trained/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at CHAR_BERT_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with 15000 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15000/15000 [1:16:58<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4963931386184516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4963931386184516"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_masked_character_accuracy(fill_mask, words):\n",
    "    was_correct = []\n",
    "    print(f\"Evaluating with {len(words)} words\")\n",
    "    for word in tqdm(words):\n",
    "        masked_strings = [word[:i] + '<mask>' + word[i+1:] for i in range(len(word))]\n",
    "        predictions = [fill_mask(word)[0]['sequence'] for word in masked_strings]\n",
    "        was_correct += [pred == word for pred in predictions]\n",
    "    \n",
    "    acc = np.sum(was_correct) / len(was_correct)\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    return acc\n",
    "\n",
    "fill_mask = pipeline(\"fill-mask\",\n",
    "                     model=\"CHAR_BERT_trained\",\n",
    "                     tokenizer=\"CHAR_BERT_trained\")\n",
    "\n",
    "compute_masked_character_accuracy(fill_mask, \n",
    "                                  text_encoder.dataset_split[\"test\"][\"word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c0800",
   "metadata": {},
   "source": [
    "## Obtaining a path for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fe49409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130015</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130016</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130017</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130018</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130019</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130020 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id language tokens\n",
       "0             0       de      u\n",
       "1             0       de      n\n",
       "2             0       de      b\n",
       "3             0       de      e\n",
       "4             0       de      h\n",
       "...         ...      ...    ...\n",
       "130015    11999       sv      n\n",
       "130016    11999       sv      g\n",
       "130017    11999       sv      a\n",
       "130018    11999       sv      r\n",
       "130019    11999       sv      s\n",
       "\n",
       "[130020 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokenized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e449b010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130020, 768)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9e27111d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Note 'datetime' is not a column in `.df`, so only 'timeline_index' is added.\n",
      "[INFO] As 'datetime' is not a column in `.df`, we assume that the data is ordered by time with respect to the id.\n",
      "[INFO] Adding 'timeline_index' feature...\n"
     ]
    }
   ],
   "source": [
    "dataset = PrepareData(text_encoder.tokenized_df,\n",
    "                             id_column=\"text_id\",\n",
    "                             labels_column=\"language\",\n",
    "                             embeddings=token_embeddings,\n",
    "                             pooled_embeddings=pooled_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e9d0056f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.df[\"text_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "07349a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12000/12000 [00:30<00:00, 387.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12000, 10, 771)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_path = dataset.pad(pad_by=\"id\",\n",
    "                        zero_padding=True,\n",
    "                        method=\"k_last\",\n",
    "                        k=10,\n",
    "                        time_feature=[\"timeline_index\"],\n",
    "                        standardise_time_feature=False)\n",
    "word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c228b103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeline_index</th>\n",
       "      <th>e1</th>\n",
       "      <th>e2</th>\n",
       "      <th>e3</th>\n",
       "      <th>e4</th>\n",
       "      <th>e5</th>\n",
       "      <th>e6</th>\n",
       "      <th>e7</th>\n",
       "      <th>e8</th>\n",
       "      <th>e9</th>\n",
       "      <th>...</th>\n",
       "      <th>e761</th>\n",
       "      <th>e762</th>\n",
       "      <th>e763</th>\n",
       "      <th>e764</th>\n",
       "      <th>e765</th>\n",
       "      <th>e766</th>\n",
       "      <th>e767</th>\n",
       "      <th>e768</th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.143588</td>\n",
       "      <td>-1.417048</td>\n",
       "      <td>1.320182</td>\n",
       "      <td>-0.508459</td>\n",
       "      <td>1.247967</td>\n",
       "      <td>0.034426</td>\n",
       "      <td>-1.468047</td>\n",
       "      <td>-0.961864</td>\n",
       "      <td>0.559406</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.980690</td>\n",
       "      <td>0.403656</td>\n",
       "      <td>0.398150</td>\n",
       "      <td>0.487656</td>\n",
       "      <td>-0.676191</td>\n",
       "      <td>0.807380</td>\n",
       "      <td>-1.213304</td>\n",
       "      <td>-0.957085</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1.107637</td>\n",
       "      <td>-0.667894</td>\n",
       "      <td>2.376304</td>\n",
       "      <td>-0.928480</td>\n",
       "      <td>3.609042</td>\n",
       "      <td>-0.043664</td>\n",
       "      <td>-1.700852</td>\n",
       "      <td>0.552448</td>\n",
       "      <td>-0.633671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.324514</td>\n",
       "      <td>-0.190638</td>\n",
       "      <td>-0.429270</td>\n",
       "      <td>1.990711</td>\n",
       "      <td>-0.213387</td>\n",
       "      <td>0.543733</td>\n",
       "      <td>0.014306</td>\n",
       "      <td>-0.571665</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.777725</td>\n",
       "      <td>-0.197840</td>\n",
       "      <td>2.643306</td>\n",
       "      <td>-1.788009</td>\n",
       "      <td>-0.637072</td>\n",
       "      <td>-0.254247</td>\n",
       "      <td>0.406706</td>\n",
       "      <td>-0.182973</td>\n",
       "      <td>0.702718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.377799</td>\n",
       "      <td>2.157183</td>\n",
       "      <td>-0.826511</td>\n",
       "      <td>1.861895</td>\n",
       "      <td>-1.222253</td>\n",
       "      <td>-0.212417</td>\n",
       "      <td>-1.194266</td>\n",
       "      <td>-1.107049</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.717557</td>\n",
       "      <td>-0.304074</td>\n",
       "      <td>1.029350</td>\n",
       "      <td>-1.073429</td>\n",
       "      <td>1.875072</td>\n",
       "      <td>-0.084052</td>\n",
       "      <td>-1.756982</td>\n",
       "      <td>0.406479</td>\n",
       "      <td>-0.279253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262103</td>\n",
       "      <td>1.245355</td>\n",
       "      <td>-0.515692</td>\n",
       "      <td>1.316131</td>\n",
       "      <td>-0.730478</td>\n",
       "      <td>-0.612286</td>\n",
       "      <td>0.887850</td>\n",
       "      <td>-0.179549</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.921733</td>\n",
       "      <td>0.051804</td>\n",
       "      <td>0.740113</td>\n",
       "      <td>-0.494959</td>\n",
       "      <td>0.141836</td>\n",
       "      <td>0.293235</td>\n",
       "      <td>0.499587</td>\n",
       "      <td>0.465118</td>\n",
       "      <td>-0.825649</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.601243</td>\n",
       "      <td>1.046709</td>\n",
       "      <td>0.133001</td>\n",
       "      <td>1.805218</td>\n",
       "      <td>-0.145986</td>\n",
       "      <td>0.592019</td>\n",
       "      <td>-2.297613</td>\n",
       "      <td>-1.601534</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>6</td>\n",
       "      <td>1.178958</td>\n",
       "      <td>-0.148862</td>\n",
       "      <td>1.265958</td>\n",
       "      <td>-0.734617</td>\n",
       "      <td>0.983744</td>\n",
       "      <td>-0.335252</td>\n",
       "      <td>0.362385</td>\n",
       "      <td>0.789220</td>\n",
       "      <td>-0.720118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082894</td>\n",
       "      <td>2.911485</td>\n",
       "      <td>-0.729118</td>\n",
       "      <td>2.193193</td>\n",
       "      <td>-0.566522</td>\n",
       "      <td>-0.983746</td>\n",
       "      <td>-1.191947</td>\n",
       "      <td>0.632889</td>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>7</td>\n",
       "      <td>1.024163</td>\n",
       "      <td>-2.861085</td>\n",
       "      <td>0.770218</td>\n",
       "      <td>-0.320513</td>\n",
       "      <td>0.436667</td>\n",
       "      <td>0.087550</td>\n",
       "      <td>-0.765312</td>\n",
       "      <td>0.123461</td>\n",
       "      <td>-0.976917</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.779674</td>\n",
       "      <td>1.250211</td>\n",
       "      <td>-0.421141</td>\n",
       "      <td>1.575737</td>\n",
       "      <td>-0.471906</td>\n",
       "      <td>1.111535</td>\n",
       "      <td>-1.863977</td>\n",
       "      <td>0.369330</td>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>8</td>\n",
       "      <td>1.684086</td>\n",
       "      <td>-0.072866</td>\n",
       "      <td>1.837575</td>\n",
       "      <td>-1.241489</td>\n",
       "      <td>1.546939</td>\n",
       "      <td>0.040575</td>\n",
       "      <td>-0.274818</td>\n",
       "      <td>0.115267</td>\n",
       "      <td>-0.512420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.870751</td>\n",
       "      <td>1.708995</td>\n",
       "      <td>-0.186876</td>\n",
       "      <td>1.444939</td>\n",
       "      <td>0.052923</td>\n",
       "      <td>0.052861</td>\n",
       "      <td>-0.449925</td>\n",
       "      <td>0.161691</td>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>9</td>\n",
       "      <td>2.046932</td>\n",
       "      <td>-0.022341</td>\n",
       "      <td>0.495391</td>\n",
       "      <td>-1.086745</td>\n",
       "      <td>0.310161</td>\n",
       "      <td>-0.112842</td>\n",
       "      <td>0.781924</td>\n",
       "      <td>0.100807</td>\n",
       "      <td>0.078282</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.145458</td>\n",
       "      <td>3.236154</td>\n",
       "      <td>-0.256295</td>\n",
       "      <td>1.222923</td>\n",
       "      <td>-0.354660</td>\n",
       "      <td>-0.288702</td>\n",
       "      <td>-0.704664</td>\n",
       "      <td>-0.622784</td>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>10</td>\n",
       "      <td>1.661484</td>\n",
       "      <td>1.436595</td>\n",
       "      <td>1.242175</td>\n",
       "      <td>-0.588015</td>\n",
       "      <td>1.184628</td>\n",
       "      <td>-0.622242</td>\n",
       "      <td>-1.603853</td>\n",
       "      <td>0.529488</td>\n",
       "      <td>-0.543837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506650</td>\n",
       "      <td>0.352198</td>\n",
       "      <td>-0.147210</td>\n",
       "      <td>1.799329</td>\n",
       "      <td>-0.267002</td>\n",
       "      <td>0.553767</td>\n",
       "      <td>-0.547994</td>\n",
       "      <td>-0.739612</td>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows Ã— 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        timeline_index        e1        e2        e3        e4        e5  \\\n",
       "0                    2  0.143588 -1.417048  1.320182 -0.508459  1.247967   \n",
       "1                    3  1.107637 -0.667894  2.376304 -0.928480  3.609042   \n",
       "2                    4  0.777725 -0.197840  2.643306 -1.788009 -0.637072   \n",
       "3                    5  0.717557 -0.304074  1.029350 -1.073429  1.875072   \n",
       "4                    6  0.921733  0.051804  0.740113 -0.494959  0.141836   \n",
       "...                ...       ...       ...       ...       ...       ...   \n",
       "119995               6  1.178958 -0.148862  1.265958 -0.734617  0.983744   \n",
       "119996               7  1.024163 -2.861085  0.770218 -0.320513  0.436667   \n",
       "119997               8  1.684086 -0.072866  1.837575 -1.241489  1.546939   \n",
       "119998               9  2.046932 -0.022341  0.495391 -1.086745  0.310161   \n",
       "119999              10  1.661484  1.436595  1.242175 -0.588015  1.184628   \n",
       "\n",
       "              e6        e7        e8        e9  ...      e761      e762  \\\n",
       "0       0.034426 -1.468047 -0.961864  0.559406  ... -0.980690  0.403656   \n",
       "1      -0.043664 -1.700852  0.552448 -0.633671  ... -0.324514 -0.190638   \n",
       "2      -0.254247  0.406706 -0.182973  0.702718  ... -0.377799  2.157183   \n",
       "3      -0.084052 -1.756982  0.406479 -0.279253  ...  0.262103  1.245355   \n",
       "4       0.293235  0.499587  0.465118 -0.825649  ... -1.601243  1.046709   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "119995 -0.335252  0.362385  0.789220 -0.720118  ...  0.082894  2.911485   \n",
       "119996  0.087550 -0.765312  0.123461 -0.976917  ... -0.779674  1.250211   \n",
       "119997  0.040575 -0.274818  0.115267 -0.512420  ... -0.870751  1.708995   \n",
       "119998 -0.112842  0.781924  0.100807  0.078282  ... -1.145458  3.236154   \n",
       "119999 -0.622242 -1.603853  0.529488 -0.543837  ...  0.506650  0.352198   \n",
       "\n",
       "            e763      e764      e765      e766      e767      e768  text_id  \\\n",
       "0       0.398150  0.487656 -0.676191  0.807380 -1.213304 -0.957085        0   \n",
       "1      -0.429270  1.990711 -0.213387  0.543733  0.014306 -0.571665        0   \n",
       "2      -0.826511  1.861895 -1.222253 -0.212417 -1.194266 -1.107049        0   \n",
       "3      -0.515692  1.316131 -0.730478 -0.612286  0.887850 -0.179549        0   \n",
       "4       0.133001  1.805218 -0.145986  0.592019 -2.297613 -1.601534        0   \n",
       "...          ...       ...       ...       ...       ...       ...      ...   \n",
       "119995 -0.729118  2.193193 -0.566522 -0.983746 -1.191947  0.632889    11999   \n",
       "119996 -0.421141  1.575737 -0.471906  1.111535 -1.863977  0.369330    11999   \n",
       "119997 -0.186876  1.444939  0.052923  0.052861 -0.449925  0.161691    11999   \n",
       "119998 -0.256295  1.222923 -0.354660 -0.288702 -0.704664 -0.622784    11999   \n",
       "119999 -0.147210  1.799329 -0.267002  0.553767 -0.547994 -0.739612    11999   \n",
       "\n",
       "        language  \n",
       "0             de  \n",
       "1             de  \n",
       "2             de  \n",
       "3             de  \n",
       "4             de  \n",
       "...          ...  \n",
       "119995        sv  \n",
       "119996        sv  \n",
       "119997        sv  \n",
       "119998        sv  \n",
       "119999        sv  \n",
       "\n",
       "[120000 rows x 771 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f0c78ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeline_index</th>\n",
       "      <th>e1</th>\n",
       "      <th>e2</th>\n",
       "      <th>e3</th>\n",
       "      <th>e4</th>\n",
       "      <th>e5</th>\n",
       "      <th>e6</th>\n",
       "      <th>e7</th>\n",
       "      <th>e8</th>\n",
       "      <th>e9</th>\n",
       "      <th>...</th>\n",
       "      <th>e761</th>\n",
       "      <th>e762</th>\n",
       "      <th>e763</th>\n",
       "      <th>e764</th>\n",
       "      <th>e765</th>\n",
       "      <th>e766</th>\n",
       "      <th>e767</th>\n",
       "      <th>e768</th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.143588</td>\n",
       "      <td>-1.417048</td>\n",
       "      <td>1.320182</td>\n",
       "      <td>-0.508459</td>\n",
       "      <td>1.247967</td>\n",
       "      <td>0.034426</td>\n",
       "      <td>-1.468047</td>\n",
       "      <td>-0.961864</td>\n",
       "      <td>0.559406</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.980690</td>\n",
       "      <td>0.403656</td>\n",
       "      <td>0.398150</td>\n",
       "      <td>0.487656</td>\n",
       "      <td>-0.676191</td>\n",
       "      <td>0.807380</td>\n",
       "      <td>-1.213304</td>\n",
       "      <td>-0.957085</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1.107637</td>\n",
       "      <td>-0.667894</td>\n",
       "      <td>2.376304</td>\n",
       "      <td>-0.928480</td>\n",
       "      <td>3.609042</td>\n",
       "      <td>-0.043664</td>\n",
       "      <td>-1.700852</td>\n",
       "      <td>0.552448</td>\n",
       "      <td>-0.633671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.324514</td>\n",
       "      <td>-0.190638</td>\n",
       "      <td>-0.429270</td>\n",
       "      <td>1.990711</td>\n",
       "      <td>-0.213387</td>\n",
       "      <td>0.543733</td>\n",
       "      <td>0.014306</td>\n",
       "      <td>-0.571665</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.777725</td>\n",
       "      <td>-0.197840</td>\n",
       "      <td>2.643306</td>\n",
       "      <td>-1.788009</td>\n",
       "      <td>-0.637072</td>\n",
       "      <td>-0.254247</td>\n",
       "      <td>0.406706</td>\n",
       "      <td>-0.182973</td>\n",
       "      <td>0.702718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.377799</td>\n",
       "      <td>2.157183</td>\n",
       "      <td>-0.826511</td>\n",
       "      <td>1.861895</td>\n",
       "      <td>-1.222253</td>\n",
       "      <td>-0.212417</td>\n",
       "      <td>-1.194266</td>\n",
       "      <td>-1.107049</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.717557</td>\n",
       "      <td>-0.304074</td>\n",
       "      <td>1.029350</td>\n",
       "      <td>-1.073429</td>\n",
       "      <td>1.875072</td>\n",
       "      <td>-0.084052</td>\n",
       "      <td>-1.756982</td>\n",
       "      <td>0.406479</td>\n",
       "      <td>-0.279253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262103</td>\n",
       "      <td>1.245355</td>\n",
       "      <td>-0.515692</td>\n",
       "      <td>1.316131</td>\n",
       "      <td>-0.730478</td>\n",
       "      <td>-0.612286</td>\n",
       "      <td>0.887850</td>\n",
       "      <td>-0.179549</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.921733</td>\n",
       "      <td>0.051804</td>\n",
       "      <td>0.740113</td>\n",
       "      <td>-0.494959</td>\n",
       "      <td>0.141836</td>\n",
       "      <td>0.293235</td>\n",
       "      <td>0.499587</td>\n",
       "      <td>0.465118</td>\n",
       "      <td>-0.825649</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.601243</td>\n",
       "      <td>1.046709</td>\n",
       "      <td>0.133001</td>\n",
       "      <td>1.805218</td>\n",
       "      <td>-0.145986</td>\n",
       "      <td>0.592019</td>\n",
       "      <td>-2.297613</td>\n",
       "      <td>-1.601534</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>1.960512</td>\n",
       "      <td>-0.114054</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.959227</td>\n",
       "      <td>-0.274134</td>\n",
       "      <td>-0.047917</td>\n",
       "      <td>0.303097</td>\n",
       "      <td>0.895570</td>\n",
       "      <td>0.704869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654793</td>\n",
       "      <td>2.553124</td>\n",
       "      <td>-0.306052</td>\n",
       "      <td>1.188357</td>\n",
       "      <td>-0.323826</td>\n",
       "      <td>-0.661785</td>\n",
       "      <td>-0.127747</td>\n",
       "      <td>-0.642438</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.858337</td>\n",
       "      <td>0.980120</td>\n",
       "      <td>-0.480119</td>\n",
       "      <td>-1.347086</td>\n",
       "      <td>0.100639</td>\n",
       "      <td>0.609157</td>\n",
       "      <td>-1.403996</td>\n",
       "      <td>0.971574</td>\n",
       "      <td>1.116285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560951</td>\n",
       "      <td>-0.665503</td>\n",
       "      <td>0.562110</td>\n",
       "      <td>1.043689</td>\n",
       "      <td>0.655348</td>\n",
       "      <td>0.996938</td>\n",
       "      <td>-0.654039</td>\n",
       "      <td>-0.913582</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.739917</td>\n",
       "      <td>-0.593066</td>\n",
       "      <td>1.324941</td>\n",
       "      <td>-1.596288</td>\n",
       "      <td>0.311475</td>\n",
       "      <td>-0.266285</td>\n",
       "      <td>-0.007784</td>\n",
       "      <td>0.910885</td>\n",
       "      <td>1.055482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558452</td>\n",
       "      <td>1.238724</td>\n",
       "      <td>-0.340453</td>\n",
       "      <td>1.134031</td>\n",
       "      <td>-0.879360</td>\n",
       "      <td>0.472009</td>\n",
       "      <td>1.334212</td>\n",
       "      <td>-0.187999</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>1.059729</td>\n",
       "      <td>1.284173</td>\n",
       "      <td>0.815329</td>\n",
       "      <td>-0.547035</td>\n",
       "      <td>1.821547</td>\n",
       "      <td>-0.032910</td>\n",
       "      <td>-0.811382</td>\n",
       "      <td>0.412806</td>\n",
       "      <td>0.762482</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.534226</td>\n",
       "      <td>0.423330</td>\n",
       "      <td>0.452225</td>\n",
       "      <td>-0.418872</td>\n",
       "      <td>0.391442</td>\n",
       "      <td>0.305640</td>\n",
       "      <td>0.317617</td>\n",
       "      <td>-0.111468</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>-0.218854</td>\n",
       "      <td>-0.604878</td>\n",
       "      <td>1.608435</td>\n",
       "      <td>-0.389211</td>\n",
       "      <td>1.714435</td>\n",
       "      <td>-0.739268</td>\n",
       "      <td>-1.454955</td>\n",
       "      <td>0.345475</td>\n",
       "      <td>-1.222678</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.966916</td>\n",
       "      <td>0.498792</td>\n",
       "      <td>-0.507472</td>\n",
       "      <td>1.434245</td>\n",
       "      <td>-0.376011</td>\n",
       "      <td>1.072734</td>\n",
       "      <td>-0.002285</td>\n",
       "      <td>-0.825246</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   timeline_index        e1        e2        e3        e4        e5        e6  \\\n",
       "0               2  0.143588 -1.417048  1.320182 -0.508459  1.247967  0.034426   \n",
       "1               3  1.107637 -0.667894  2.376304 -0.928480  3.609042 -0.043664   \n",
       "2               4  0.777725 -0.197840  2.643306 -1.788009 -0.637072 -0.254247   \n",
       "3               5  0.717557 -0.304074  1.029350 -1.073429  1.875072 -0.084052   \n",
       "4               6  0.921733  0.051804  0.740113 -0.494959  0.141836  0.293235   \n",
       "5               7  1.960512 -0.114054 -0.089443 -0.959227 -0.274134 -0.047917   \n",
       "6               8  0.858337  0.980120 -0.480119 -1.347086  0.100639  0.609157   \n",
       "7               9  0.739917 -0.593066  1.324941 -1.596288  0.311475 -0.266285   \n",
       "8              10  1.059729  1.284173  0.815329 -0.547035  1.821547 -0.032910   \n",
       "9              11 -0.218854 -0.604878  1.608435 -0.389211  1.714435 -0.739268   \n",
       "\n",
       "         e7        e8        e9  ...      e761      e762      e763      e764  \\\n",
       "0 -1.468047 -0.961864  0.559406  ... -0.980690  0.403656  0.398150  0.487656   \n",
       "1 -1.700852  0.552448 -0.633671  ... -0.324514 -0.190638 -0.429270  1.990711   \n",
       "2  0.406706 -0.182973  0.702718  ... -0.377799  2.157183 -0.826511  1.861895   \n",
       "3 -1.756982  0.406479 -0.279253  ...  0.262103  1.245355 -0.515692  1.316131   \n",
       "4  0.499587  0.465118 -0.825649  ... -1.601243  1.046709  0.133001  1.805218   \n",
       "5  0.303097  0.895570  0.704869  ... -0.654793  2.553124 -0.306052  1.188357   \n",
       "6 -1.403996  0.971574  1.116285  ...  0.560951 -0.665503  0.562110  1.043689   \n",
       "7 -0.007784  0.910885  1.055482  ...  0.558452  1.238724 -0.340453  1.134031   \n",
       "8 -0.811382  0.412806  0.762482  ... -0.534226  0.423330  0.452225 -0.418872   \n",
       "9 -1.454955  0.345475 -1.222678  ... -0.966916  0.498792 -0.507472  1.434245   \n",
       "\n",
       "       e765      e766      e767      e768  text_id  language  \n",
       "0 -0.676191  0.807380 -1.213304 -0.957085        0        de  \n",
       "1 -0.213387  0.543733  0.014306 -0.571665        0        de  \n",
       "2 -1.222253 -0.212417 -1.194266 -1.107049        0        de  \n",
       "3 -0.730478 -0.612286  0.887850 -0.179549        0        de  \n",
       "4 -0.145986  0.592019 -2.297613 -1.601534        0        de  \n",
       "5 -0.323826 -0.661785 -0.127747 -0.642438        0        de  \n",
       "6  0.655348  0.996938 -0.654039 -0.913582        0        de  \n",
       "7 -0.879360  0.472009  1.334212 -0.187999        0        de  \n",
       "8  0.391442  0.305640  0.317617 -0.111468        0        de  \n",
       "9 -0.376011  1.072734 -0.002285 -0.825246        0        de  \n",
       "\n",
       "[10 rows x 771 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# still has the labels and the ids\n",
    "dataset.df_padded[dataset.df_padded[\"text_id\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6bbf09de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12000, 10, 769])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by default keeps the time features\n",
    "torch_word_path = dataset.get_torch_path()\n",
    "torch_word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0fcf2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = torch.from_numpy(dataset.pooled_embeddings.astype(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "329fb5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_emb = emb.unsqueeze(2).repeat(1,1,10).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9e8d2569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The path was created for each text_id in the dataframe, so to include embeddings in the FFN input, we concatenate the pooled embeddings.\n",
      "path shape: torch.Size([12000, 10, 1538])\n",
      "input_channels: 769\n"
     ]
    }
   ],
   "source": [
    "torch_word_path_for_deepsignet = dataset.get_torch_path_for_deepsignet(include_time_features_in_path=True,\n",
    "                                                                       include_time_features_in_input=True,\n",
    "                                                                       include_embedding_in_input=True)\n",
    "print(f\"path shape: {torch_word_path_for_deepsignet[0].shape}\")\n",
    "print(f\"input_channels: {torch_word_path_for_deepsignet[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "69fe20ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path shape: torch.Size([12000, 10, 770])\n",
      "input_channels: 769\n"
     ]
    }
   ],
   "source": [
    "torch_word_path_for_deepsignet = dataset.get_torch_path_for_deepsignet(include_time_features_in_path=True,\n",
    "                                                                       include_time_features_in_input=True,\n",
    "                                                                       include_embedding_in_input=False)\n",
    "print(f\"path shape: {torch_word_path_for_deepsignet[0].shape}\")\n",
    "print(f\"input_channels: {torch_word_path_for_deepsignet[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "931082ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path shape: torch.Size([12000, 10, 769])\n",
      "input_channels: 769\n"
     ]
    }
   ],
   "source": [
    "torch_word_path_for_deepsignet = dataset.get_torch_path_for_deepsignet(include_time_features_in_path=True,\n",
    "                                                                       include_time_features_in_input=False,\n",
    "                                                                       include_embedding_in_input=False)\n",
    "print(f\"path shape: {torch_word_path_for_deepsignet[0].shape}\")\n",
    "print(f\"input_channels: {torch_word_path_for_deepsignet[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f8a9b108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path shape: torch.Size([12000, 10, 769])\n",
      "input_channels: 768\n"
     ]
    }
   ],
   "source": [
    "torch_word_path_for_deepsignet = dataset.get_torch_path_for_deepsignet(include_time_features_in_path=False,\n",
    "                                                                       include_time_features_in_input=True,\n",
    "                                                                       include_embedding_in_input=False)\n",
    "print(f\"path shape: {torch_word_path_for_deepsignet[0].shape}\")\n",
    "print(f\"input_channels: {torch_word_path_for_deepsignet[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c344883d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path shape: torch.Size([12000, 10, 768])\n",
      "input_channels: 768\n"
     ]
    }
   ],
   "source": [
    "torch_word_path_for_deepsignet = dataset.get_torch_path_for_deepsignet(include_time_features_in_path=False,\n",
    "                                                                       include_time_features_in_input=False,\n",
    "                                                                       include_embedding_in_input=False)\n",
    "print(f\"path shape: {torch_word_path_for_deepsignet[0].shape}\")\n",
    "print(f\"input_channels: {torch_word_path_for_deepsignet[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4610cda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  3.,  4.,  ...,  9., 10., 11.],\n",
       "        [ 9., 10., 11.,  ..., 16., 17., 18.],\n",
       "        [ 1.,  2.,  3.,  ...,  8.,  9., 10.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0.,  ...,  4.,  5.,  6.],\n",
       "        [ 0.,  0.,  0.,  ...,  5.,  6.,  7.],\n",
       "        [ 1.,  2.,  3.,  ...,  8.,  9., 10.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "52f3359f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12000, 1])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch_word_path_for_deepsignet[0][:,:,torch_word_path_for_deepsignet[1]:torch_word_path_for_deepsignet[1]+1].max(1)[0]\n",
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2e97fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch_word_path_for_deepsignet[0][:, 0, (torch_word_path_for_deepsignet[1] + 1) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0481a7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12000, 773])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([t1,t2], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c264621d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12000, 1])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch_word_path_for_deepsignet[0]\n",
    "x[:, :, 0:1].max(1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "80ef3165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12000, 1527])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 0, (10 + 1) :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "41fe2c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12000, 1528])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([x[:, :, 0:1].max(1)[0], x[:, 0, (10 + 1) :]], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc0624c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py38esig)",
   "language": "python",
   "name": "py38esig"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
