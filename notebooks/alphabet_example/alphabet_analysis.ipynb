{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec1fc17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    RobertaConfig,\n",
    "    RobertaForMaskedLM,\n",
    "    RobertaTokenizerFast,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "import tokenizers\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nlpsig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54465a3",
   "metadata": {},
   "source": [
    "## Language dataset\n",
    "\n",
    "In the `data/` folder, we have several text folders of words from different languages:\n",
    "- `wordlist_de.txt`: German words\n",
    "- `wordlist_en.txt`: English words\n",
    "- `wordlist_fr.txt`: French words\n",
    "- `wordlist_it.txt`: Italian words\n",
    "- `wordlist_pl.txt`: Polish words\n",
    "- `wordlist_sv.txt`: Swedish words\n",
    "\n",
    "We additionally have a `alphabet.txt` file which just stores the alphabet characters ('a', 'b', 'c', ...).\n",
    "\n",
    "The task is to split the words into its individual characters and to obtain an embedding for each of them. We can represent a word by a path of its character embeddings and compute its path signature to use as features in predicting the language for which the word belongs.\n",
    "\n",
    "Here we look at obtaining embeddings using a Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b531b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "ALPHABET_FILE = 'data/alphabet.txt'\n",
    "with open(ALPHABET_FILE) as f:\n",
    "    alphabet = f.read().splitlines()\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59499e8",
   "metadata": {},
   "source": [
    "## Set up Tokenizer for word corpora\n",
    "\n",
    "If we were to fine-tune an existing pretrained transformer, we could use the same tokenizer that the model was pretrained with. However, here we want to train a model from stratch, and so using a tokenizer that was pretrained on a corpus that looks quite different to ours is suboptimal.\n",
    "\n",
    "Here, we need to use the `tokenizers` library to set up and train a new tokenizer for our text.\n",
    "\n",
    "In particular, we're going to start off with a character-based tokenizer (as we're going to split up our words into characters), and train it to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45b20eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAR_BERT/vocab.json', 'CHAR_BERT/merges.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialise character based tokenizer\n",
    "tokenizer = tokenizers.CharBPETokenizer()\n",
    "tokenizer.train(files=[ALPHABET_FILE], show_progress=False)\n",
    "\n",
    "# save the tokenizer to \"CHAR_BERT/\" folder\n",
    "if not os.path.exists(\"CHAR_BERT\"):\n",
    "    os.makedirs(\"CHAR_BERT\")\n",
    "\n",
    "tokenizer.save_model(\"CHAR_BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b016043",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist_files = [\"data/wordlist_de.txt\",\n",
    "                  \"data/wordlist_en.txt\",\n",
    "                  \"data/wordlist_fr.txt\",\n",
    "                  \"data/wordlist_it.txt\",\n",
    "                  \"data/wordlist_pl.txt\",\n",
    "                  \"data/wordlist_sv.txt\"]\n",
    "\n",
    "wordlist_dfs = []\n",
    "for filename in wordlist_files:\n",
    "    with open(filename, \"r\") as f:\n",
    "        words = f.read().splitlines()\n",
    "        words_df = pd.DataFrame({\"word\": words})\n",
    "        words_df[\"language\"] = filename.split(\"_\")[1][0:2]\n",
    "        wordlist_dfs.append(words_df)\n",
    "\n",
    "corpus_df = pd.concat(wordlist_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aea1d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aal</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aale</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aalen</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aalend</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77121</th>\n",
       "      <td>zons</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77122</th>\n",
       "      <td>zoo</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77123</th>\n",
       "      <td>zoologisk</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77124</th>\n",
       "      <td>zoologiska</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77125</th>\n",
       "      <td>zoologiskt</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3922535 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word language\n",
       "0               a       de\n",
       "1             aal       de\n",
       "2            aale       de\n",
       "3           aalen       de\n",
       "4          aalend       de\n",
       "...           ...      ...\n",
       "77121        zons       sv\n",
       "77122         zoo       sv\n",
       "77123   zoologisk       sv\n",
       "77124  zoologiska       sv\n",
       "77125  zoologiskt       sv\n",
       "\n",
       "[3922535 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31554556",
   "metadata": {},
   "source": [
    "Question: what do we do with words that appear twice in the data? i.e. different languages have the same word in their vocabularies, e.g. the word \"zoo\" is a valid word in english, french, italian, polish and swedish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecadd708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80613</th>\n",
       "      <td>zoo</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198413</th>\n",
       "      <td>zoo</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861555</th>\n",
       "      <td>zoo</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508829</th>\n",
       "      <td>zoo</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77122</th>\n",
       "      <td>zoo</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word language\n",
       "80613    zoo       en\n",
       "198413   zoo       fr\n",
       "1861555  zoo       it\n",
       "1508829  zoo       pl\n",
       "77122    zoo       sv"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df[corpus_df[\"word\"]==\"zoo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a4aa8e",
   "metadata": {},
   "source": [
    "We take a random sample of the 3.9 million words in the corpora..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2a2b268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unbehelligte</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>herauszuschlagender</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anzahlbarer</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unbeseelte</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>imaginativem</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>ifatt</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>formgavs</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>bestods</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>kurirers</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>satsningars</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      word language\n",
       "0             unbehelligte       de\n",
       "1      herauszuschlagender       de\n",
       "2              anzahlbarer       de\n",
       "3               unbeseelte       de\n",
       "4             imaginativem       de\n",
       "...                    ...      ...\n",
       "11995                ifatt       sv\n",
       "11996             formgavs       sv\n",
       "11997              bestods       sv\n",
       "11998             kurirers       sv\n",
       "11999          satsningars       sv\n",
       "\n",
       "[12000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "seed = 2022\n",
    "random.seed(seed)\n",
    "balanced = True\n",
    "\n",
    "# take \n",
    "n_words = 12000\n",
    "if balanced:\n",
    "    languages = corpus_df[\"language\"].unique()\n",
    "    words_per_language = math.floor(n_words / len(languages))\n",
    "    corpus_sample_df = pd.concat(\n",
    "        [corpus_df[corpus_df[\"language\"]==lang].sample(words_per_language, random_state=seed)\n",
    "         for lang in languages]\n",
    "    )\n",
    "    corpus_sample_df = corpus_sample_df.reset_index(drop=True)\n",
    "else:\n",
    "    corpus_sample_df = corpus_df.iloc[random.sample(range(len(corpus_df)), n_words)]\n",
    "    corpus_sample_df = corpus_sample_df.reset_index(drop=True)\n",
    "\n",
    "corpus_sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f158e33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "de    2000\n",
       "en    2000\n",
       "fr    2000\n",
       "it    2000\n",
       "pl    2000\n",
       "sv    2000\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample_df[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83dffccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "max_length = 512\n",
    "\n",
    "# load in tokenizer for architecture\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('CHAR_BERT/',\n",
    "                                                 max_len=max_length)\n",
    "\n",
    "# set up data_collator to use (intially just one that adds padding)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# initialise transformer architecture (random weights)\n",
    "config_args = {\"vocab_size\": tokenizer.backend_tokenizer.get_vocab_size(),\n",
    "               \"hidden_size\": 768,\n",
    "               \"max_length\": 512,\n",
    "               \"max_position_embeddings\": max_length + 2,\n",
    "               \"hidden_dropout_prob\": 0.1,\n",
    "               \"num_attention_heads\": 12,\n",
    "               \"num_hidden_layers\": 6,\n",
    "               \"type_vocab_size\": 1}\n",
    "\n",
    "config = RobertaConfig(**config_args)\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "059975c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = nlpsig.TextEncoder(df=corpus_sample_df,\n",
    "                                  feature_name=\"word\",\n",
    "                                  model=model,\n",
    "                                  config=config,\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63f5df2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting return_special_tokens_mask=True\n",
      "[INFO] Tokenizing the datatset...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f74faf2a6ac46fb84fbb88f9b75568a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving the tokenized text for each sentence into `.df['tokens']`...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e172c2385b4343ff8aa043bb4c7577a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating tokenized dataframe and setting in `.tokenized_df` attribute...\n",
      "[INFO] Note: 'text_id' is the column name for denoting the corresponding text id\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['word', 'language', 'input_ids', 'attention_mask', 'special_tokens_mask', 'tokens'],\n",
       "    num_rows: 12000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokenize_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c202bfa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unbehelligte'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.dataset[\"word\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce0c4bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53, 21, 14, 2, 5, 8, 5, 12, 12, 9, 7, 20, 5, 54]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.dataset[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e42a263d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u', 'n', 'b', 'e', 'h', 'e', 'l', 'l', 'i', 'g', 't', 'e']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.dataset[\"tokens\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab8a93a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130015</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130016</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130017</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130018</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130019</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130020 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id language tokens\n",
       "0             0       de      u\n",
       "1             0       de      n\n",
       "2             0       de      b\n",
       "3             0       de      e\n",
       "4             0       de      h\n",
       "...         ...      ...    ...\n",
       "130015    11999       sv      n\n",
       "130016    11999       sv      g\n",
       "130017    11999       sv      a\n",
       "130018    11999       sv      r\n",
       "130019    11999       sv      s\n",
       "\n",
       "[130020 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokenized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "233a829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                               | 0/120 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:45<00:00,  2.62it/s]\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = text_encoder.obtain_embeddings(method = \"hidden_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d140f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130020, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e854f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 12000\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c41750a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12000/12000 [00:01<00:00, 6156.56it/s]\n"
     ]
    }
   ],
   "source": [
    "pooled_embeddings = text_encoder.pool_token_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d917b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 768)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eb6237",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cab237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data_collator for language modelling (has dynamic padding)\n",
    "data_collator_for_LM = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "                                                       mlm=True,\n",
    "                                                       mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e7f7826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Splitting up dataset into train / validation / test sets, and saving to `.dataset_split`.\n",
      "[INFO] Setting up TrainingArguments object and saving to `.training_args`.\n",
      "[INFO] Setting up Trainer object, and saving to `.trainer`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x2cf48f6d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.split_dataset()\n",
    "text_encoder.set_up_training_args(output_dir=\"CHAR_BERT_trained\",\n",
    "                                  num_train_epochs=20,\n",
    "                                  per_device_train_batch_size=128,\n",
    "                                  seed=seed)\n",
    "text_encoder.set_up_trainer(data_collator=data_collator_for_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bf2b761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, language, special_tokens_mask, tokens. If word, language, special_tokens_mask, tokens are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/Users/rchan/opt/miniconda3/envs/py38esig/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 120000\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 46900\n",
      "  Number of trainable parameters = 43560249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training model with 43560249 parameters...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='46900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   10/46900 00:09 < 15:15:37, 0.85 it/s, Epoch 0.01/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtext_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transformer_with_trainer_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/rough_paths/path_signatures_bert/nlpsig/encode_text.py:1209\u001b[0m, in \u001b[0;36mTextEncoder.fit_transformer_with_trainer_api\u001b[0;34m(self, output_dir, data_collator, compute_metrics, training_args, trainer_args)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_up_trainer(\n\u001b[1;32m   1204\u001b[0m         data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m   1205\u001b[0m         compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrainer_args,\n\u001b[1;32m   1207\u001b[0m     )\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Training model with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_parameters()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m parameters...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1209\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Training completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py38esig/lib/python3.8/site-packages/transformers/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1498\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py38esig/lib/python3.8/site-packages/transformers/trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1752\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1754\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1755\u001b[0m ):\n\u001b[1;32m   1756\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py38esig/lib/python3.8/site-packages/transformers/trainer.py:2526\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2524\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2526\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py38esig/lib/python3.8/site-packages/torch/_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    248\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    249\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    254\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 255\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py38esig/lib/python3.8/site-packages/torch/autograd/__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 147\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text_encoder.fit_transformer_with_trainer_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f252267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to CHAR_BERT_trained/\n",
      "Configuration saved in CHAR_BERT_trained/config.json\n",
      "Model weights saved in CHAR_BERT_trained/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "text_encoder.trainer.save_model(\"CHAR_BERT_trained/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2840f8a1",
   "metadata": {},
   "source": [
    "## Evaluating trained model\n",
    "\n",
    "Evaluating the performance on predicting the masked letter for the test dataset. To do this, for each word in our test dataset, we will mask each letter on its own and ask the model to predict the masked letter. So for a 5 letter word, we have 5 predictions to make - one for each letter given the other letters.\n",
    "\n",
    "For our tokenizer, we see that \"\\<mask>\" is used as the mask token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccfb2bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66c9ffa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file CHAR_BERT_trained/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"CHAR_BERT_trained\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 57\n",
      "}\n",
      "\n",
      "loading configuration file CHAR_BERT_trained/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"CHAR_BERT_trained\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 57\n",
      "}\n",
      "\n",
      "loading weights file CHAR_BERT_trained/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at CHAR_BERT_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with 15000 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15000/15000 [1:16:58<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4963931386184516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4963931386184516"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_masked_character_accuracy(fill_mask, words):\n",
    "    was_correct = []\n",
    "    print(f\"Evaluating with {len(words)} words\")\n",
    "    for word in tqdm(words):\n",
    "        masked_strings = [word[:i] + '<mask>' + word[i+1:] for i in range(len(word))]\n",
    "        predictions = [fill_mask(word)[0]['sequence'] for word in masked_strings]\n",
    "        was_correct += [pred == word for pred in predictions]\n",
    "    \n",
    "    acc = np.sum(was_correct) / len(was_correct)\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    return acc\n",
    "\n",
    "fill_mask = pipeline(\"fill-mask\",\n",
    "                     model=\"CHAR_BERT_trained\",\n",
    "                     tokenizer=\"CHAR_BERT_trained\")\n",
    "\n",
    "compute_masked_character_accuracy(fill_mask, \n",
    "                                  text_encoder.dataset_split[\"test\"][\"word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa52b9a3",
   "metadata": {},
   "source": [
    "## Obtaining a path for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7304560a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130015</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130016</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130017</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130018</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130019</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130020 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id language tokens\n",
       "0             0       de      u\n",
       "1             0       de      n\n",
       "2             0       de      b\n",
       "3             0       de      e\n",
       "4             0       de      h\n",
       "...         ...      ...    ...\n",
       "130015    11999       sv      n\n",
       "130016    11999       sv      g\n",
       "130017    11999       sv      a\n",
       "130018    11999       sv      r\n",
       "130019    11999       sv      s\n",
       "\n",
       "[130020 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokenized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b470c209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130020, 768)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1a8ccd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class PrepareData:\n",
    "    \"\"\"\n",
    "    Class to prepare dataset for computing signatures.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_df: pd.DataFrame,\n",
    "        embeddings: np.array,\n",
    "        embeddings_reduced: Optional[np.array] = None,\n",
    "        pooled_embeddings: Optional[np.array] = None,\n",
    "        id_column: Optional[str] = None,\n",
    "        labels_column: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Class to prepare dataset for computing signatures.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        original_df : pd.DataFrame\n",
    "            Dataset as a pandas dataframe.\n",
    "        embeddings : np.array\n",
    "            Embeddings for each of the items in `original_df`.\n",
    "        embeddings_reduced : Optional[np.array], optional\n",
    "            Dimension reduced embeddings, by default None.\n",
    "        pooled_embeddings : Optional[np.array], optional\n",
    "            Pooled embeddings for each unique id in `id_column`, by default None.\n",
    "        id_column : Optional[str]\n",
    "            Name of the column which identifies each of the text, e.g.\n",
    "            - \"text_id\" (if each item in `original_df` is a word or sentence from a particular text),\n",
    "            - \"user_id\" (if each item in `original_df` is a post from a particular user)\n",
    "            - \"timeline_id\" (if each item in `original_df` is a post from a particular time)\n",
    "            If None, it will create a dummy id_column named \"dummy_id\" and fill with zeros.\n",
    "        labels_column : Optional[str]\n",
    "            Name of the column which are corresponds to the labels of the data.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            if `original_df` and `embeddings` does not have the same number of rows.\n",
    "        ValueError\n",
    "            if `original_df` and `embeddings_reduced` does not have the same number of rows\n",
    "            (if `embeddings_reduced` is provided).\n",
    "        \"\"\"\n",
    "        # perform checks that original_df have the right column names to work with\n",
    "        if embeddings.ndim != 2:\n",
    "            raise ValueError(\"`embeddings` should be a 2-dimensional array.\")\n",
    "        if original_df.shape[0] != embeddings.shape[0]:\n",
    "            raise ValueError(\n",
    "                \"`original_df` and `embeddings` should have the same number of rows.\"\n",
    "            )\n",
    "        if embeddings_reduced is not None:\n",
    "            if embeddings_reduced.ndim != 2:\n",
    "                raise ValueError(\"If provided, `embeddings_reduced` should be a 2-dimensional array.\")\n",
    "            if original_df.shape[0] != embeddings_reduced.shape[0]:\n",
    "                raise ValueError(\n",
    "                    \"`original_df`, `embeddings` and `embeddings_reduced` \"\n",
    "                    \"should have the same number of rows\"\n",
    "                )\n",
    "        self.original_df: pd.DataFrame = original_df\n",
    "        self.id_column: Optional[str] = id_column\n",
    "        self.label_column: Optional[str] = labels_column\n",
    "        # set embeddings\n",
    "        self.embeddings: np.array = embeddings\n",
    "        self.embeddings_reduced: Optional[np.array] = embeddings_reduced\n",
    "        # obtain modelling dataframe\n",
    "        self.df: Optional[pd.DataFrame] = None\n",
    "        self.df = self._get_modeling_dataframe()\n",
    "        # set pooled embeddings if provided\n",
    "        if pooled_embeddings is not None:\n",
    "            if pooled_embeddings.ndim != 2:\n",
    "                raise ValueError(\"If provided, `pooled_embeddings` should be a 2-dimensional array.\")\n",
    "            if len(self.df[self.id_column].unique()) != pooled_embeddings.shape[0]:\n",
    "                raise ValueError(\n",
    "                    \"If  provided, `pooled_embeddings` should have the same number of \"\n",
    "                    \"rows as there are different ids, i.e. we should have \"\n",
    "                    \"`len(self.df[self.id_column].unique()) != pooled_embeddings.shape[0]`.\"\n",
    "                )\n",
    "        self.pooled_embeddings: Optional[np.array] = pooled_embeddings\n",
    "        # obtain time features\n",
    "        self._time_feature_choices: List[str] = []\n",
    "        self.time_features_added: bool = False\n",
    "        self.df = self._set_time_features()\n",
    "        self.df_padded: Optional[pd.DataFrame] = None\n",
    "        self.array_padded: Optional[np.array] = None\n",
    "        # record method for creating the path\n",
    "        self.pad_method = None\n",
    "\n",
    "    def _get_modeling_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        [Private] Combines `.original_df` with the sentence\n",
    "        embeddings and the dimension reduced embeddings\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Original dataframe concatenated with the embeddings and\n",
    "            dimension reduced embeddings (column-wise)\n",
    "            - columns starting with \"e\" followed by a number denotes each\n",
    "              dimension of the embeddings\n",
    "            - columns starting with \"d\" followed by a number denotes each\n",
    "              dimension of the dimension reduced embeddings\n",
    "        \"\"\"\n",
    "        if self.df is not None:\n",
    "            return self.df\n",
    "        else:\n",
    "            print(\"[INFO] Concatenating the embeddings to the dataframe...\")\n",
    "            print(\"[INFO] - columns beginning with 'e' denote the full embddings.\")\n",
    "            embedding_df = pd.DataFrame(\n",
    "                self.embeddings,\n",
    "                columns=[f\"e{i+1}\" for i in range(self.embeddings.shape[1])],\n",
    "            )\n",
    "            if self.embeddings_reduced is not None:\n",
    "                print(\n",
    "                    \"[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\"\n",
    "                )\n",
    "                embeddings_reduced_df = pd.DataFrame(\n",
    "                    self.embeddings_reduced,\n",
    "                    columns=[\n",
    "                        f\"d{i+1}\" for i in range(self.embeddings_reduced.shape[1])\n",
    "                    ],\n",
    "                )\n",
    "                df = pd.concat(\n",
    "                    [\n",
    "                        self.original_df.reset_index(drop=True),\n",
    "                        embeddings_reduced_df,\n",
    "                        embedding_df,\n",
    "                    ],\n",
    "                    axis=1,\n",
    "                )\n",
    "            else:\n",
    "                df = pd.concat(\n",
    "                    [self.original_df.reset_index(drop=True), embedding_df],\n",
    "                    axis=1,\n",
    "                )\n",
    "            if self.id_column is None:\n",
    "                self.id_column = \"dummy_id\"\n",
    "                print(\n",
    "                    f\"[INFO] No id_column was passed, so setting id_column to '{self.id_column}'.\"\n",
    "                )\n",
    "            if self.id_column not in self.original_df.columns:\n",
    "                # set default value to id_column\n",
    "                print(\n",
    "                    f\"[INFO] There is no column in `.original_df` called '{self.id_column}'. \"\n",
    "                    \"Adding a new column named '{self.id_column}' of zeros.\"\n",
    "                )\n",
    "                df[self.id_column] = 0\n",
    "            return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _time_fraction(x: pd.Timestamp) -> float:\n",
    "        \"\"\"\n",
    "        [Private] Converts a date, x, as a fraction of the year.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : pd.Timestamp\n",
    "            Date.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The date as a fraction of the year.\n",
    "        \"\"\"\n",
    "        # compute how many seconds the date is into the year\n",
    "        x_year_start = pd.Timestamp(x.year, 1, 1)\n",
    "        seconds_into_cal_year = abs(x - x_year_start).total_seconds()\n",
    "        # compute the time fraction into the year\n",
    "        time_frac = seconds_into_cal_year / (365 * 24 * 60 * 60)\n",
    "        return x.year + time_frac\n",
    "\n",
    "    def _set_time_features(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        [Private] Updates the dataframe in `.df` to include time features:\n",
    "        - `time_encoding`: the date as a fraction of the year\n",
    "           (only if 'datetime' is a column in `.df` dataframe).\n",
    "        - `time_diff`: the difference in time (in minutes) between successive records\n",
    "           (only if 'datetime' is a column in `.df` dataframe).\n",
    "        - `timeline_index`: the index of each post for each id.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Updated dataframe with time features.\n",
    "        \"\"\"\n",
    "        if self.time_features_added:\n",
    "            print(\"Time features have already been added.\")\n",
    "            return\n",
    "        print(\"[INFO] Adding time feature columns into dataframe in `.df`.\")\n",
    "        if \"datetime\" in self.df.columns:\n",
    "            self._time_feature_choices += [\"time_encoding\", \"time_diff\"]\n",
    "\n",
    "            # checking 'datetime' column is datatime type\n",
    "            self.df[\"datetime\"] = pd.to_datetime(self.df[\"datetime\"])\n",
    "\n",
    "            # obtain time encoding by computing the fraction of year it is in\n",
    "            print(\"[INFO] Adding 'time_encoding' and feature...\")\n",
    "            self.df[\"time_encoding\"] = self.df[\"datetime\"].map(\n",
    "                lambda t: self._time_fraction(t)\n",
    "            )\n",
    "            # sort by the id and the date\n",
    "            self.df = self.df.sort_values(by=[self.id_column, \"datetime\"]).reset_index(\n",
    "                drop=True\n",
    "            )\n",
    "\n",
    "            # calculate time difference between posts\n",
    "            print(\"[INFO] Adding 'time_diff' and feature...\")\n",
    "            self.df[\"time_diff\"] = list(\n",
    "                self.df.groupby(\"timeline_id\")\n",
    "                .apply(\n",
    "                    lambda x: [0.0]\n",
    "                    + [\n",
    "                        (\n",
    "                            x[\"datetime\"].iloc[i] - x[\"datetime\"].iloc[i - 1]\n",
    "                        ).total_seconds()\n",
    "                        / 60\n",
    "                        for i in range(1, len(x))\n",
    "                    ]\n",
    "                )\n",
    "                .explode()\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                \"[INFO] Note 'datetime' is not a column in `.df`, \"\n",
    "                \"so only 'timeline_index' is added.\"\n",
    "            )\n",
    "            print(\n",
    "                \"[INFO] As 'datetime' is not a column in `.df`, \"\n",
    "                \"we assume that the data is ordered by time with respect to the id.\"\n",
    "            )\n",
    "        # assign index for each post in each timeline\n",
    "        self._time_feature_choices += [\"timeline_index\"]\n",
    "\n",
    "        print(\"[INFO] Adding 'timeline_index' feature...\")\n",
    "        self.df[\"timeline_index\"] = list(\n",
    "            self.df.groupby(self.id_column)\n",
    "            .apply(lambda x: list(range(len(x))))\n",
    "            .explode()\n",
    "        )\n",
    "        self.time_features_added = True\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def _obtain_colnames(self, embeddings: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        [Private] Obtains the column names storing the embeddings.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        embeddings : str\n",
    "            Options are:\n",
    "            - \"dim_reduced\": dimension reduced embeddings.\n",
    "            - \"full\": full embeddings.\n",
    "            - \"both\": concatenation of dimension reduced and full embeddings.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "            List of column names which store the embeddings.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            if embeddings is not either of 'dim_reduced', 'full', or 'both'.\n",
    "        \"\"\"\n",
    "        if embeddings not in [\"dim_reduced\", \"full\", \"both\"]:\n",
    "            raise ValueError(\n",
    "                \"Embeddings must be either 'dim_reduced', 'full', or 'both'\"\n",
    "            )\n",
    "        if embeddings == \"dim_reduced\":\n",
    "            # obtain columns for the dimension reduced embeddings\n",
    "            # these are columns which start with 'd' and have a number following it\n",
    "            colnames = [col for col in self.df.columns if re.match(r\"^d\\w*[0-9]\", col)]\n",
    "        elif embeddings == \"full\":\n",
    "            # obtain columns for the full embeddings\n",
    "            # these are columns which start with 'e' and have a number following it\n",
    "            colnames = [col for col in self.df.columns if re.match(r\"^e\\w*[0-9]\", col)]\n",
    "        elif embeddings == \"both\":\n",
    "            # add columns for the embeddings\n",
    "            colnames = [col for col in self.df.columns if re.match(r\"^d\\w*[0-9]\", col)]\n",
    "            colnames += [col for col in self.df.columns if re.match(r\"^e\\w*[0-9]\", col)]\n",
    "        return colnames\n",
    "\n",
    "    def _obtain_time_feature_columns(\n",
    "        self,\n",
    "        time_feature: Optional[Union[List[str], str]],\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        [Private] Obtains the column names storing the time features requested.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        time_feature : Optional[Union[List[str], str]]\n",
    "            If is a string, it must be the list found in\n",
    "            `_time_feature_choices` attribute. If is a list,\n",
    "            each item must be a string and it must be in the\n",
    "            list found in `_time_feature_choices` attribute.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "            List of column names which store the time features.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            if `time_feature` is a string, and it is not found in `_time_feature_choices`.\n",
    "        ValueError\n",
    "            if `time_feature` is a list of strings, and one of the items\n",
    "            is not found in `_time_feature_choices`.\n",
    "        \"\"\"\n",
    "        if time_feature is None:\n",
    "            time_feature = []\n",
    "        else:\n",
    "            if not self.time_features_added:\n",
    "                self.set_time_features()\n",
    "            if isinstance(time_feature, str):\n",
    "                if time_feature not in self._time_feature_choices:\n",
    "                    raise ValueError(\n",
    "                        \"If `time_feature` is a string, it must \"\n",
    "                        + f\"be in {self._time_feature_choices}.\"\n",
    "                    )\n",
    "                else:\n",
    "                    time_feature = [time_feature]\n",
    "            elif isinstance(time_feature, list):\n",
    "                if not all(\n",
    "                    [item in self._time_feature_choices for item in time_feature]\n",
    "                ):\n",
    "                    raise ValueError(\n",
    "                        f\"Each item in   should be in {self._time_feature_choices}.\"\n",
    "                    )\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"`time_feature` must be either None, a string, or a list of strings.\"\n",
    "                )\n",
    "        return time_feature\n",
    "\n",
    "    def _pad_dataframe(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        k: int,\n",
    "        padding_n: int,\n",
    "        zero_padding: bool,\n",
    "        colnames: List[str],\n",
    "        time_feature: List[str],\n",
    "        id: int,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        [Private] If `padding_n > 0`, we pad `padding_n` number of entries\n",
    "        to the dataframe (either by zeros if `zero_padding==True`, or by the last post\n",
    "        in df if `zero_padding==False`). If `padding_n <= 0`, we don't need to pad\n",
    "        and we simply return the last `k` entries (throws error if `k` is less than number\n",
    "        of entries in `.df`).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Dataframe to pad with.\n",
    "        k : int\n",
    "            Number of items to keep.\n",
    "        padding_n : int\n",
    "            Number of entries to pad.\n",
    "        zero_padding : bool\n",
    "            If True, will pad with zeros. Otherwise, pad with the latest\n",
    "            text associated to the id.\n",
    "        colnames : List[str]\n",
    "            List of column names that we wish to keep from the dataframe.\n",
    "        time_feature : List[str]\n",
    "            List of time feature column names that we wish to keep from the dataframe.\n",
    "        id : int\n",
    "            Which id are we padding.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Padded dataframe.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            if k is not a positive integer.\n",
    "        ValueError\n",
    "            if padding_n is less than or equal to zero, but there aren't enough entries\n",
    "            in `df` to take the last `k` entries.\n",
    "        \"\"\"\n",
    "        if k < 0:\n",
    "            raise ValueError(\"`k` must be a positive integer\")\n",
    "        columns = time_feature + colnames + [self.id_column]\n",
    "        if self.label_column is not None:\n",
    "            columns += [self.label_column]\n",
    "        if padding_n > 0:\n",
    "            # need to pad to fill up\n",
    "            if zero_padding or len(df) == 0:\n",
    "                # pad by having zero entries\n",
    "                if self.label_column is not None:\n",
    "                    # set labels to be -1 to indicate that they're padded values\n",
    "                    data_dict = {\n",
    "                        **dict.fromkeys(time_feature, [0]),\n",
    "                        **{c: [0] for c in colnames},\n",
    "                        **{self.id_column: [id], self.label_column: [-1]},\n",
    "                    }\n",
    "                else:\n",
    "                    # no label column to add\n",
    "                    data_dict = {\n",
    "                        **dict.fromkeys(time_feature, [0]),\n",
    "                        **{c: [0] for c in colnames},\n",
    "                        **{self.id_column: [id]},\n",
    "                    }\n",
    "                df_padded = pd.concat(\n",
    "                    [\n",
    "                        pd.concat([pd.DataFrame(data_dict)] * padding_n),\n",
    "                        df[columns],\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                # pad by repeating the latest text\n",
    "                latest_text = df[columns].tail(1)\n",
    "                df_padded = pd.concat(\n",
    "                    [\n",
    "                        df[columns],\n",
    "                        pd.concat([latest_text] * padding_n),\n",
    "                    ]\n",
    "                )\n",
    "            return df_padded.reset_index(drop=True)\n",
    "        else:\n",
    "            if len(df) < k:\n",
    "                raise ValueError(\n",
    "                    \"Requested to not pad, but there aren't enough entries in `df`.\"\n",
    "                )\n",
    "            return df[columns].tail(k).reset_index(drop=True)\n",
    "\n",
    "    def _pad_id(\n",
    "        self,\n",
    "        k: int,\n",
    "        zero_padding: bool,\n",
    "        colnames: List[str],\n",
    "        id_counts: pd.Series,\n",
    "        id: int,\n",
    "        time_feature: List[str],\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        [Private] For a given id, the function slices the dataframe in .df\n",
    "        by finding those with id_column == id and keeping only the columns\n",
    "        found in colnames.\n",
    "        The function returns a dataframe with k rows:\n",
    "        - If the number of records with id_column == id is less than k, it \"pads\" the\n",
    "        dataframe by adding in empty records (with label = -1 to indicate they're padded).\n",
    "        - If the number of records with id_column == id is equal to k, it just returns\n",
    "        the records with id_column == id.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        k : int\n",
    "            Number of items to keep.\n",
    "        zero_padding : bool\n",
    "            If True, will pad with zeros. Otherwise, pad with the latest\n",
    "            text associated to the id.\n",
    "        colnames : List[str]\n",
    "            List of column names that we wish to keep from the dataframe.\n",
    "        id_counts : pd.Series\n",
    "            The number of records in associated to each id_column.\n",
    "        id : int\n",
    "            Which id are we padding.\n",
    "        time_feature : List[str]\n",
    "            List of time feature column names that we wish to keep from the dataframe.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Padded dataframe for a particular id.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            if k is not a positive integer.\n",
    "        \"\"\"\n",
    "        if k < 0:\n",
    "            raise ValueError(\"`k` must be a positive integer\")\n",
    "        history = self.df[self.df[self.id_column] == id]\n",
    "        padding_n = k - id_counts[id]\n",
    "        return self._pad_dataframe(\n",
    "            df=history,\n",
    "            k=k,\n",
    "            padding_n=padding_n,\n",
    "            zero_padding=zero_padding,\n",
    "            colnames=colnames,\n",
    "            time_feature=time_feature,\n",
    "            id=id,\n",
    "        )\n",
    "\n",
    "    def _pad_history(\n",
    "        self,\n",
    "        k: int,\n",
    "        zero_padding: bool,\n",
    "        colnames: List[str],\n",
    "        index: int,\n",
    "        time_feature: List[str],\n",
    "        include_current_embedding: bool,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        [Private]\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        k : int\n",
    "            Number of items to keep.\n",
    "        zero_padding : bool\n",
    "            If True, will pad with zeros. Otherwise, pad with the latest\n",
    "            text associated to the id.\n",
    "        colnames : List[str]\n",
    "            List of column names that we wish to keep from the dataframe.\n",
    "        index : int\n",
    "            Which index of the dataframe are we padding.\n",
    "        time_feature : List[str]\n",
    "            List of time feature column names that we wish to keep from the dataframe.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Padded dataframe for a particular index of the dataframe by looking\n",
    "            at the previous texts of a particular id.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            if k is not a positive integer.\n",
    "        ValueError\n",
    "            if index is outside the range of indicies of the dataframe ([0, 1, ..., len(.df)]).\n",
    "        \"\"\"\n",
    "        if k < 0:\n",
    "            raise ValueError(\"`k` must be a positive integer.\")\n",
    "        if index not in range(len(self.df)):\n",
    "            raise ValueError(\"`index` is outside of [0, 1, ..., len(.df)].\")\n",
    "        # look at particular text at a given index\n",
    "        text = self.df.iloc[index]\n",
    "        id = text[self.id_column]\n",
    "        timeline_index = text[\"timeline_index\"]\n",
    "        # obtain history for the piece of text\n",
    "        if include_current_embedding:\n",
    "            history = self.df[\n",
    "                (self.df[self.id_column] == id)\n",
    "                & (self.df[\"timeline_index\"] <= timeline_index)\n",
    "            ]\n",
    "        else:\n",
    "            history = self.df[\n",
    "                (self.df[self.id_column] == id)\n",
    "                & (self.df[\"timeline_index\"] < timeline_index)\n",
    "            ]\n",
    "        padding_n = k - len(history)\n",
    "        return self._pad_dataframe(\n",
    "            df=history,\n",
    "            k=k,\n",
    "            padding_n=padding_n,\n",
    "            zero_padding=zero_padding,\n",
    "            colnames=colnames,\n",
    "            time_feature=time_feature,\n",
    "            id=id,\n",
    "        )\n",
    "\n",
    "    def pad(\n",
    "        self,\n",
    "        pad_by: str,\n",
    "        method: str = \"k_last\",\n",
    "        zero_padding: bool = True,\n",
    "        k: int = 5,\n",
    "        time_feature: Optional[Union[List[str], str]] = None,\n",
    "        standardise_time_feature: bool = True,\n",
    "        standardise_method: str = \"standardise\",\n",
    "        embeddings: str = \"full\",\n",
    "        include_current_embedding: bool = False,\n",
    "    ) -> np.array:\n",
    "        \"\"\"\n",
    "        Creates an array which stores the path.\n",
    "        We create a path for each id in id_column if `pad_by=\"id\"`\n",
    "        (by constructing a path of the embeddings of the texts associated to each id),\n",
    "        or for each item in `.df` if `pad_by=\"history\"`\n",
    "        (by constructing a path of the embeddings of the previous texts).\n",
    "\n",
    "        We can decide how long our path is by letting `method=\"k_last` and specifying `k`.\n",
    "        Alternatively, we can set `method=\"max\"`, which sets the length of the path\n",
    "        by setting `k` to be the largest number of texts associated to an individual id.\n",
    "\n",
    "        The function \"pads\" if there aren't enough texts to fill in (e.g. if requesting for\n",
    "        the last 5 posts for an id, but there are less than 5 posts available),\n",
    "        by adding empty records (if `zero_padding=True`)\n",
    "        or by the last previous text (if `zero_padding=False`). This ensures that each\n",
    "        path has the same number of data points.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pad_by : str\n",
    "            How to construct the path. Options are:\n",
    "            - \"id\": constructs a path of the embeddings of the texts associated to each id\n",
    "            - \"history\": constructs a path by looking at the embeddings of the previous texts\n",
    "              for each text\n",
    "        method : str\n",
    "            How long the path is, default \"k_last\". Options are:\n",
    "            - \"k_last\": specifying the length of the path through the choice of `k` (see below)\n",
    "            - \"max\": the length of the path is chosen by looking at the largest number\n",
    "              of texts associated to an individual id in `.id_column`\n",
    "        zero_padding : bool\n",
    "            If True, will pad with zeros. Otherwise, pad with the latest\n",
    "            text associated to the id.\n",
    "        k : int\n",
    "            The requested length of the path, default 5. This is ignored if `method=\"max\"`.\n",
    "        time_feature : Optional[Union[List[str], str]]\n",
    "            Which time feature(s) to keep. If None, then doesn't keep any.\n",
    "        embeddings : str, optional\n",
    "            Which embeddings to keep, by default \"full\". Options:\n",
    "            - \"dim_reduced\": dimension reduced embeddings\n",
    "            - \"full\": full embeddings\n",
    "            - \"both\": keeps both dimension reduced and full embeddings\n",
    "        include_current_embedding : bool, optional\n",
    "            If `pad_by=\"history\", this determines whether or not the embedding for the\n",
    "            text is included in it's history, by default False. If `pad_by=\"id\"`,\n",
    "            this argument is ignored.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            3 dimension array of the path:\n",
    "            - First dimension is ids (if `pad_by=\"id\"`)\n",
    "              or each text (if `pad_by=\"history\"`)\n",
    "            - Second dimension is the associated texts\n",
    "            - Third dimension are the features (e.g. embeddings /\n",
    "              dimension reduced embeddings, time features)\n",
    "        \"\"\"\n",
    "        print(\n",
    "            \"[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\"\n",
    "        )\n",
    "        if pad_by not in [\"id\", \"history\"]:\n",
    "            raise ValueError(\"`pad_by` must be either 'id' or 'history'.\")\n",
    "        # obtain id_column counts\n",
    "        id_counts = self.df[self.id_column].value_counts(sort=False)\n",
    "        # determine padding length\n",
    "        if method == \"k_last\":\n",
    "            # use k that was passed in\n",
    "            pass\n",
    "        elif method == \"max\":\n",
    "            # let k be the largest number of items associated to an id\n",
    "            k = id_counts.max()\n",
    "        else:\n",
    "            raise ValueError(\"Method must be either 'k_last' or 'max'.\")\n",
    "        # obtain time feature colnames\n",
    "        time_feature_colnames = self._obtain_time_feature_columns(\n",
    "            time_feature=time_feature\n",
    "        )\n",
    "        # obtain colnames of embeddings\n",
    "        colnames = self._obtain_colnames(embeddings=embeddings)\n",
    "        if pad_by == \"id\":\n",
    "            # pad each of the ids in id_column and store them in a list\n",
    "            self.pad_method = \"id\"\n",
    "            padded_dfs = [\n",
    "                self._pad_id(\n",
    "                    k=k,\n",
    "                    zero_padding=zero_padding,\n",
    "                    colnames=colnames,\n",
    "                    id_counts=id_counts,\n",
    "                    id=id,\n",
    "                    time_feature=time_feature_colnames,\n",
    "                )\n",
    "                for id in tqdm(id_counts.index)\n",
    "            ]\n",
    "            self.df_padded = pd.concat(padded_dfs).reset_index(drop=True)\n",
    "            if standardise_time_feature:\n",
    "                # standardises the time features in .df_padded\n",
    "                for tf in time_feature_colnames:\n",
    "                    self.df_padded[tf] = self._standardise_pd(\n",
    "                        vec=self.df_padded[tf], method=standardise_method\n",
    "                    )\n",
    "            self.array_padded = np.array(self.df_padded).reshape(\n",
    "                len(id_counts), k, len(self.df_padded.columns)\n",
    "            )\n",
    "        elif pad_by == \"history\":\n",
    "            # pad each of the ids in id_column and store them in a list\n",
    "            self.pad_method = \"history\"\n",
    "            padded_dfs = [\n",
    "                self._pad_history(\n",
    "                    k=k,\n",
    "                    zero_padding=zero_padding,\n",
    "                    colnames=colnames,\n",
    "                    index=index,\n",
    "                    time_feature=time_feature_colnames,\n",
    "                    include_current_embedding=include_current_embedding,\n",
    "                )\n",
    "                for index in tqdm(range(len(self.df)))\n",
    "            ]\n",
    "            self.df_padded = pd.concat(padded_dfs).reset_index(drop=True)\n",
    "            if standardise_time_feature:\n",
    "                # standardises the time features in .df_padded\n",
    "                for tf in time_feature_colnames:\n",
    "                    self.df_padded[tf] = self._standardise_pd(\n",
    "                        vec=self.df_padded[tf], method=standardise_method\n",
    "                    )\n",
    "            self.array_padded = np.array(self.df_padded).reshape(\n",
    "                len(self.df), k, len(self.df_padded.columns)\n",
    "            )\n",
    "        return self.array_padded\n",
    "\n",
    "    @staticmethod\n",
    "    def _standardise_pd(vec: pd.Series, method: str) -> pd.Series:\n",
    "        # standardised pandas series\n",
    "        if method == \"standardise\":\n",
    "            return (vec - vec.mean()) / vec.std()\n",
    "        elif method == \"normalise\":\n",
    "            return vec / vec.sum()\n",
    "        else:\n",
    "            raise ValueError(\"Method must be either 'standardise' or 'normalise'.\")\n",
    "\n",
    "    def get_torch_time_feature(\n",
    "        self,\n",
    "        time_feature: str = \"timeline_index\",\n",
    "        standardise: bool = True,\n",
    "        standardise_method: str = \"standardise\",\n",
    "    ) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Returns a `torch.tensor` object of the time_feature that is requested\n",
    "        (the string passed has to be one of the strings in `._time_feature_choices`).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        time_feature : str, optional\n",
    "            Which time feature to obtain `torch.tensor` for, by default \"timeline_index\".\n",
    "        standardise : bool, optional\n",
    "            Whether or not to standardise the time feature, by default True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            Time feature.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            if `time_feature` is not in the possible time_features\n",
    "            (can be found in `._time_feature_choices` attribute).\n",
    "        \"\"\"\n",
    "        if time_feature not in self._time_feature_choices:\n",
    "            raise ValueError(\n",
    "                f\"`time_feature` should be in {self._time_feature_choices}\"\n",
    "            )\n",
    "        if not self.time_features_added:\n",
    "            self.set_time_features()\n",
    "        if standardise:\n",
    "            feature = self._standardise_pd(\n",
    "                vec=self.df[time_feature], method=standardise_method\n",
    "            )\n",
    "            return torch.tensor(feature)\n",
    "        else:\n",
    "            return torch.tensor(self.df[time_feature])\n",
    "\n",
    "    def get_torch_path(self,\n",
    "                       include_time_features: bool = True) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Returns a torch.tensor object of the path.\n",
    "        Includes the time features by default (if they are present after the padding).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        include_time_features : bool, optional\n",
    "            Whether or not to keep the time features, by default True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            Path.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            if `self.array_padded` is `None`. In this case, need to call `.pad()` first.\n",
    "        \"\"\"\n",
    "        if self.array_padded is None:\n",
    "            raise ValueError(\"Need to first call to create the path `.pad()`.\")\n",
    "        # first strip away the id_column and label_column (if exists)\n",
    "        if self.label_column is not None:\n",
    "            # remove last two columns in the third dimension\n",
    "            # (which store id_column and label_column)\n",
    "            path = torch.from_numpy(self.array_padded[:, :, :-2].astype(\"float\"))\n",
    "        else:\n",
    "            # there are no labels, so just remove last column in third dimension\n",
    "            # (which stores id_column)\n",
    "            path = torch.from_numpy(self.array_padded[:, :, :-1].astype(\"float\"))\n",
    "        if not include_time_features:\n",
    "            # computes how many time features there are currently\n",
    "            # (which occur in the first n_time_features columns)\n",
    "            n_time_features = len(\n",
    "                [item for item in self._time_feature_choices if item in self.df_padded]\n",
    "            )\n",
    "            # removes any time features (if they're present)\n",
    "            path = path[:, :, n_time_features:]\n",
    "        return path\n",
    "\n",
    "    def get_torch_embeddings(self, reduced_embeddings: bool = False) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Returns a `torch.tensor` object of the the embeddings.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reduced_embeddings : bool, optional\n",
    "            If True, returns `torch.tensor` of dimension reduced embeddings,\n",
    "            by default False.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            Embeddings.\n",
    "        \"\"\"\n",
    "        if reduced_embeddings:\n",
    "            if self.embeddings_reduced is None:\n",
    "                raise ValueError(\n",
    "                    \"There were no reduced embeddings passed into the class.\"\n",
    "                )\n",
    "            else:\n",
    "                colnames = [\n",
    "                    col for col in self.df.columns if re.match(r\"^d\\w*[0-9]\", col)\n",
    "                ]\n",
    "        else:\n",
    "            colnames = [col for col in self.df.columns if re.match(r\"^e\\w*[0-9]\", col)]\n",
    "        return torch.tensor(self.df[colnames].values)\n",
    "\n",
    "    def get_torch_path_for_deepsignet(self,\n",
    "                                      include_time_features_in_path: bool,\n",
    "                                      include_time_features_in_input: bool,\n",
    "                                      include_embedding_in_input: bool,\n",
    "                                      reduced_embeddings: bool = False):\n",
    "        if self.array_padded is None:\n",
    "            raise ValueError(\"Need to first call to create the path `.pad()`.\")\n",
    "        \n",
    "        # obtains a torch tensor which can be inputted into deepsignet\n",
    "        # computes how many time features there are currently\n",
    "        # (which occur in the first n_time_features columns)\n",
    "        n_time_features = len(\n",
    "            [item for item in self._time_feature_choices if item in self.df_padded]\n",
    "        )\n",
    "\n",
    "        if include_embedding_in_input:\n",
    "            # repeat the embeddings which will be concatenated to the path later\n",
    "            if self.pad_method == \"id\":\n",
    "                print(f\"[INFO] The path was created for each {self.id_column} in the dataframe, \"\n",
    "                      \"so to include embeddings in the FFN input, we concatenate the \"\n",
    "                      \"pooled embeddings.\")\n",
    "                if self.pooled_embeddings is None:\n",
    "                    raise ValueError(\n",
    "                        \"There were no pooled embeddings passed into the class.\"\n",
    "                    )\n",
    "                if self.array_padded.shape[0] != self.pooled_embeddings.shape[0]:\n",
    "                    raise ValueError(\n",
    "                        \"If want to include the pooled embeddings in the FFN input, the path \"\n",
    "                        \"(found in `.array_padded`) must have the same number of \"\n",
    "                        \"samples as there are pooled embeddings, i.e `.array_padded.shape[0]` \"\n",
    "                        \"must equal `.pooled_embeddings.shape[0]`.\"\n",
    "                    )\n",
    "                else:\n",
    "                    emb = torch.from_numpy(self.pooled_embeddings.astype(\"float\"))\n",
    "            elif self.pad_method == \"history\":\n",
    "                print(f\"[INFO] The path was created for each item in the dataframe, \"\n",
    "                      \"by looking at its history, so to include embeddings in the FFN input, \"\n",
    "                      \"we concatenate the embeddings for each sentence / text.\")\n",
    "                if reduced_embeddings:\n",
    "                    if self.embeddings_reduced is None:\n",
    "                        raise ValueError(\n",
    "                            \"There were no reduced embeddings passed into the class.\"\n",
    "                        )\n",
    "                    elif self.array_padded.shape[0] != self.embeddings_reduced.shape[0]:\n",
    "                        raise ValueError(\n",
    "                            \"If want to include reduced embeddings in the FFN input, the path \"\n",
    "                            \"(found in `.array_padded`) must have the same number of \"\n",
    "                            \"samples as there are embeddings, i.e `.array_padded.shape[0]` \"\n",
    "                            \"must equal `.embeddings_reduced.shape[0]`.\"\n",
    "                        )\n",
    "                    else:\n",
    "                        emb = torch.from_numpy(self.embeddings_reduced.astype(\"float\"))\n",
    "                else:\n",
    "                    if self.array_padded.shape[0] != self.embeddings.shape[0]:\n",
    "                        raise ValueError(\n",
    "                            \"If want to include the full embeddings in the FFN input, the path \"\n",
    "                            \"(found in `.array_padded`) must have the same number of \"\n",
    "                            \"samples as there are embeddings, i.e `.array_padded.shape[0]` \"\n",
    "                            \"must equal `.embeddings.shape[0]`.\"\n",
    "                        )\n",
    "                    else:\n",
    "                        emb = torch.from_numpy(self.embeddings.astype(\"float\"))\n",
    "            repeat_emb = emb.unsqueeze(2).repeat(1, 1, self.array_padded.shape[1]).transpose(1, 2)\n",
    "\n",
    "        if include_time_features_in_path:\n",
    "            # make sure path includes the time features\n",
    "            path = self.get_torch_path(include_time_features=True)\n",
    "            input_channels = path.shape[2]\n",
    "            if include_time_features_in_input:\n",
    "                # need to repeat the time feature columns\n",
    "                # if there are no time features, then we don't need to repeat anything\n",
    "                if n_time_features == 1:\n",
    "                    path = torch.cat([path, path[:,:,0].unsqueeze(2)], dim = 2)\n",
    "                elif n_time_features > 1:\n",
    "                    path = torch.cat([path, path[:,:,0:n_time_features]], dim = 2)\n",
    "        else:\n",
    "            if include_time_features_in_input:    \n",
    "                # path doesn't need to include the time features\n",
    "                # but we still want to include them in the input to the FFN for classification\n",
    "                path = self.get_torch_path(include_time_features=True)                \n",
    "                input_channels = path.shape[2]-n_time_features\n",
    "                # need to move time features to the end of the path\n",
    "                # if there are no time features, then we don't need to move anything\n",
    "                if n_time_features == 1:\n",
    "                    path = torch.cat([path[:,:,n_time_features:], path[:,:,0].unsqueeze(2)], dim = 2)\n",
    "                elif n_time_features > 1:\n",
    "                    path = torch.cat([path[:,:,n_time_features:], path[:,:,0:n_time_features]], dim = 2)\n",
    "            else:\n",
    "                # path doesn't need to include the time features\n",
    "                # and don't need to include them in the input to the FFN for classification\n",
    "                path = self.get_torch_path(include_time_features=False)\n",
    "                input_channels = path.shape[2]\n",
    "\n",
    "        if include_embedding_in_input:\n",
    "            path = torch.cat([path, repeat_emb], dim = 2)\n",
    "\n",
    "        return path, input_channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a0f155e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Note 'datetime' is not a column in `.df`, so only 'timeline_index' is added.\n",
      "[INFO] As 'datetime' is not a column in `.df`, we assume that the data is ordered by time with respect to the id.\n",
      "[INFO] Adding 'timeline_index' feature...\n"
     ]
    }
   ],
   "source": [
    "dataset = PrepareData(text_encoder.tokenized_df,\n",
    "                             id_column=\"text_id\",\n",
    "                             labels_column=\"language\",\n",
    "                             embeddings=token_embeddings,\n",
    "                             pooled_embeddings=pooled_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "801c74d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.df[\"text_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1e951718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12000/12000 [00:30<00:00, 387.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12000, 10, 771)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_path = dataset.pad(pad_by=\"id\",\n",
    "                        zero_padding=True,\n",
    "                        method=\"k_last\",\n",
    "                        k=10,\n",
    "                        time_feature=[\"timeline_index\"],\n",
    "                        standardise_time_feature=False)\n",
    "word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f847528a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeline_index</th>\n",
       "      <th>e1</th>\n",
       "      <th>e2</th>\n",
       "      <th>e3</th>\n",
       "      <th>e4</th>\n",
       "      <th>e5</th>\n",
       "      <th>e6</th>\n",
       "      <th>e7</th>\n",
       "      <th>e8</th>\n",
       "      <th>e9</th>\n",
       "      <th>...</th>\n",
       "      <th>e761</th>\n",
       "      <th>e762</th>\n",
       "      <th>e763</th>\n",
       "      <th>e764</th>\n",
       "      <th>e765</th>\n",
       "      <th>e766</th>\n",
       "      <th>e767</th>\n",
       "      <th>e768</th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.143588</td>\n",
       "      <td>-1.417048</td>\n",
       "      <td>1.320182</td>\n",
       "      <td>-0.508459</td>\n",
       "      <td>1.247967</td>\n",
       "      <td>0.034426</td>\n",
       "      <td>-1.468047</td>\n",
       "      <td>-0.961864</td>\n",
       "      <td>0.559406</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.980690</td>\n",
       "      <td>0.403656</td>\n",
       "      <td>0.398150</td>\n",
       "      <td>0.487656</td>\n",
       "      <td>-0.676191</td>\n",
       "      <td>0.807380</td>\n",
       "      <td>-1.213304</td>\n",
       "      <td>-0.957085</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1.107637</td>\n",
       "      <td>-0.667894</td>\n",
       "      <td>2.376304</td>\n",
       "      <td>-0.928480</td>\n",
       "      <td>3.609042</td>\n",
       "      <td>-0.043664</td>\n",
       "      <td>-1.700852</td>\n",
       "      <td>0.552448</td>\n",
       "      <td>-0.633671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.324514</td>\n",
       "      <td>-0.190638</td>\n",
       "      <td>-0.429270</td>\n",
       "      <td>1.990711</td>\n",
       "      <td>-0.213387</td>\n",
       "      <td>0.543733</td>\n",
       "      <td>0.014306</td>\n",
       "      <td>-0.571665</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.777725</td>\n",
       "      <td>-0.197840</td>\n",
       "      <td>2.643306</td>\n",
       "      <td>-1.788009</td>\n",
       "      <td>-0.637072</td>\n",
       "      <td>-0.254247</td>\n",
       "      <td>0.406706</td>\n",
       "      <td>-0.182973</td>\n",
       "      <td>0.702718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.377799</td>\n",
       "      <td>2.157183</td>\n",
       "      <td>-0.826511</td>\n",
       "      <td>1.861895</td>\n",
       "      <td>-1.222253</td>\n",
       "      <td>-0.212417</td>\n",
       "      <td>-1.194266</td>\n",
       "      <td>-1.107049</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.717557</td>\n",
       "      <td>-0.304074</td>\n",
       "      <td>1.029350</td>\n",
       "      <td>-1.073429</td>\n",
       "      <td>1.875072</td>\n",
       "      <td>-0.084052</td>\n",
       "      <td>-1.756982</td>\n",
       "      <td>0.406479</td>\n",
       "      <td>-0.279253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262103</td>\n",
       "      <td>1.245355</td>\n",
       "      <td>-0.515692</td>\n",
       "      <td>1.316131</td>\n",
       "      <td>-0.730478</td>\n",
       "      <td>-0.612286</td>\n",
       "      <td>0.887850</td>\n",
       "      <td>-0.179549</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.921733</td>\n",
       "      <td>0.051804</td>\n",
       "      <td>0.740113</td>\n",
       "      <td>-0.494959</td>\n",
       "      <td>0.141836</td>\n",
       "      <td>0.293235</td>\n",
       "      <td>0.499587</td>\n",
       "      <td>0.465118</td>\n",
       "      <td>-0.825649</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.601243</td>\n",
       "      <td>1.046709</td>\n",
       "      <td>0.133001</td>\n",
       "      <td>1.805218</td>\n",
       "      <td>-0.145986</td>\n",
       "      <td>0.592019</td>\n",
       "      <td>-2.297613</td>\n",
       "      <td>-1.601534</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>6</td>\n",
       "      <td>1.178958</td>\n",
       "      <td>-0.148862</td>\n",
       "      <td>1.265958</td>\n",
       "      <td>-0.734617</td>\n",
       "      <td>0.983744</td>\n",
       "      <td>-0.335252</td>\n",
       "      <td>0.362385</td>\n",
       "      <td>0.789220</td>\n",
       "      <td>-0.720118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082894</td>\n",
       "      <td>2.911485</td>\n",
       "      <td>-0.729118</td>\n",
       "      <td>2.193193</td>\n",
       "      <td>-0.566522</td>\n",
       "      <td>-0.983746</td>\n",
       "      <td>-1.191947</td>\n",
       "      <td>0.632889</td>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>7</td>\n",
       "      <td>1.024163</td>\n",
       "      <td>-2.861085</td>\n",
       "      <td>0.770218</td>\n",
       "      <td>-0.320513</td>\n",
       "      <td>0.436667</td>\n",
       "      <td>0.087550</td>\n",
       "      <td>-0.765312</td>\n",
       "      <td>0.123461</td>\n",
       "      <td>-0.976917</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.779674</td>\n",
       "      <td>1.250211</td>\n",
       "      <td>-0.421141</td>\n",
       "      <td>1.575737</td>\n",
       "      <td>-0.471906</td>\n",
       "      <td>1.111535</td>\n",
       "      <td>-1.863977</td>\n",
       "      <td>0.369330</td>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>8</td>\n",
       "      <td>1.684086</td>\n",
       "      <td>-0.072866</td>\n",
       "      <td>1.837575</td>\n",
       "      <td>-1.241489</td>\n",
       "      <td>1.546939</td>\n",
       "      <td>0.040575</td>\n",
       "      <td>-0.274818</td>\n",
       "      <td>0.115267</td>\n",
       "      <td>-0.512420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.870751</td>\n",
       "      <td>1.708995</td>\n",
       "      <td>-0.186876</td>\n",
       "      <td>1.444939</td>\n",
       "      <td>0.052923</td>\n",
       "      <td>0.052861</td>\n",
       "      <td>-0.449925</td>\n",
       "      <td>0.161691</td>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>9</td>\n",
       "      <td>2.046932</td>\n",
       "      <td>-0.022341</td>\n",
       "      <td>0.495391</td>\n",
       "      <td>-1.086745</td>\n",
       "      <td>0.310161</td>\n",
       "      <td>-0.112842</td>\n",
       "      <td>0.781924</td>\n",
       "      <td>0.100807</td>\n",
       "      <td>0.078282</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.145458</td>\n",
       "      <td>3.236154</td>\n",
       "      <td>-0.256295</td>\n",
       "      <td>1.222923</td>\n",
       "      <td>-0.354660</td>\n",
       "      <td>-0.288702</td>\n",
       "      <td>-0.704664</td>\n",
       "      <td>-0.622784</td>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>10</td>\n",
       "      <td>1.661484</td>\n",
       "      <td>1.436595</td>\n",
       "      <td>1.242175</td>\n",
       "      <td>-0.588015</td>\n",
       "      <td>1.184628</td>\n",
       "      <td>-0.622242</td>\n",
       "      <td>-1.603853</td>\n",
       "      <td>0.529488</td>\n",
       "      <td>-0.543837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506650</td>\n",
       "      <td>0.352198</td>\n",
       "      <td>-0.147210</td>\n",
       "      <td>1.799329</td>\n",
       "      <td>-0.267002</td>\n",
       "      <td>0.553767</td>\n",
       "      <td>-0.547994</td>\n",
       "      <td>-0.739612</td>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows Ã— 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        timeline_index        e1        e2        e3        e4        e5  \\\n",
       "0                    2  0.143588 -1.417048  1.320182 -0.508459  1.247967   \n",
       "1                    3  1.107637 -0.667894  2.376304 -0.928480  3.609042   \n",
       "2                    4  0.777725 -0.197840  2.643306 -1.788009 -0.637072   \n",
       "3                    5  0.717557 -0.304074  1.029350 -1.073429  1.875072   \n",
       "4                    6  0.921733  0.051804  0.740113 -0.494959  0.141836   \n",
       "...                ...       ...       ...       ...       ...       ...   \n",
       "119995               6  1.178958 -0.148862  1.265958 -0.734617  0.983744   \n",
       "119996               7  1.024163 -2.861085  0.770218 -0.320513  0.436667   \n",
       "119997               8  1.684086 -0.072866  1.837575 -1.241489  1.546939   \n",
       "119998               9  2.046932 -0.022341  0.495391 -1.086745  0.310161   \n",
       "119999              10  1.661484  1.436595  1.242175 -0.588015  1.184628   \n",
       "\n",
       "              e6        e7        e8        e9  ...      e761      e762  \\\n",
       "0       0.034426 -1.468047 -0.961864  0.559406  ... -0.980690  0.403656   \n",
       "1      -0.043664 -1.700852  0.552448 -0.633671  ... -0.324514 -0.190638   \n",
       "2      -0.254247  0.406706 -0.182973  0.702718  ... -0.377799  2.157183   \n",
       "3      -0.084052 -1.756982  0.406479 -0.279253  ...  0.262103  1.245355   \n",
       "4       0.293235  0.499587  0.465118 -0.825649  ... -1.601243  1.046709   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "119995 -0.335252  0.362385  0.789220 -0.720118  ...  0.082894  2.911485   \n",
       "119996  0.087550 -0.765312  0.123461 -0.976917  ... -0.779674  1.250211   \n",
       "119997  0.040575 -0.274818  0.115267 -0.512420  ... -0.870751  1.708995   \n",
       "119998 -0.112842  0.781924  0.100807  0.078282  ... -1.145458  3.236154   \n",
       "119999 -0.622242 -1.603853  0.529488 -0.543837  ...  0.506650  0.352198   \n",
       "\n",
       "            e763      e764      e765      e766      e767      e768  text_id  \\\n",
       "0       0.398150  0.487656 -0.676191  0.807380 -1.213304 -0.957085        0   \n",
       "1      -0.429270  1.990711 -0.213387  0.543733  0.014306 -0.571665        0   \n",
       "2      -0.826511  1.861895 -1.222253 -0.212417 -1.194266 -1.107049        0   \n",
       "3      -0.515692  1.316131 -0.730478 -0.612286  0.887850 -0.179549        0   \n",
       "4       0.133001  1.805218 -0.145986  0.592019 -2.297613 -1.601534        0   \n",
       "...          ...       ...       ...       ...       ...       ...      ...   \n",
       "119995 -0.729118  2.193193 -0.566522 -0.983746 -1.191947  0.632889    11999   \n",
       "119996 -0.421141  1.575737 -0.471906  1.111535 -1.863977  0.369330    11999   \n",
       "119997 -0.186876  1.444939  0.052923  0.052861 -0.449925  0.161691    11999   \n",
       "119998 -0.256295  1.222923 -0.354660 -0.288702 -0.704664 -0.622784    11999   \n",
       "119999 -0.147210  1.799329 -0.267002  0.553767 -0.547994 -0.739612    11999   \n",
       "\n",
       "        language  \n",
       "0             de  \n",
       "1             de  \n",
       "2             de  \n",
       "3             de  \n",
       "4             de  \n",
       "...          ...  \n",
       "119995        sv  \n",
       "119996        sv  \n",
       "119997        sv  \n",
       "119998        sv  \n",
       "119999        sv  \n",
       "\n",
       "[120000 rows x 771 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bf6408bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeline_index</th>\n",
       "      <th>e1</th>\n",
       "      <th>e2</th>\n",
       "      <th>e3</th>\n",
       "      <th>e4</th>\n",
       "      <th>e5</th>\n",
       "      <th>e6</th>\n",
       "      <th>e7</th>\n",
       "      <th>e8</th>\n",
       "      <th>e9</th>\n",
       "      <th>...</th>\n",
       "      <th>e761</th>\n",
       "      <th>e762</th>\n",
       "      <th>e763</th>\n",
       "      <th>e764</th>\n",
       "      <th>e765</th>\n",
       "      <th>e766</th>\n",
       "      <th>e767</th>\n",
       "      <th>e768</th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.143588</td>\n",
       "      <td>-1.417048</td>\n",
       "      <td>1.320182</td>\n",
       "      <td>-0.508459</td>\n",
       "      <td>1.247967</td>\n",
       "      <td>0.034426</td>\n",
       "      <td>-1.468047</td>\n",
       "      <td>-0.961864</td>\n",
       "      <td>0.559406</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.980690</td>\n",
       "      <td>0.403656</td>\n",
       "      <td>0.398150</td>\n",
       "      <td>0.487656</td>\n",
       "      <td>-0.676191</td>\n",
       "      <td>0.807380</td>\n",
       "      <td>-1.213304</td>\n",
       "      <td>-0.957085</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1.107637</td>\n",
       "      <td>-0.667894</td>\n",
       "      <td>2.376304</td>\n",
       "      <td>-0.928480</td>\n",
       "      <td>3.609042</td>\n",
       "      <td>-0.043664</td>\n",
       "      <td>-1.700852</td>\n",
       "      <td>0.552448</td>\n",
       "      <td>-0.633671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.324514</td>\n",
       "      <td>-0.190638</td>\n",
       "      <td>-0.429270</td>\n",
       "      <td>1.990711</td>\n",
       "      <td>-0.213387</td>\n",
       "      <td>0.543733</td>\n",
       "      <td>0.014306</td>\n",
       "      <td>-0.571665</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.777725</td>\n",
       "      <td>-0.197840</td>\n",
       "      <td>2.643306</td>\n",
       "      <td>-1.788009</td>\n",
       "      <td>-0.637072</td>\n",
       "      <td>-0.254247</td>\n",
       "      <td>0.406706</td>\n",
       "      <td>-0.182973</td>\n",
       "      <td>0.702718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.377799</td>\n",
       "      <td>2.157183</td>\n",
       "      <td>-0.826511</td>\n",
       "      <td>1.861895</td>\n",
       "      <td>-1.222253</td>\n",
       "      <td>-0.212417</td>\n",
       "      <td>-1.194266</td>\n",
       "      <td>-1.107049</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.717557</td>\n",
       "      <td>-0.304074</td>\n",
       "      <td>1.029350</td>\n",
       "      <td>-1.073429</td>\n",
       "      <td>1.875072</td>\n",
       "      <td>-0.084052</td>\n",
       "      <td>-1.756982</td>\n",
       "      <td>0.406479</td>\n",
       "      <td>-0.279253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262103</td>\n",
       "      <td>1.245355</td>\n",
       "      <td>-0.515692</td>\n",
       "      <td>1.316131</td>\n",
       "      <td>-0.730478</td>\n",
       "      <td>-0.612286</td>\n",
       "      <td>0.887850</td>\n",
       "      <td>-0.179549</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.921733</td>\n",
       "      <td>0.051804</td>\n",
       "      <td>0.740113</td>\n",
       "      <td>-0.494959</td>\n",
       "      <td>0.141836</td>\n",
       "      <td>0.293235</td>\n",
       "      <td>0.499587</td>\n",
       "      <td>0.465118</td>\n",
       "      <td>-0.825649</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.601243</td>\n",
       "      <td>1.046709</td>\n",
       "      <td>0.133001</td>\n",
       "      <td>1.805218</td>\n",
       "      <td>-0.145986</td>\n",
       "      <td>0.592019</td>\n",
       "      <td>-2.297613</td>\n",
       "      <td>-1.601534</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>1.960512</td>\n",
       "      <td>-0.114054</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.959227</td>\n",
       "      <td>-0.274134</td>\n",
       "      <td>-0.047917</td>\n",
       "      <td>0.303097</td>\n",
       "      <td>0.895570</td>\n",
       "      <td>0.704869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654793</td>\n",
       "      <td>2.553124</td>\n",
       "      <td>-0.306052</td>\n",
       "      <td>1.188357</td>\n",
       "      <td>-0.323826</td>\n",
       "      <td>-0.661785</td>\n",
       "      <td>-0.127747</td>\n",
       "      <td>-0.642438</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.858337</td>\n",
       "      <td>0.980120</td>\n",
       "      <td>-0.480119</td>\n",
       "      <td>-1.347086</td>\n",
       "      <td>0.100639</td>\n",
       "      <td>0.609157</td>\n",
       "      <td>-1.403996</td>\n",
       "      <td>0.971574</td>\n",
       "      <td>1.116285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560951</td>\n",
       "      <td>-0.665503</td>\n",
       "      <td>0.562110</td>\n",
       "      <td>1.043689</td>\n",
       "      <td>0.655348</td>\n",
       "      <td>0.996938</td>\n",
       "      <td>-0.654039</td>\n",
       "      <td>-0.913582</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.739917</td>\n",
       "      <td>-0.593066</td>\n",
       "      <td>1.324941</td>\n",
       "      <td>-1.596288</td>\n",
       "      <td>0.311475</td>\n",
       "      <td>-0.266285</td>\n",
       "      <td>-0.007784</td>\n",
       "      <td>0.910885</td>\n",
       "      <td>1.055482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558452</td>\n",
       "      <td>1.238724</td>\n",
       "      <td>-0.340453</td>\n",
       "      <td>1.134031</td>\n",
       "      <td>-0.879360</td>\n",
       "      <td>0.472009</td>\n",
       "      <td>1.334212</td>\n",
       "      <td>-0.187999</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>1.059729</td>\n",
       "      <td>1.284173</td>\n",
       "      <td>0.815329</td>\n",
       "      <td>-0.547035</td>\n",
       "      <td>1.821547</td>\n",
       "      <td>-0.032910</td>\n",
       "      <td>-0.811382</td>\n",
       "      <td>0.412806</td>\n",
       "      <td>0.762482</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.534226</td>\n",
       "      <td>0.423330</td>\n",
       "      <td>0.452225</td>\n",
       "      <td>-0.418872</td>\n",
       "      <td>0.391442</td>\n",
       "      <td>0.305640</td>\n",
       "      <td>0.317617</td>\n",
       "      <td>-0.111468</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>-0.218854</td>\n",
       "      <td>-0.604878</td>\n",
       "      <td>1.608435</td>\n",
       "      <td>-0.389211</td>\n",
       "      <td>1.714435</td>\n",
       "      <td>-0.739268</td>\n",
       "      <td>-1.454955</td>\n",
       "      <td>0.345475</td>\n",
       "      <td>-1.222678</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.966916</td>\n",
       "      <td>0.498792</td>\n",
       "      <td>-0.507472</td>\n",
       "      <td>1.434245</td>\n",
       "      <td>-0.376011</td>\n",
       "      <td>1.072734</td>\n",
       "      <td>-0.002285</td>\n",
       "      <td>-0.825246</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   timeline_index        e1        e2        e3        e4        e5        e6  \\\n",
       "0               2  0.143588 -1.417048  1.320182 -0.508459  1.247967  0.034426   \n",
       "1               3  1.107637 -0.667894  2.376304 -0.928480  3.609042 -0.043664   \n",
       "2               4  0.777725 -0.197840  2.643306 -1.788009 -0.637072 -0.254247   \n",
       "3               5  0.717557 -0.304074  1.029350 -1.073429  1.875072 -0.084052   \n",
       "4               6  0.921733  0.051804  0.740113 -0.494959  0.141836  0.293235   \n",
       "5               7  1.960512 -0.114054 -0.089443 -0.959227 -0.274134 -0.047917   \n",
       "6               8  0.858337  0.980120 -0.480119 -1.347086  0.100639  0.609157   \n",
       "7               9  0.739917 -0.593066  1.324941 -1.596288  0.311475 -0.266285   \n",
       "8              10  1.059729  1.284173  0.815329 -0.547035  1.821547 -0.032910   \n",
       "9              11 -0.218854 -0.604878  1.608435 -0.389211  1.714435 -0.739268   \n",
       "\n",
       "         e7        e8        e9  ...      e761      e762      e763      e764  \\\n",
       "0 -1.468047 -0.961864  0.559406  ... -0.980690  0.403656  0.398150  0.487656   \n",
       "1 -1.700852  0.552448 -0.633671  ... -0.324514 -0.190638 -0.429270  1.990711   \n",
       "2  0.406706 -0.182973  0.702718  ... -0.377799  2.157183 -0.826511  1.861895   \n",
       "3 -1.756982  0.406479 -0.279253  ...  0.262103  1.245355 -0.515692  1.316131   \n",
       "4  0.499587  0.465118 -0.825649  ... -1.601243  1.046709  0.133001  1.805218   \n",
       "5  0.303097  0.895570  0.704869  ... -0.654793  2.553124 -0.306052  1.188357   \n",
       "6 -1.403996  0.971574  1.116285  ...  0.560951 -0.665503  0.562110  1.043689   \n",
       "7 -0.007784  0.910885  1.055482  ...  0.558452  1.238724 -0.340453  1.134031   \n",
       "8 -0.811382  0.412806  0.762482  ... -0.534226  0.423330  0.452225 -0.418872   \n",
       "9 -1.454955  0.345475 -1.222678  ... -0.966916  0.498792 -0.507472  1.434245   \n",
       "\n",
       "       e765      e766      e767      e768  text_id  language  \n",
       "0 -0.676191  0.807380 -1.213304 -0.957085        0        de  \n",
       "1 -0.213387  0.543733  0.014306 -0.571665        0        de  \n",
       "2 -1.222253 -0.212417 -1.194266 -1.107049        0        de  \n",
       "3 -0.730478 -0.612286  0.887850 -0.179549        0        de  \n",
       "4 -0.145986  0.592019 -2.297613 -1.601534        0        de  \n",
       "5 -0.323826 -0.661785 -0.127747 -0.642438        0        de  \n",
       "6  0.655348  0.996938 -0.654039 -0.913582        0        de  \n",
       "7 -0.879360  0.472009  1.334212 -0.187999        0        de  \n",
       "8  0.391442  0.305640  0.317617 -0.111468        0        de  \n",
       "9 -0.376011  1.072734 -0.002285 -0.825246        0        de  \n",
       "\n",
       "[10 rows x 771 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# still has the labels and the ids\n",
    "dataset.df_padded[dataset.df_padded[\"text_id\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7266a636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12000, 10, 769])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by default keeps the time features\n",
    "torch_word_path = dataset.get_torch_path()\n",
    "torch_word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "73329213",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = torch.from_numpy(dataset.pooled_embeddings.astype(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e6638252",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_emb = emb.unsqueeze(2).repeat(1,1,10).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3bd48eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The path was created for each text_id in the dataframe, so to include embeddings in the FFN input, we concatenate the pooled embeddings.\n",
      "path shape: torch.Size([12000, 10, 1538])\n",
      "input_channels: 769\n"
     ]
    }
   ],
   "source": [
    "torch_word_path_for_deepsignet = dataset.get_torch_path_for_deepsignet(include_time_features_in_path=True,\n",
    "                                                                       include_time_features_in_input=True,\n",
    "                                                                       include_embedding_in_input=True)\n",
    "print(f\"path shape: {torch_word_path_for_deepsignet[0].shape}\")\n",
    "print(f\"input_channels: {torch_word_path_for_deepsignet[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "33806b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path shape: torch.Size([12000, 10, 770])\n",
      "input_channels: 769\n"
     ]
    }
   ],
   "source": [
    "torch_word_path_for_deepsignet = dataset.get_torch_path_for_deepsignet(include_time_features_in_path=True,\n",
    "                                                                       include_time_features_in_input=True,\n",
    "                                                                       include_embedding_in_input=False)\n",
    "print(f\"path shape: {torch_word_path_for_deepsignet[0].shape}\")\n",
    "print(f\"input_channels: {torch_word_path_for_deepsignet[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b1a980f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path shape: torch.Size([12000, 10, 769])\n",
      "input_channels: 769\n"
     ]
    }
   ],
   "source": [
    "torch_word_path_for_deepsignet = dataset.get_torch_path_for_deepsignet(include_time_features_in_path=True,\n",
    "                                                                       include_time_features_in_input=False,\n",
    "                                                                       include_embedding_in_input=False)\n",
    "print(f\"path shape: {torch_word_path_for_deepsignet[0].shape}\")\n",
    "print(f\"input_channels: {torch_word_path_for_deepsignet[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1e422bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path shape: torch.Size([12000, 10, 769])\n",
      "input_channels: 768\n"
     ]
    }
   ],
   "source": [
    "torch_word_path_for_deepsignet = dataset.get_torch_path_for_deepsignet(include_time_features_in_path=False,\n",
    "                                                                       include_time_features_in_input=True,\n",
    "                                                                       include_embedding_in_input=False)\n",
    "print(f\"path shape: {torch_word_path_for_deepsignet[0].shape}\")\n",
    "print(f\"input_channels: {torch_word_path_for_deepsignet[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "543c2893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path shape: torch.Size([12000, 10, 768])\n",
      "input_channels: 768\n"
     ]
    }
   ],
   "source": [
    "torch_word_path_for_deepsignet = dataset.get_torch_path_for_deepsignet(include_time_features_in_path=False,\n",
    "                                                                       include_time_features_in_input=False,\n",
    "                                                                       include_embedding_in_input=False)\n",
    "print(f\"path shape: {torch_word_path_for_deepsignet[0].shape}\")\n",
    "print(f\"input_channels: {torch_word_path_for_deepsignet[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce63dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py38esig)",
   "language": "python",
   "name": "py38esig"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
