{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c43e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    RobertaConfig,\n",
    "    RobertaForMaskedLM,\n",
    "    RobertaTokenizerFast,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "import tokenizers\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nlpsig\n",
    "\n",
    "import random\n",
    "import math\n",
    "seed = 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d3859",
   "metadata": {},
   "source": [
    "## Language dataset\n",
    "\n",
    "In the `data/` folder, we have several text folders of words from different languages:\n",
    "- `wordlist_de.txt`: German words\n",
    "- `wordlist_en.txt`: English words\n",
    "- `wordlist_fr.txt`: French words\n",
    "- `wordlist_it.txt`: Italian words\n",
    "- `wordlist_pl.txt`: Polish words\n",
    "- `wordlist_sv.txt`: Swedish words\n",
    "\n",
    "We additionally have a `alphabet.txt` file which just stores the alphabet characters ('a', 'b', 'c', ...).\n",
    "\n",
    "The task is to split the words into its individual characters and to obtain an embedding for each of them. We can represent a word by a path of its character embeddings and compute its path signature to use as features in predicting the language for which the word belongs.\n",
    "\n",
    "Here we look at obtaining embeddings using a Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53a6aaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "ALPHABET_FILE = 'data/alphabet.txt'\n",
    "with open(ALPHABET_FILE) as f:\n",
    "    alphabet = f.read().splitlines()\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b2270d",
   "metadata": {},
   "source": [
    "## Set up Tokenizer for word corpora\n",
    "\n",
    "If we were to fine-tune an existing pretrained transformer, we could use the same tokenizer that the model was pretrained with. However, here we want to train a model from stratch, and so using a tokenizer that was pretrained on a corpus that looks quite different to ours is suboptimal.\n",
    "\n",
    "Here, we need to use the `tokenizers` library to set up and train a new tokenizer for our text.\n",
    "\n",
    "In particular, we're going to start off with a character-based tokenizer (as we're going to split up our words into characters), and train it to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c39ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAR_BERT/vocab.json', 'CHAR_BERT/merges.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialise character based tokenizer\n",
    "tokenizer = tokenizers.CharBPETokenizer()\n",
    "tokenizer.train(files=[ALPHABET_FILE], show_progress=False)\n",
    "\n",
    "# save the tokenizer to \"CHAR_BERT/\" folder\n",
    "if not os.path.exists(\"CHAR_BERT\"):\n",
    "    os.makedirs(\"CHAR_BERT\")\n",
    "\n",
    "tokenizer.save_model(\"CHAR_BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "941eb204",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist_files = [\"data/wordlist_de.txt\",\n",
    "                  \"data/wordlist_en.txt\",\n",
    "                  \"data/wordlist_fr.txt\",\n",
    "                  \"data/wordlist_it.txt\",\n",
    "                  \"data/wordlist_pl.txt\",\n",
    "                  \"data/wordlist_sv.txt\"]\n",
    "\n",
    "wordlist_dfs = []\n",
    "for filename in wordlist_files:\n",
    "    with open(filename, \"r\") as f:\n",
    "        words = f.read().splitlines()\n",
    "        words_df = pd.DataFrame({\"word\": words})\n",
    "        words_df[\"language\"] = filename.split(\"_\")[1][0:2]\n",
    "        wordlist_dfs.append(words_df)\n",
    "\n",
    "corpus_df = pd.concat(wordlist_dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f88916a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aal</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aale</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aalen</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aalend</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922530</th>\n",
       "      <td>zons</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922531</th>\n",
       "      <td>zoo</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922532</th>\n",
       "      <td>zoologisk</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922533</th>\n",
       "      <td>zoologiska</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922534</th>\n",
       "      <td>zoologiskt</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3922535 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               word language\n",
       "0                 a       de\n",
       "1               aal       de\n",
       "2              aale       de\n",
       "3             aalen       de\n",
       "4            aalend       de\n",
       "...             ...      ...\n",
       "3922530        zons       sv\n",
       "3922531         zoo       sv\n",
       "3922532   zoologisk       sv\n",
       "3922533  zoologiska       sv\n",
       "3922534  zoologiskt       sv\n",
       "\n",
       "[3922535 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbabb77a",
   "metadata": {},
   "source": [
    "We can see that there are relatively fewer English words than the other languages..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "791a1f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "it    1862929\n",
       "pl    1517274\n",
       "fr     198538\n",
       "de     186027\n",
       "en      80641\n",
       "sv      77126\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfd1032",
   "metadata": {},
   "source": [
    "**Question:** what do we do with words that appear twice in the data? i.e. different languages have the same word in their vocabularies, e.g. the word \"zoo\" is a valid word in english, french, italian, polish and swedish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ac0dd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>266640</th>\n",
       "      <td>zoo</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465081</th>\n",
       "      <td>zoo</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2326761</th>\n",
       "      <td>zoo</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3836964</th>\n",
       "      <td>zoo</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922531</th>\n",
       "      <td>zoo</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word language\n",
       "266640   zoo       en\n",
       "465081   zoo       fr\n",
       "2326761  zoo       it\n",
       "3836964  zoo       pl\n",
       "3922531  zoo       sv"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df[corpus_df[\"word\"]==\"zoo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f4c498",
   "metadata": {},
   "source": [
    "We are going to train our language model on the English words, so taking out a sample of English words from the corpus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "066d9241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for sampling\n",
    "random.seed(seed)\n",
    "\n",
    "n_words = 60000\n",
    "english_train = corpus_df[corpus_df[\"language\"]==\"en\"].sample(n_words)\n",
    "english_train = english_train.reset_index(drop=True)\n",
    "\n",
    "# remove the words that we use to train language model from the corpus\n",
    "corpus_df = corpus_df.drop(english_train.index)\n",
    "corpus_df = corpus_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "739f58df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fishhook</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daffodils</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rickety</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>manse</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lightfaced</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>certify</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>shameless</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>nose</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>nymphets</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>swivelled</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word language\n",
       "0        fishhook       en\n",
       "1       daffodils       en\n",
       "2         rickety       en\n",
       "3           manse       en\n",
       "4      lightfaced       en\n",
       "...           ...      ...\n",
       "59995     certify       en\n",
       "59996   shameless       en\n",
       "59997        nose       en\n",
       "59998    nymphets       en\n",
       "59999   swivelled       en\n",
       "\n",
       "[60000 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf89f51",
   "metadata": {},
   "source": [
    "To make the dataset bit more manageable, I'll just take a sample of each of the languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e60fabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sterilster</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unterschiedenes</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>verschuldende</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>verbratender</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>staatsrechtlich</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>ifatt</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>formgavs</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>bestods</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>kurirers</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>satsningars</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  word language\n",
       "0           sterilster       de\n",
       "1      unterschiedenes       de\n",
       "2        verschuldende       de\n",
       "3         verbratender       de\n",
       "4      staatsrechtlich       de\n",
       "...                ...      ...\n",
       "11995            ifatt       sv\n",
       "11996         formgavs       sv\n",
       "11997          bestods       sv\n",
       "11998         kurirers       sv\n",
       "11999      satsningars       sv\n",
       "\n",
       "[12000 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed for sampling\n",
    "random.seed(seed)\n",
    "balanced = True\n",
    "\n",
    "# take \n",
    "n_words = 12000\n",
    "if balanced:\n",
    "    languages = corpus_df[\"language\"].unique()\n",
    "    words_per_language = math.floor(n_words / len(languages))\n",
    "    corpus_sample_df = pd.concat(\n",
    "        [corpus_df[corpus_df[\"language\"]==lang].sample(words_per_language, random_state=seed)\n",
    "         for lang in languages]\n",
    "    )\n",
    "    corpus_sample_df = corpus_sample_df.reset_index(drop=True)\n",
    "else:\n",
    "    corpus_sample_df = corpus_df.iloc[random.sample(range(len(corpus_df)), n_words)]\n",
    "    corpus_sample_df = corpus_sample_df.reset_index(drop=True)\n",
    "\n",
    "corpus_sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d9e53354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "de    2000\n",
       "en    2000\n",
       "fr    2000\n",
       "it    2000\n",
       "pl    2000\n",
       "sv    2000\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sample_df[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0eb7ae",
   "metadata": {},
   "source": [
    "## Training a language model\n",
    "\n",
    "We want to train a masked language model for our corpus of English words. In particular, we mask out particular letters and ask our model to try predict the masked letter.\n",
    "\n",
    "Here, we initialise our tokenizer (here we tokenize by character), data collator (with padding) and set up our transformer model by specifying the config (we use the RoBERTa here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5008adb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "max_length = 512\n",
    "\n",
    "# load in tokenizer for architecture\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('CHAR_BERT/',\n",
    "                                                 max_len=max_length)\n",
    "\n",
    "# set up data_collator to use (intially just one that adds padding)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# initialise transformer architecture (random weights)\n",
    "config_args = {\"vocab_size\": tokenizer.backend_tokenizer.get_vocab_size(),\n",
    "               \"hidden_size\": 768,\n",
    "               \"max_length\": 512,\n",
    "               \"max_position_embeddings\": max_length + 2,\n",
    "               \"hidden_dropout_prob\": 0.1,\n",
    "               \"num_attention_heads\": 12,\n",
    "               \"num_hidden_layers\": 6,\n",
    "               \"type_vocab_size\": 1}\n",
    "\n",
    "config = RobertaConfig(**config_args)\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe7e32",
   "metadata": {},
   "source": [
    "## Using the `TextEncoder` class\n",
    "\n",
    "The `TextEncoder` class in the `nlpsig` package is able to take a dataframe with a column of text. We can use this class to obtain embeddings for the input text, or to train the model with the input text.\n",
    "\n",
    "Note: In the initial writing of the package, the idea was that we can fit our transformer to some data (some text), and then we can obtain embeddings for them. But in our setting, we actually want to fit our data to our sample of English words (which we call `english_train`), but then obtain embeddings for our sample of the remaining words (which we call `corpus_sample_df`) - noting that this also contains some English words.\n",
    "\n",
    "So we will actually use two instances of `TextEncoder` - one to pass in `corpus_df` and train the model. And then another to obtain embeddings for the words in `corpus_sample_df`. This is not optimal and not clean, but some adjustment to `TextEncoder` will be able. In particular, we can perhaps make changes to the `.tokenize_text()` method (which we will see how to use later) which can take in some external text data. \n",
    "\n",
    "I envisage that we can pass in different data to train the model rather than training it on the data that is passed...\n",
    "\n",
    "But for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0848121e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fishhook</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daffodils</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rickety</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>manse</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lightfaced</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word language\n",
       "0    fishhook       en\n",
       "1   daffodils       en\n",
       "2     rickety       en\n",
       "3       manse       en\n",
       "4  lightfaced       en"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "059975c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = nlpsig.TextEncoder(df=english_train,\n",
    "                                  feature_name=\"word\",\n",
    "                                  model=model,\n",
    "                                  config=config,\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3730092",
   "metadata": {},
   "source": [
    "We can tokenize the text with the `.tokenize_text()` method, which tokenizes each of the items in the column of the dataframe that we passed in. So in the above, we tokenise the `word` column of the `english_train` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d7c5e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting return_special_tokens_mask=True\n",
      "[INFO] Tokenizing the datatset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad378cd345d4a8592d0a6fffd148252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving the tokenized text for each sentence into `.df['tokens']`...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53e95d97089425f8d44ae6323757fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating tokenized dataframe and setting in `.tokenized_df` attribute...\n",
      "[INFO] Note: 'text_id' is the column name for denoting the corresponding text id\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['word', 'language', 'input_ids', 'attention_mask', 'special_tokens_mask', 'tokens'],\n",
       "    num_rows: 60000\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokenize_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecec9ec",
   "metadata": {},
   "source": [
    "Note that the `text_encoder` object (instance of `TextEncoder`) also keeps the data as a Huggingface Dataset object too which is stored in the `.dataset` attribute of the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec831c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['word', 'language', 'input_ids', 'attention_mask', 'special_tokens_mask', 'tokens'],\n",
       "    num_rows: 60000\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1361fc4",
   "metadata": {},
   "source": [
    "We can see that we have tokenized this as there are `input_ids`, `attention_mask`, `special_tokens_mask`, and `tokens` features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "19de61ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fishhook'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.dataset[\"word\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "376415cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53, 6, 9, 19, 8, 8, 15, 15, 11, 54]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.dataset[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df4d026",
   "metadata": {},
   "source": [
    "We can see that this word has been tokenized by character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d717a3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f', 'i', 's', 'h', 'h', 'o', 'o', 'k']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.dataset[\"tokens\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c6da2c",
   "metadata": {},
   "source": [
    "We can also see that we have saved the tokenized text in the `'token'` column of the dataframe stored in `.df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "13596f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fishhook</td>\n",
       "      <td>en</td>\n",
       "      <td>[f, i, s, h, h, o, o, k]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daffodils</td>\n",
       "      <td>en</td>\n",
       "      <td>[d, a, f, f, o, d, i, l, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rickety</td>\n",
       "      <td>en</td>\n",
       "      <td>[r, i, c, k, e, t, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>manse</td>\n",
       "      <td>en</td>\n",
       "      <td>[m, a, n, s, e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lightfaced</td>\n",
       "      <td>en</td>\n",
       "      <td>[l, i, g, h, t, f, a, c, e, d]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>certify</td>\n",
       "      <td>en</td>\n",
       "      <td>[c, e, r, t, i, f, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>shameless</td>\n",
       "      <td>en</td>\n",
       "      <td>[s, h, a, m, e, l, e, s, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>nose</td>\n",
       "      <td>en</td>\n",
       "      <td>[n, o, s, e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>nymphets</td>\n",
       "      <td>en</td>\n",
       "      <td>[n, y, m, p, h, e, t, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>swivelled</td>\n",
       "      <td>en</td>\n",
       "      <td>[s, w, i, v, e, l, l, e, d]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word language                          tokens\n",
       "0        fishhook       en        [f, i, s, h, h, o, o, k]\n",
       "1       daffodils       en     [d, a, f, f, o, d, i, l, s]\n",
       "2         rickety       en           [r, i, c, k, e, t, y]\n",
       "3           manse       en                 [m, a, n, s, e]\n",
       "4      lightfaced       en  [l, i, g, h, t, f, a, c, e, d]\n",
       "...           ...      ...                             ...\n",
       "59995     certify       en           [c, e, r, t, i, f, y]\n",
       "59996   shameless       en     [s, h, a, m, e, l, e, s, s]\n",
       "59997        nose       en                    [n, o, s, e]\n",
       "59998    nymphets       en        [n, y, m, p, h, e, t, s]\n",
       "59999   swivelled       en     [s, w, i, v, e, l, l, e, d]\n",
       "\n",
       "[60000 rows x 3 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb134e7",
   "metadata": {},
   "source": [
    "We also store the tokens in `.tokens` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec97953f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 60000\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef8628",
   "metadata": {},
   "source": [
    "After applying the `.tokenize_text()` method, we store a tokenized dataframe in the `.tokenized_df` attribue. Here, we have each token in our corpus and their associated `'text_id'` (which is just the index they were given in the original dataframe that we pass):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d527e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511576</th>\n",
       "      <td>59999</td>\n",
       "      <td>en</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511577</th>\n",
       "      <td>59999</td>\n",
       "      <td>en</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511578</th>\n",
       "      <td>59999</td>\n",
       "      <td>en</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511579</th>\n",
       "      <td>59999</td>\n",
       "      <td>en</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511580</th>\n",
       "      <td>59999</td>\n",
       "      <td>en</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>511581 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id language tokens\n",
       "0             0       en      f\n",
       "1             0       en      i\n",
       "2             0       en      s\n",
       "3             0       en      h\n",
       "4             0       en      h\n",
       "...         ...      ...    ...\n",
       "511576    59999       en      e\n",
       "511577    59999       en      l\n",
       "511578    59999       en      l\n",
       "511579    59999       en      e\n",
       "511580    59999       en      d\n",
       "\n",
       "[511581 rows x 3 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokenized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1fe039",
   "metadata": {},
   "source": [
    "So if we looked at `text_id==0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f7d5ee43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>k</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_id language tokens\n",
       "0        0       en      f\n",
       "1        0       en      i\n",
       "2        0       en      s\n",
       "3        0       en      h\n",
       "4        0       en      h\n",
       "5        0       en      o\n",
       "6        0       en      o\n",
       "7        0       en      k"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokenized_df[text_encoder.tokenized_df[\"text_id\"]==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e149567e",
   "metadata": {},
   "source": [
    "If we had passed in a pre-trained model (remember above, we just initialised one with a config and so have random weight), we can obtain token embeddings by the `.obtain_embeddings()` method. \n",
    "\n",
    "There are many ways in which one can get embeddings from the transformer network, as the output is the layers for the full network. A few ways are:\n",
    "\n",
    "- Returning the output of a particular hidden layer\n",
    "    - use `.obtain_embeddings(method = \"hidden_layer\", layers = l)` where `l` is the layer you want\n",
    "    - If no layer is requested, it will just give you the second-to-last hidden layer of the transformer network.\n",
    "- Concatenate the output of several hidden layers\n",
    "    - use `.obtain_embeddings(method = \"concatenate\", layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of layers you want to concatenate\n",
    "- Element-wise sum the output of several hidden layers\n",
    "    - use `.obtain_embeddings(method = \"sum\" , layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of layers you want to sum\n",
    "- Mean the output of several hidden layers\n",
    "    - use `.obtain_embeddings(method = \"mean\" , layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of layers you want to mean\n",
    "\n",
    "If a more custom way to obtain embeddings from the hidden layers, you can specify what layers you want, and it will return them (i.e. using `.obtain_embeddings(method = \"hidden_layer\", layers = [l_1, l_2, ...])` where `[l_1, l_2, ...]` is a list of hidden layers you want) and so the output will be a 3-dimensional array with dimensions `[layer, token, embedding]` for which you would need to combine in such a way that you would have an embedding for each token. The above methods would return a 2-dimensional array with dimensions `[token, embedding]`.\n",
    "\n",
    "In the below, we just obtain the second-to-last hidden layer of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9c068bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                        | 0/600 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████████████████████████| 600/600 [03:02<00:00,  3.29it/s]\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = text_encoder.obtain_embeddings(method = \"hidden_layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d0762c",
   "metadata": {},
   "source": [
    "By inspecting the shape of this, we can see that we have a 2-dimensional array with dimensions `[token, embedding]` where the embeddings are 768 dimensional in this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e1851492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(511581, 768)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7459e498",
   "metadata": {},
   "source": [
    "Now that we have token embeddings for each text, it is possible to pool these embeddings to obtain an embedding for the full text (for this case, this embedding would represent the word itself. We can use the `.pool_token_embeddings()` method for doing this.\n",
    "\n",
    "Again, there are several methods and full details can be found in the documentation, but a few are:\n",
    "\n",
    "- take the mean of the token embeddings\n",
    "    - use `.pool_token_embeddings(method = \"mean\")`\n",
    "- take the element-wise max of the token embeddings\n",
    "    - use `.pool_token_embeddings(method = \"max\")`\n",
    "- take the element-wise sum of the token embeddings\n",
    "    - use `.pool_token_embeddings(method = \"sum\")`\n",
    "- take the token-embedding for the CLS token\n",
    "    - this is a special token that is used in some transformers like BERT\n",
    "    - but this is only available to us if we set `skip_special_tokens=False` when tokenizing the text (note by default, this is set to `True`)\n",
    "    - use `.pool_token_embeddings(method = \"sum\")`\n",
    "        - this will produce an error if the CLS token is not available..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cffd88db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████| 60000/60000 [00:23<00:00, 2530.65it/s]\n"
     ]
    }
   ],
   "source": [
    "pooled_embeddings = text_encoder.pool_token_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7832f9bc",
   "metadata": {},
   "source": [
    "Again, we can inspect the shape and we can see that we have embeddings for each of our words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "41e4f4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 768)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927da67c",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "The above embeddings will not be good for any downstream task as the model itself has not been trained to the text. For this we will use other methods in the `TextEncoder` class which allows us to do this by using the Huggingface trainer API.\n",
    "\n",
    "First, we need to set up a data collator for training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5677cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data_collator for language modelling (has dynamic padding)\n",
    "data_collator_for_LM = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "                                                       mlm=True,\n",
    "                                                       mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9c23f",
   "metadata": {},
   "source": [
    "To train our dataset, we will split it into a train, validation and test set with the `.split_dataset()` method.\n",
    "\n",
    "We can set up the trainer's arguments with `.set_up_training_args()` which sets up a `TrainingArguments` object (from the `transformers` package) and stores it in the `.training_args` attribute. And lastly, we set up a `Trainer` object (from the `transformers` package) and store it in the `.trainer` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "490c2393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Splitting up dataset into train / validation / test sets, and saving to `.dataset_split`.\n",
      "[INFO] Setting up TrainingArguments object and saving to `.training_args`.\n",
      "[INFO] Setting up Trainer object, and saving to `.trainer`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x2c1d9b580>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.split_dataset()\n",
    "text_encoder.set_up_training_args(output_dir=\"CHAR_BERT_trained\",\n",
    "                                  num_train_epochs=20,\n",
    "                                  per_device_train_batch_size=128,\n",
    "                                  seed=seed)\n",
    "text_encoder.set_up_trainer(data_collator=data_collator_for_LM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe59eed2",
   "metadata": {},
   "source": [
    "Once everything is set up, we just train our model by calling `.fit_transformer_with_trainer_api()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e05dd2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/Users/rchan/opt/miniconda3/envs/py38esig/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 48000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7500\n",
      "  Number of trainable parameters = 43560249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training model with 43560249 parameters...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 2:21:13, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.163147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.399900</td>\n",
       "      <td>1.925708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.965900</td>\n",
       "      <td>1.826439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.847300</td>\n",
       "      <td>1.722984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.847300</td>\n",
       "      <td>1.702801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.761900</td>\n",
       "      <td>1.667223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.699100</td>\n",
       "      <td>1.640108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.669400</td>\n",
       "      <td>1.608557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.669400</td>\n",
       "      <td>1.577421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.627500</td>\n",
       "      <td>1.583390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.594200</td>\n",
       "      <td>1.540734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.565700</td>\n",
       "      <td>1.520202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.565700</td>\n",
       "      <td>1.513731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.544900</td>\n",
       "      <td>1.489817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.518200</td>\n",
       "      <td>1.475588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.503700</td>\n",
       "      <td>1.481067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.503700</td>\n",
       "      <td>1.422606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.488300</td>\n",
       "      <td>1.436395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.483100</td>\n",
       "      <td>1.466076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.476300</td>\n",
       "      <td>1.433229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-1000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-1000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-1500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-1500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-2000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-2000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-2500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-2500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-3000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-3000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-3500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-3500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-4000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-4000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-4500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-4500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-5000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-5000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-5500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-5500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-5500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-6000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-6000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-6500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-6500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-6500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-7000\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-7000/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-7000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to CHAR_BERT_trained/checkpoint-7500\n",
      "Configuration saved in CHAR_BERT_trained/checkpoint-7500/config.json\n",
      "Model weights saved in CHAR_BERT_trained/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/checkpoint-7500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word, special_tokens_mask, tokens, language. If word, special_tokens_mask, tokens, language are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training completed!\n"
     ]
    }
   ],
   "source": [
    "text_encoder.fit_transformer_with_trainer_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5b0ae3",
   "metadata": {},
   "source": [
    "Saving our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2330b558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to CHAR_BERT_trained/\n",
      "Configuration saved in CHAR_BERT_trained/config.json\n",
      "Model weights saved in CHAR_BERT_trained/pytorch_model.bin\n",
      "tokenizer config file saved in CHAR_BERT_trained/tokenizer_config.json\n",
      "Special tokens file saved in CHAR_BERT_trained/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "text_encoder.trainer.save_model(\"CHAR_BERT_trained/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6978236",
   "metadata": {},
   "source": [
    "## Evaluating trained model\n",
    "\n",
    "Evaluating the performance on predicting the masked letter for the test dataset. To do this, for each word in our test dataset, we will mask each letter on its own and ask the model to predict the masked letter. So for a 5 letter word, we have 5 predictions to make - one for each letter given the other letters.\n",
    "\n",
    "For our tokenizer, we see that `\\<mask>` is used as the mask token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0ccfacdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6fdb04bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file CHAR_BERT_trained/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"CHAR_BERT_trained\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 57\n",
      "}\n",
      "\n",
      "loading configuration file CHAR_BERT_trained/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"CHAR_BERT_trained\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 57\n",
      "}\n",
      "\n",
      "loading weights file CHAR_BERT_trained/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at CHAR_BERT_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with 6000 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 6000/6000 [23:59<00:00,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5537810994198052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5537810994198052"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_masked_character_accuracy(fill_mask, words):\n",
    "    was_correct = []\n",
    "    print(f\"Evaluating with {len(words)} words\")\n",
    "    for word in tqdm(words):\n",
    "        masked_strings = [word[:i] + '<mask>' + word[i+1:] for i in range(len(word))]\n",
    "        predictions = [fill_mask(word)[0]['sequence'] for word in masked_strings]\n",
    "        was_correct += [pred == word for pred in predictions]\n",
    "    \n",
    "    acc = np.sum(was_correct) / len(was_correct)\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    return acc\n",
    "\n",
    "fill_mask = pipeline(\"fill-mask\",\n",
    "                     model=\"CHAR_BERT_trained\",\n",
    "                     tokenizer=\"CHAR_BERT_trained\")\n",
    "\n",
    "compute_masked_character_accuracy(fill_mask, \n",
    "                                  text_encoder.dataset_split[\"test\"][\"word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1ad55",
   "metadata": {},
   "source": [
    "## Obtaining a path for each word\n",
    "\n",
    "Now that we have trained our model, we want to obtain embeddings for the words in `corpus_df`. Currently, `TextEncoder` only works with the data that is passed into the function and stored in `.df` and `.dataset`, so we need to initialise a new `TextEncoder` object with the `corpus_df` dataframe and also the trained model.\n",
    "\n",
    "We can then obtain embeddings easily (recall from above we first need to tokenize the text, and then use the `.obtain_embeddings()` and `.pool_token_embeddings()` methods to do this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "87ba9ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_2 = nlpsig.TextEncoder(df=corpus_sample_df,\n",
    "                                    feature_name=\"word\",\n",
    "                                    model=text_encoder.model,\n",
    "                                    config=text_encoder.config,\n",
    "                                    tokenizer=text_encoder.tokenizer,\n",
    "                                    data_collator=text_encoder.data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "713edf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting return_special_tokens_mask=True\n",
      "[INFO] Tokenizing the datatset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e1e501bcf04628928be4f17168caf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving the tokenized text for each sentence into `.df['tokens']`...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca775fa877f441f5a2db68e9dd579780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating tokenized dataframe and setting in `.tokenized_df` attribute...\n",
      "[INFO] Note: 'text_id' is the column name for denoting the corresponding text id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                        | 0/120 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▎                               | 1/120 [00:00<00:37,  3.16it/s]\u001b[A\n",
      "  2%|▌                               | 2/120 [00:00<00:36,  3.25it/s]\u001b[A\n",
      "  2%|▊                               | 3/120 [00:00<00:35,  3.28it/s]\u001b[A\n",
      "  3%|█                               | 4/120 [00:01<00:34,  3.40it/s]\u001b[A\n",
      "  4%|█▎                              | 5/120 [00:01<00:33,  3.44it/s]\u001b[A\n",
      "  5%|█▌                              | 6/120 [00:01<00:34,  3.33it/s]\u001b[A\n",
      "  6%|█▊                              | 7/120 [00:02<00:32,  3.45it/s]\u001b[A\n",
      "  7%|██▏                             | 8/120 [00:02<00:32,  3.43it/s]\u001b[A\n",
      "  8%|██▍                             | 9/120 [00:02<00:33,  3.34it/s]\u001b[A\n",
      "  8%|██▌                            | 10/120 [00:02<00:32,  3.38it/s]\u001b[A\n",
      "  9%|██▊                            | 11/120 [00:03<00:31,  3.41it/s]\u001b[A\n",
      " 10%|███                            | 12/120 [00:03<00:32,  3.37it/s]\u001b[A\n",
      " 11%|███▎                           | 13/120 [00:03<00:31,  3.44it/s]\u001b[A\n",
      " 12%|███▌                           | 14/120 [00:04<00:31,  3.41it/s]\u001b[A\n",
      " 12%|███▉                           | 15/120 [00:04<00:31,  3.38it/s]\u001b[A\n",
      " 13%|████▏                          | 16/120 [00:04<00:31,  3.35it/s]\u001b[A\n",
      " 14%|████▍                          | 17/120 [00:05<00:31,  3.26it/s]\u001b[A\n",
      " 15%|████▋                          | 18/120 [00:05<00:30,  3.40it/s]\u001b[A\n",
      " 16%|████▉                          | 19/120 [00:05<00:29,  3.38it/s]\u001b[A\n",
      " 17%|█████▏                         | 20/120 [00:05<00:29,  3.41it/s]\u001b[A\n",
      " 18%|█████▍                         | 21/120 [00:06<00:27,  3.54it/s]\u001b[A\n",
      " 18%|█████▋                         | 22/120 [00:06<00:26,  3.71it/s]\u001b[A\n",
      " 19%|█████▉                         | 23/120 [00:06<00:26,  3.65it/s]\u001b[A\n",
      " 20%|██████▏                        | 24/120 [00:06<00:25,  3.84it/s]\u001b[A\n",
      " 21%|██████▍                        | 25/120 [00:07<00:23,  4.04it/s]\u001b[A\n",
      " 22%|██████▋                        | 26/120 [00:07<00:22,  4.10it/s]\u001b[A\n",
      " 22%|██████▉                        | 27/120 [00:07<00:23,  3.90it/s]\u001b[A\n",
      " 23%|███████▏                       | 28/120 [00:07<00:22,  4.04it/s]\u001b[A\n",
      " 24%|███████▍                       | 29/120 [00:08<00:21,  4.14it/s]\u001b[A\n",
      " 25%|███████▊                       | 30/120 [00:08<00:21,  4.21it/s]\u001b[A\n",
      " 26%|████████                       | 31/120 [00:08<00:20,  4.26it/s]\u001b[A\n",
      " 27%|████████▎                      | 32/120 [00:08<00:20,  4.30it/s]\u001b[A\n",
      " 28%|████████▌                      | 33/120 [00:09<00:20,  4.33it/s]\u001b[A\n",
      " 28%|████████▊                      | 34/120 [00:09<00:19,  4.35it/s]\u001b[A\n",
      " 29%|█████████                      | 35/120 [00:09<00:20,  4.25it/s]\u001b[A\n",
      " 30%|█████████▎                     | 36/120 [00:09<00:19,  4.32it/s]\u001b[A\n",
      " 31%|█████████▌                     | 37/120 [00:09<00:19,  4.36it/s]\u001b[A\n",
      " 32%|█████████▊                     | 38/120 [00:10<00:18,  4.37it/s]\u001b[A\n",
      " 32%|██████████                     | 39/120 [00:10<00:18,  4.48it/s]\u001b[A\n",
      " 33%|██████████▎                    | 40/120 [00:10<00:18,  4.31it/s]\u001b[A\n",
      " 34%|██████████▌                    | 41/120 [00:10<00:18,  4.22it/s]\u001b[A\n",
      " 35%|██████████▊                    | 42/120 [00:11<00:19,  4.01it/s]\u001b[A\n",
      " 36%|███████████                    | 43/120 [00:11<00:18,  4.08it/s]\u001b[A\n",
      " 37%|███████████▎                   | 44/120 [00:11<00:18,  4.09it/s]\u001b[A\n",
      " 38%|███████████▋                   | 45/120 [00:11<00:18,  4.14it/s]\u001b[A\n",
      " 38%|███████████▉                   | 46/120 [00:12<00:17,  4.23it/s]\u001b[A\n",
      " 39%|████████████▏                  | 47/120 [00:12<00:17,  4.19it/s]\u001b[A\n",
      " 40%|████████████▍                  | 48/120 [00:12<00:17,  4.16it/s]\u001b[A\n",
      " 41%|████████████▋                  | 49/120 [00:12<00:17,  4.09it/s]\u001b[A\n",
      " 42%|████████████▉                  | 50/120 [00:13<00:17,  4.11it/s]\u001b[A\n",
      " 42%|█████████████▏                 | 51/120 [00:13<00:16,  4.15it/s]\u001b[A\n",
      " 43%|█████████████▍                 | 52/120 [00:13<00:16,  4.13it/s]\u001b[A\n",
      " 44%|█████████████▋                 | 53/120 [00:13<00:16,  4.07it/s]\u001b[A\n",
      " 45%|█████████████▉                 | 54/120 [00:14<00:15,  4.19it/s]\u001b[A\n",
      " 46%|██████████████▏                | 55/120 [00:14<00:15,  4.29it/s]\u001b[A\n",
      " 47%|██████████████▍                | 56/120 [00:14<00:15,  4.24it/s]\u001b[A\n",
      " 48%|██████████████▋                | 57/120 [00:14<00:14,  4.37it/s]\u001b[A\n",
      " 48%|██████████████▉                | 58/120 [00:14<00:14,  4.37it/s]\u001b[A\n",
      " 49%|███████████████▏               | 59/120 [00:15<00:14,  4.34it/s]\u001b[A\n",
      " 50%|███████████████▌               | 60/120 [00:15<00:13,  4.42it/s]\u001b[A\n",
      " 51%|███████████████▊               | 61/120 [00:15<00:15,  3.90it/s]\u001b[A\n",
      " 52%|████████████████               | 62/120 [00:16<00:16,  3.52it/s]\u001b[A\n",
      " 52%|████████████████▎              | 63/120 [00:16<00:15,  3.62it/s]\u001b[A\n",
      " 53%|████████████████▌              | 64/120 [00:16<00:15,  3.66it/s]\u001b[A\n",
      " 54%|████████████████▊              | 65/120 [00:16<00:14,  3.75it/s]\u001b[A\n",
      " 55%|█████████████████              | 66/120 [00:17<00:15,  3.54it/s]\u001b[A\n",
      " 56%|█████████████████▎             | 67/120 [00:17<00:16,  3.19it/s]\u001b[A\n",
      " 57%|█████████████████▌             | 68/120 [00:17<00:15,  3.26it/s]\u001b[A\n",
      " 57%|█████████████████▊             | 69/120 [00:18<00:17,  3.00it/s]\u001b[A\n",
      " 58%|██████████████████             | 70/120 [00:18<00:16,  2.98it/s]\u001b[A\n",
      " 59%|██████████████████▎            | 71/120 [00:18<00:15,  3.20it/s]\u001b[A\n",
      " 60%|██████████████████▌            | 72/120 [00:19<00:14,  3.40it/s]\u001b[A\n",
      " 61%|██████████████████▊            | 73/120 [00:19<00:13,  3.46it/s]\u001b[A\n",
      " 62%|███████████████████            | 74/120 [00:19<00:14,  3.20it/s]\u001b[A\n",
      " 62%|███████████████████▍           | 75/120 [00:20<00:14,  3.05it/s]\u001b[A\n",
      " 63%|███████████████████▋           | 76/120 [00:20<00:13,  3.29it/s]\u001b[A\n",
      " 64%|███████████████████▉           | 77/120 [00:20<00:12,  3.46it/s]\u001b[A\n",
      " 65%|████████████████████▏          | 78/120 [00:20<00:11,  3.52it/s]\u001b[A\n",
      " 66%|████████████████████▍          | 79/120 [00:21<00:11,  3.70it/s]\u001b[A\n",
      " 67%|████████████████████▋          | 80/120 [00:21<00:10,  3.70it/s]\u001b[A\n",
      " 68%|████████████████████▉          | 81/120 [00:21<00:10,  3.59it/s]\u001b[A\n",
      " 68%|█████████████████████▏         | 82/120 [00:21<00:10,  3.49it/s]\u001b[A\n",
      " 69%|█████████████████████▍         | 83/120 [00:22<00:10,  3.47it/s]\u001b[A\n",
      " 70%|█████████████████████▋         | 84/120 [00:22<00:10,  3.55it/s]\u001b[A\n",
      " 71%|█████████████████████▉         | 85/120 [00:22<00:09,  3.56it/s]\u001b[A\n",
      " 72%|██████████████████████▏        | 86/120 [00:23<00:09,  3.60it/s]\u001b[A\n",
      " 72%|██████████████████████▍        | 87/120 [00:23<00:09,  3.55it/s]\u001b[A\n",
      " 73%|██████████████████████▋        | 88/120 [00:23<00:08,  3.65it/s]\u001b[A\n",
      " 74%|██████████████████████▉        | 89/120 [00:23<00:08,  3.63it/s]\u001b[A\n",
      " 75%|███████████████████████▎       | 90/120 [00:24<00:08,  3.54it/s]\u001b[A\n",
      " 76%|███████████████████████▌       | 91/120 [00:24<00:08,  3.51it/s]\u001b[A\n",
      " 77%|███████████████████████▊       | 92/120 [00:24<00:07,  3.55it/s]\u001b[A\n",
      " 78%|████████████████████████       | 93/120 [00:25<00:07,  3.60it/s]\u001b[A\n",
      " 78%|████████████████████████▎      | 94/120 [00:25<00:07,  3.66it/s]\u001b[A\n",
      " 79%|████████████████████████▌      | 95/120 [00:25<00:06,  3.68it/s]\u001b[A\n",
      " 80%|████████████████████████▊      | 96/120 [00:25<00:06,  3.75it/s]\u001b[A\n",
      " 81%|█████████████████████████      | 97/120 [00:26<00:06,  3.80it/s]\u001b[A\n",
      " 82%|█████████████████████████▎     | 98/120 [00:26<00:05,  3.76it/s]\u001b[A\n",
      " 82%|█████████████████████████▌     | 99/120 [00:26<00:05,  3.77it/s]\u001b[A\n",
      " 83%|█████████████████████████     | 100/120 [00:26<00:05,  3.68it/s]\u001b[A\n",
      " 84%|█████████████████████████▎    | 101/120 [00:27<00:04,  3.86it/s]\u001b[A\n",
      " 85%|█████████████████████████▌    | 102/120 [00:27<00:04,  3.93it/s]\u001b[A\n",
      " 86%|█████████████████████████▊    | 103/120 [00:27<00:04,  3.97it/s]\u001b[A\n",
      " 87%|██████████████████████████    | 104/120 [00:27<00:03,  4.11it/s]\u001b[A\n",
      " 88%|██████████████████████████▎   | 105/120 [00:28<00:03,  4.06it/s]\u001b[A\n",
      " 88%|██████████████████████████▌   | 106/120 [00:28<00:03,  4.13it/s]\u001b[A\n",
      " 89%|██████████████████████████▊   | 107/120 [00:28<00:03,  4.07it/s]\u001b[A\n",
      " 90%|███████████████████████████   | 108/120 [00:28<00:02,  4.04it/s]\u001b[A\n",
      " 91%|███████████████████████████▎  | 109/120 [00:29<00:02,  4.11it/s]\u001b[A\n",
      " 92%|███████████████████████████▌  | 110/120 [00:29<00:02,  3.89it/s]\u001b[A\n",
      " 92%|███████████████████████████▊  | 111/120 [00:29<00:02,  3.95it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|████████████████████████████  | 112/120 [00:29<00:02,  3.80it/s]\u001b[A\n",
      " 94%|████████████████████████████▎ | 113/120 [00:30<00:01,  3.79it/s]\u001b[A\n",
      " 95%|████████████████████████████▌ | 114/120 [00:30<00:01,  3.77it/s]\u001b[A\n",
      " 96%|████████████████████████████▊ | 115/120 [00:30<00:01,  3.64it/s]\u001b[A\n",
      " 97%|█████████████████████████████ | 116/120 [00:31<00:01,  3.69it/s]\u001b[A\n",
      " 98%|█████████████████████████████▎| 117/120 [00:31<00:00,  3.81it/s]\u001b[A\n",
      " 98%|█████████████████████████████▌| 118/120 [00:31<00:00,  3.79it/s]\u001b[A\n",
      " 99%|█████████████████████████████▊| 119/120 [00:31<00:00,  3.86it/s]\u001b[A\n",
      "100%|██████████████████████████████| 120/120 [00:32<00:00,  3.75it/s]\u001b[A\n",
      "\n",
      "  0%|                                      | 0/12000 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|█▊                        | 832/12000 [00:00<00:01, 8313.53it/s]\u001b[A\n",
      " 14%|███▍                     | 1664/12000 [00:00<00:01, 8268.38it/s]\u001b[A\n",
      " 21%|█████▏                   | 2491/12000 [00:00<00:01, 8210.05it/s]\u001b[A\n",
      " 28%|██████▉                  | 3313/12000 [00:00<00:01, 8109.57it/s]\u001b[A\n",
      " 34%|████████▌                | 4125/12000 [00:00<00:01, 7672.88it/s]\u001b[A\n",
      " 41%|██████████▎              | 4954/12000 [00:00<00:00, 7870.82it/s]\u001b[A\n",
      " 48%|███████████▉             | 5745/12000 [00:00<00:00, 7297.68it/s]\u001b[A\n",
      " 55%|█████████████▋           | 6577/12000 [00:00<00:00, 7599.07it/s]\u001b[A\n",
      " 61%|███████████████▎         | 7346/12000 [00:00<00:00, 6939.66it/s]\u001b[A\n",
      " 67%|████████████████▊        | 8055/12000 [00:01<00:00, 6848.64it/s]\u001b[A\n",
      " 73%|██████████████████▏      | 8750/12000 [00:01<00:00, 6299.02it/s]\u001b[A\n",
      " 78%|███████████████████▌     | 9393/12000 [00:01<00:00, 6076.55it/s]\u001b[A\n",
      " 83%|████████████████████    | 10009/12000 [00:01<00:00, 5965.81it/s]\u001b[A\n",
      " 90%|█████████████████████▌  | 10811/12000 [00:01<00:00, 6525.47it/s]\u001b[A\n",
      "100%|████████████████████████| 12000/12000 [00:01<00:00, 7098.62it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "text_encoder_2.tokenize_text()\n",
    "token_embeddings = text_encoder_2.obtain_embeddings(method = \"hidden_layer\")\n",
    "pooled_embeddings = text_encoder_2.pool_token_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3ed8f78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130063</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130064</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130065</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130066</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130067</th>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130068 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id language tokens\n",
       "0             0       de      s\n",
       "1             0       de      t\n",
       "2             0       de      e\n",
       "3             0       de      r\n",
       "4             0       de      i\n",
       "...         ...      ...    ...\n",
       "130063    11999       sv      n\n",
       "130064    11999       sv      g\n",
       "130065    11999       sv      a\n",
       "130066    11999       sv      r\n",
       "130067    11999       sv      s\n",
       "\n",
       "[130068 rows x 3 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_2.tokenized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4c1885be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130068, 768)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5698a3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 768)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec9e3e",
   "metadata": {},
   "source": [
    "## Dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5511d",
   "metadata": {},
   "source": [
    "We can perform dimension reduction with `nlpsig` using the `DimReduce` class. Here, we will use UMAP (implemented using the [`umap-learn`](https://umap-learn.readthedocs.io/en/latest/api.html) package, but there are other standard methods available:\n",
    "- PCA (implemented using [`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html))\n",
    "    - `method=\"pca\"`\n",
    "- TSNE (implemented using [`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html))\n",
    "    - `method=\"tsne\"`\n",
    "- Post Processing Algorithm (PPA) with PCA (PPA-PCA)\n",
    "    - `method=\"ppapca\"`\n",
    "    - see _Mu, J., Bhat, S., and Viswanath, P. (2017). All-but-the-top: Simple and effective postprocessing for word representations. arXiv preprint arXiv:1702.01417._\n",
    "- PPA-PCA-PPA\n",
    "    - `method=\"ppapacppa\"`\n",
    "    - see _Raunak, V., Gupta, V., and Metze, F. (2019). Effective dimensionality reduction for word embeddings. In Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP- 2019), pages 235–243._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "59a6234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = nlpsig.DimReduce(method=\"umap\",\n",
    "                             n_components=10,\n",
    "                             dim_reduction_kwargs={\n",
    "                                 \"metric\": \"cosine\",\n",
    "                             })\n",
    "embeddings_reduced = reduction.fit_transform(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8ff30831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130068, 10)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03f86ba",
   "metadata": {},
   "source": [
    "As we have embeddings for each token, we can obtain a path for each word by constructing a path of the token embeddings. To do this, we can use the `PrepareData` class and pass in our tokenized dataframe (the dataframe where we have each token in our data and we also have the corresponding id for each word which is saved in the `text_id` column of the tokenized dataframe.\n",
    "\n",
    "We pass in the column which defines the ids, `text_id`, the column which defines the labels, `language`, the token embeddings and the pooled embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "83bd0e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Note 'datetime' is not a column in `.df`, so only 'timeline_index' is added.\n",
      "[INFO] As 'datetime' is not a column in `.df`, we assume that the data is ordered by time with respect to the id.\n",
      "[INFO] Adding 'timeline_index' feature...\n"
     ]
    }
   ],
   "source": [
    "dataset = nlpsig.PrepareData(text_encoder_2.tokenized_df,\n",
    "                             id_column=\"text_id\",\n",
    "                             labels_column=\"language\",\n",
    "                             embeddings=token_embeddings,\n",
    "                             embeddings_reduced=embeddings_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89126b9",
   "metadata": {},
   "source": [
    "We can construct a path by using the `.pad()` method, and result of this is a multi-dimensional array or tensor (in particular a numpy array or PyTorch tensor) which can be then used in some downstream task. It is called \"pad\" because arrays and tensors are rectangular and if there are cases where there isn't enough data (e.g. if a word only has 3 letters/tokens and we want to make paths of length 4), we \"pad\" with either the last token embedding (set `zero_padding=False`) or with zeros (set `zero_padding=True`).\n",
    "\n",
    "Here, we construct paths by setting a length of the paths (we call this method `k_last` in the code and we have to specify the length with `k=10`). We alternatively can construct to the longest word possible (by setting `method=\"max\"`). The `time_feature` argument allows us to specify what time features we want to keep. Here we don't have any besides the index in which the word is, which is given by `timeline_index` and we choose not to standardise that by specifying `standardise_time_feature=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "acebfe66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████| 12000/12000 [00:06<00:00, 1888.12it/s]\n"
     ]
    }
   ],
   "source": [
    "word_path = dataset.pad(pad_by=\"id\",\n",
    "                        zero_padding=True,\n",
    "                        method=\"k_last\",\n",
    "                        k=10,\n",
    "                        time_feature=[\"timeline_index\"],\n",
    "                        standardise_time_feature=False,\n",
    "                        embeddings=\"dim_reduced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1821cda1",
   "metadata": {},
   "source": [
    "By inspecting the shape of `word_path`, we see that we have a path for each word and the dimension of the array is `[batch, length of path, channels]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "058cede0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 10, 13)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6e0f9dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.df[\"text_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07122f7",
   "metadata": {},
   "source": [
    "We store this array as a dataframe in `.df_padded` so that you can see what the columns correspond to, where columns beginning with `e` denote the dimensions of embeddings obtained from the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "70089c53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeline_index</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>d10</th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9.316443</td>\n",
       "      <td>4.845233</td>\n",
       "      <td>4.327503</td>\n",
       "      <td>12.503558</td>\n",
       "      <td>6.330883</td>\n",
       "      <td>9.639407</td>\n",
       "      <td>5.759614</td>\n",
       "      <td>5.490386</td>\n",
       "      <td>8.510323</td>\n",
       "      <td>4.942838</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8.887857</td>\n",
       "      <td>5.960692</td>\n",
       "      <td>6.048943</td>\n",
       "      <td>7.624972</td>\n",
       "      <td>4.420071</td>\n",
       "      <td>1.446805</td>\n",
       "      <td>5.870730</td>\n",
       "      <td>4.063160</td>\n",
       "      <td>2.149124</td>\n",
       "      <td>3.452542</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9.593655</td>\n",
       "      <td>5.001677</td>\n",
       "      <td>4.819080</td>\n",
       "      <td>0.763647</td>\n",
       "      <td>8.600629</td>\n",
       "      <td>6.855587</td>\n",
       "      <td>0.544169</td>\n",
       "      <td>2.737365</td>\n",
       "      <td>8.712207</td>\n",
       "      <td>4.943607</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.738387</td>\n",
       "      <td>5.116134</td>\n",
       "      <td>2.100512</td>\n",
       "      <td>4.531888</td>\n",
       "      <td>4.817560</td>\n",
       "      <td>-0.247809</td>\n",
       "      <td>8.997357</td>\n",
       "      <td>2.103364</td>\n",
       "      <td>9.245868</td>\n",
       "      <td>4.446329</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9.959823</td>\n",
       "      <td>4.621105</td>\n",
       "      <td>7.982635</td>\n",
       "      <td>2.020440</td>\n",
       "      <td>7.114519</td>\n",
       "      <td>8.217345</td>\n",
       "      <td>5.923898</td>\n",
       "      <td>9.328344</td>\n",
       "      <td>4.378319</td>\n",
       "      <td>5.210546</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>6</td>\n",
       "      <td>10.747741</td>\n",
       "      <td>4.271384</td>\n",
       "      <td>1.787655</td>\n",
       "      <td>3.467277</td>\n",
       "      <td>0.551456</td>\n",
       "      <td>8.785113</td>\n",
       "      <td>2.832803</td>\n",
       "      <td>2.179157</td>\n",
       "      <td>4.572252</td>\n",
       "      <td>7.398472</td>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>7</td>\n",
       "      <td>9.875523</td>\n",
       "      <td>5.655285</td>\n",
       "      <td>4.204875</td>\n",
       "      <td>3.621913</td>\n",
       "      <td>5.814768</td>\n",
       "      <td>-0.771930</td>\n",
       "      <td>3.293646</td>\n",
       "      <td>4.322257</td>\n",
       "      <td>3.750905</td>\n",
       "      <td>4.650285</td>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>8</td>\n",
       "      <td>18.076136</td>\n",
       "      <td>4.470689</td>\n",
       "      <td>4.693927</td>\n",
       "      <td>5.279918</td>\n",
       "      <td>4.913960</td>\n",
       "      <td>5.447477</td>\n",
       "      <td>3.608374</td>\n",
       "      <td>5.227146</td>\n",
       "      <td>7.097644</td>\n",
       "      <td>5.407924</td>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>9</td>\n",
       "      <td>9.784361</td>\n",
       "      <td>5.126089</td>\n",
       "      <td>2.388112</td>\n",
       "      <td>4.863431</td>\n",
       "      <td>5.934732</td>\n",
       "      <td>0.836809</td>\n",
       "      <td>7.378283</td>\n",
       "      <td>1.178062</td>\n",
       "      <td>8.950499</td>\n",
       "      <td>3.723052</td>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>10</td>\n",
       "      <td>8.561878</td>\n",
       "      <td>4.887213</td>\n",
       "      <td>7.236189</td>\n",
       "      <td>11.014791</td>\n",
       "      <td>6.550700</td>\n",
       "      <td>1.987310</td>\n",
       "      <td>1.908966</td>\n",
       "      <td>5.173742</td>\n",
       "      <td>8.238629</td>\n",
       "      <td>6.461129</td>\n",
       "      <td>11999</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        timeline_index         d1        d2        d3         d4        d5  \\\n",
       "0                    0   9.316443  4.845233  4.327503  12.503558  6.330883   \n",
       "1                    1   8.887857  5.960692  6.048943   7.624972  4.420071   \n",
       "2                    2   9.593655  5.001677  4.819080   0.763647  8.600629   \n",
       "3                    3  10.738387  5.116134  2.100512   4.531888  4.817560   \n",
       "4                    4   9.959823  4.621105  7.982635   2.020440  7.114519   \n",
       "...                ...        ...       ...       ...        ...       ...   \n",
       "119995               6  10.747741  4.271384  1.787655   3.467277  0.551456   \n",
       "119996               7   9.875523  5.655285  4.204875   3.621913  5.814768   \n",
       "119997               8  18.076136  4.470689  4.693927   5.279918  4.913960   \n",
       "119998               9   9.784361  5.126089  2.388112   4.863431  5.934732   \n",
       "119999              10   8.561878  4.887213  7.236189  11.014791  6.550700   \n",
       "\n",
       "              d6        d7        d8        d9       d10  text_id language  \n",
       "0       9.639407  5.759614  5.490386  8.510323  4.942838        0       de  \n",
       "1       1.446805  5.870730  4.063160  2.149124  3.452542        0       de  \n",
       "2       6.855587  0.544169  2.737365  8.712207  4.943607        0       de  \n",
       "3      -0.247809  8.997357  2.103364  9.245868  4.446329        0       de  \n",
       "4       8.217345  5.923898  9.328344  4.378319  5.210546        0       de  \n",
       "...          ...       ...       ...       ...       ...      ...      ...  \n",
       "119995  8.785113  2.832803  2.179157  4.572252  7.398472    11999       sv  \n",
       "119996 -0.771930  3.293646  4.322257  3.750905  4.650285    11999       sv  \n",
       "119997  5.447477  3.608374  5.227146  7.097644  5.407924    11999       sv  \n",
       "119998  0.836809  7.378283  1.178062  8.950499  3.723052    11999       sv  \n",
       "119999  1.987310  1.908966  5.173742  8.238629  6.461129    11999       sv  \n",
       "\n",
       "[120000 rows x 13 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c71aaf",
   "metadata": {},
   "source": [
    "We can see that the first column corresponds to the index, the columns beginning with `d` correspond to the dimension-reduced embeddings (which were 10 dimensional), and we also have the corresponding text-id and language (which we passed in the label above). If we look at the first word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "69ca9451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeline_index</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>d10</th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9.316443</td>\n",
       "      <td>4.845233</td>\n",
       "      <td>4.327503</td>\n",
       "      <td>12.503558</td>\n",
       "      <td>6.330883</td>\n",
       "      <td>9.639407</td>\n",
       "      <td>5.759614</td>\n",
       "      <td>5.490386</td>\n",
       "      <td>8.510323</td>\n",
       "      <td>4.942838</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8.887857</td>\n",
       "      <td>5.960692</td>\n",
       "      <td>6.048943</td>\n",
       "      <td>7.624972</td>\n",
       "      <td>4.420071</td>\n",
       "      <td>1.446805</td>\n",
       "      <td>5.870730</td>\n",
       "      <td>4.063160</td>\n",
       "      <td>2.149124</td>\n",
       "      <td>3.452542</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9.593655</td>\n",
       "      <td>5.001677</td>\n",
       "      <td>4.819080</td>\n",
       "      <td>0.763647</td>\n",
       "      <td>8.600629</td>\n",
       "      <td>6.855587</td>\n",
       "      <td>0.544169</td>\n",
       "      <td>2.737365</td>\n",
       "      <td>8.712207</td>\n",
       "      <td>4.943607</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.738387</td>\n",
       "      <td>5.116134</td>\n",
       "      <td>2.100512</td>\n",
       "      <td>4.531888</td>\n",
       "      <td>4.817560</td>\n",
       "      <td>-0.247809</td>\n",
       "      <td>8.997357</td>\n",
       "      <td>2.103364</td>\n",
       "      <td>9.245868</td>\n",
       "      <td>4.446329</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9.959823</td>\n",
       "      <td>4.621105</td>\n",
       "      <td>7.982635</td>\n",
       "      <td>2.020440</td>\n",
       "      <td>7.114519</td>\n",
       "      <td>8.217345</td>\n",
       "      <td>5.923898</td>\n",
       "      <td>9.328344</td>\n",
       "      <td>4.378319</td>\n",
       "      <td>5.210546</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>10.099976</td>\n",
       "      <td>5.472713</td>\n",
       "      <td>4.311344</td>\n",
       "      <td>0.887817</td>\n",
       "      <td>1.529893</td>\n",
       "      <td>2.656477</td>\n",
       "      <td>5.109613</td>\n",
       "      <td>1.446850</td>\n",
       "      <td>1.655324</td>\n",
       "      <td>6.259146</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>8.957418</td>\n",
       "      <td>4.832905</td>\n",
       "      <td>7.315309</td>\n",
       "      <td>11.573713</td>\n",
       "      <td>6.240843</td>\n",
       "      <td>5.481841</td>\n",
       "      <td>0.409815</td>\n",
       "      <td>3.607513</td>\n",
       "      <td>7.758046</td>\n",
       "      <td>5.539202</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>9.213166</td>\n",
       "      <td>6.091189</td>\n",
       "      <td>6.323124</td>\n",
       "      <td>7.939425</td>\n",
       "      <td>4.316284</td>\n",
       "      <td>0.784012</td>\n",
       "      <td>6.178899</td>\n",
       "      <td>4.419389</td>\n",
       "      <td>1.761438</td>\n",
       "      <td>3.250692</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>9.141431</td>\n",
       "      <td>5.303648</td>\n",
       "      <td>4.492472</td>\n",
       "      <td>1.272192</td>\n",
       "      <td>8.932236</td>\n",
       "      <td>6.521078</td>\n",
       "      <td>0.421085</td>\n",
       "      <td>2.273735</td>\n",
       "      <td>8.782001</td>\n",
       "      <td>5.859922</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9.047493</td>\n",
       "      <td>4.900075</td>\n",
       "      <td>5.746224</td>\n",
       "      <td>3.393111</td>\n",
       "      <td>4.659221</td>\n",
       "      <td>0.555108</td>\n",
       "      <td>9.175676</td>\n",
       "      <td>2.258206</td>\n",
       "      <td>10.495255</td>\n",
       "      <td>6.689592</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timeline_index         d1        d2        d3         d4        d5  \\\n",
       "0               0   9.316443  4.845233  4.327503  12.503558  6.330883   \n",
       "1               1   8.887857  5.960692  6.048943   7.624972  4.420071   \n",
       "2               2   9.593655  5.001677  4.819080   0.763647  8.600629   \n",
       "3               3  10.738387  5.116134  2.100512   4.531888  4.817560   \n",
       "4               4   9.959823  4.621105  7.982635   2.020440  7.114519   \n",
       "5               5  10.099976  5.472713  4.311344   0.887817  1.529893   \n",
       "6               6   8.957418  4.832905  7.315309  11.573713  6.240843   \n",
       "7               7   9.213166  6.091189  6.323124   7.939425  4.316284   \n",
       "8               8   9.141431  5.303648  4.492472   1.272192  8.932236   \n",
       "9               9   9.047493  4.900075  5.746224   3.393111  4.659221   \n",
       "\n",
       "         d6        d7        d8         d9       d10  text_id language  \n",
       "0  9.639407  5.759614  5.490386   8.510323  4.942838        0       de  \n",
       "1  1.446805  5.870730  4.063160   2.149124  3.452542        0       de  \n",
       "2  6.855587  0.544169  2.737365   8.712207  4.943607        0       de  \n",
       "3 -0.247809  8.997357  2.103364   9.245868  4.446329        0       de  \n",
       "4  8.217345  5.923898  9.328344   4.378319  5.210546        0       de  \n",
       "5  2.656477  5.109613  1.446850   1.655324  6.259146        0       de  \n",
       "6  5.481841  0.409815  3.607513   7.758046  5.539202        0       de  \n",
       "7  0.784012  6.178899  4.419389   1.761438  3.250692        0       de  \n",
       "8  6.521078  0.421085  2.273735   8.782001  5.859922        0       de  \n",
       "9  0.555108  9.175676  2.258206  10.495255  6.689592        0       de  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# still has the labels and the ids\n",
    "dataset.df_padded[dataset.df_padded[\"text_id\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f9f94c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word                            sterilster\n",
       "language                                de\n",
       "tokens      [s, t, e, r, i, l, s, t, e, r]\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_2.df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dfadea",
   "metadata": {},
   "source": [
    "We pick out a word which has less than 10 letters, and we can see that the path is padded with zeros and we give these a label `-1` to denote that they have been added.\n",
    "\n",
    "Note that for padding, the method pads from below by default, but we can pad by above by setting `pad_from_below=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0d64933e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeline_index</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>d10</th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33200</th>\n",
       "      <td>0</td>\n",
       "      <td>9.611249</td>\n",
       "      <td>4.867899</td>\n",
       "      <td>0.527855</td>\n",
       "      <td>6.496469</td>\n",
       "      <td>4.894460</td>\n",
       "      <td>7.605602</td>\n",
       "      <td>8.416375</td>\n",
       "      <td>8.800124</td>\n",
       "      <td>8.227205</td>\n",
       "      <td>2.436413</td>\n",
       "      <td>3320</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33201</th>\n",
       "      <td>1</td>\n",
       "      <td>10.523488</td>\n",
       "      <td>5.070083</td>\n",
       "      <td>4.103313</td>\n",
       "      <td>2.341068</td>\n",
       "      <td>7.382311</td>\n",
       "      <td>3.989184</td>\n",
       "      <td>8.398027</td>\n",
       "      <td>8.477536</td>\n",
       "      <td>-0.798858</td>\n",
       "      <td>6.452246</td>\n",
       "      <td>3320</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33202</th>\n",
       "      <td>2</td>\n",
       "      <td>6.772434</td>\n",
       "      <td>4.345559</td>\n",
       "      <td>3.611884</td>\n",
       "      <td>6.029821</td>\n",
       "      <td>3.352861</td>\n",
       "      <td>4.986651</td>\n",
       "      <td>4.431797</td>\n",
       "      <td>4.751046</td>\n",
       "      <td>5.456618</td>\n",
       "      <td>2.363499</td>\n",
       "      <td>3320</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33203</th>\n",
       "      <td>3</td>\n",
       "      <td>9.021976</td>\n",
       "      <td>5.100981</td>\n",
       "      <td>1.545537</td>\n",
       "      <td>3.304732</td>\n",
       "      <td>-0.104942</td>\n",
       "      <td>8.160237</td>\n",
       "      <td>3.559365</td>\n",
       "      <td>2.423001</td>\n",
       "      <td>4.994720</td>\n",
       "      <td>6.719332</td>\n",
       "      <td>3320</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33204</th>\n",
       "      <td>4</td>\n",
       "      <td>11.743196</td>\n",
       "      <td>5.022210</td>\n",
       "      <td>4.634274</td>\n",
       "      <td>3.535622</td>\n",
       "      <td>7.453012</td>\n",
       "      <td>4.446137</td>\n",
       "      <td>8.867769</td>\n",
       "      <td>8.490068</td>\n",
       "      <td>-0.583250</td>\n",
       "      <td>6.533859</td>\n",
       "      <td>3320</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33205</th>\n",
       "      <td>5</td>\n",
       "      <td>10.112203</td>\n",
       "      <td>3.670740</td>\n",
       "      <td>3.416387</td>\n",
       "      <td>5.942775</td>\n",
       "      <td>-0.490579</td>\n",
       "      <td>3.622755</td>\n",
       "      <td>5.881639</td>\n",
       "      <td>5.320231</td>\n",
       "      <td>5.691600</td>\n",
       "      <td>4.794139</td>\n",
       "      <td>3320</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33206</th>\n",
       "      <td>6</td>\n",
       "      <td>11.029837</td>\n",
       "      <td>4.917490</td>\n",
       "      <td>5.932513</td>\n",
       "      <td>0.115193</td>\n",
       "      <td>6.846435</td>\n",
       "      <td>7.232195</td>\n",
       "      <td>-0.030283</td>\n",
       "      <td>2.961208</td>\n",
       "      <td>8.966331</td>\n",
       "      <td>4.555634</td>\n",
       "      <td>3320</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33207</th>\n",
       "      <td>7</td>\n",
       "      <td>9.234504</td>\n",
       "      <td>5.119550</td>\n",
       "      <td>4.571543</td>\n",
       "      <td>2.619485</td>\n",
       "      <td>-3.523274</td>\n",
       "      <td>4.053810</td>\n",
       "      <td>1.409377</td>\n",
       "      <td>7.513707</td>\n",
       "      <td>6.222019</td>\n",
       "      <td>4.357717</td>\n",
       "      <td>3320</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33208</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3320</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33209</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3320</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       timeline_index         d1        d2        d3        d4        d5  \\\n",
       "33200               0   9.611249  4.867899  0.527855  6.496469  4.894460   \n",
       "33201               1  10.523488  5.070083  4.103313  2.341068  7.382311   \n",
       "33202               2   6.772434  4.345559  3.611884  6.029821  3.352861   \n",
       "33203               3   9.021976  5.100981  1.545537  3.304732 -0.104942   \n",
       "33204               4  11.743196  5.022210  4.634274  3.535622  7.453012   \n",
       "33205               5  10.112203  3.670740  3.416387  5.942775 -0.490579   \n",
       "33206               6  11.029837  4.917490  5.932513  0.115193  6.846435   \n",
       "33207               7   9.234504  5.119550  4.571543  2.619485 -3.523274   \n",
       "33208               0   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "33209               0   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "             d6        d7        d8        d9       d10  text_id language  \n",
       "33200  7.605602  8.416375  8.800124  8.227205  2.436413     3320       en  \n",
       "33201  3.989184  8.398027  8.477536 -0.798858  6.452246     3320       en  \n",
       "33202  4.986651  4.431797  4.751046  5.456618  2.363499     3320       en  \n",
       "33203  8.160237  3.559365  2.423001  4.994720  6.719332     3320       en  \n",
       "33204  4.446137  8.867769  8.490068 -0.583250  6.533859     3320       en  \n",
       "33205  3.622755  5.881639  5.320231  5.691600  4.794139     3320       en  \n",
       "33206  7.232195 -0.030283  2.961208  8.966331  4.555634     3320       en  \n",
       "33207  4.053810  1.409377  7.513707  6.222019  4.357717     3320       en  \n",
       "33208  0.000000  0.000000  0.000000  0.000000  0.000000     3320       -1  \n",
       "33209  0.000000  0.000000  0.000000  0.000000  0.000000     3320       -1  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df_padded[dataset.df_padded[\"text_id\"]==3320]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2478a250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word                        cognomen\n",
       "language                          en\n",
       "tokens      [c, o, g, n, o, m, e, n]\n",
       "Name: 3320, dtype: object"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_2.df.iloc[3320]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6df9a5",
   "metadata": {},
   "source": [
    "For words which are longer than 10 letters, we only take the last 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1ef8c150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeline_index</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>d10</th>\n",
       "      <th>text_id</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>4</td>\n",
       "      <td>6.765463</td>\n",
       "      <td>5.742875</td>\n",
       "      <td>7.315415</td>\n",
       "      <td>4.297583</td>\n",
       "      <td>7.099874</td>\n",
       "      <td>6.667671</td>\n",
       "      <td>1.551378</td>\n",
       "      <td>6.853375</td>\n",
       "      <td>2.757002</td>\n",
       "      <td>4.997705</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>5</td>\n",
       "      <td>10.138185</td>\n",
       "      <td>4.709245</td>\n",
       "      <td>2.165445</td>\n",
       "      <td>3.375592</td>\n",
       "      <td>0.713781</td>\n",
       "      <td>8.835148</td>\n",
       "      <td>2.635751</td>\n",
       "      <td>2.438843</td>\n",
       "      <td>4.611350</td>\n",
       "      <td>7.229488</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>6</td>\n",
       "      <td>6.249787</td>\n",
       "      <td>3.974104</td>\n",
       "      <td>4.045012</td>\n",
       "      <td>5.929241</td>\n",
       "      <td>3.161420</td>\n",
       "      <td>4.917922</td>\n",
       "      <td>4.442468</td>\n",
       "      <td>4.858975</td>\n",
       "      <td>5.672881</td>\n",
       "      <td>2.111718</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>7</td>\n",
       "      <td>9.160442</td>\n",
       "      <td>4.917056</td>\n",
       "      <td>7.164062</td>\n",
       "      <td>11.719777</td>\n",
       "      <td>6.225347</td>\n",
       "      <td>5.443470</td>\n",
       "      <td>0.693212</td>\n",
       "      <td>3.893485</td>\n",
       "      <td>7.622388</td>\n",
       "      <td>5.271125</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>8</td>\n",
       "      <td>8.451878</td>\n",
       "      <td>4.867537</td>\n",
       "      <td>3.390759</td>\n",
       "      <td>8.272752</td>\n",
       "      <td>5.518309</td>\n",
       "      <td>6.608521</td>\n",
       "      <td>5.366010</td>\n",
       "      <td>5.223764</td>\n",
       "      <td>3.831670</td>\n",
       "      <td>2.555008</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>9</td>\n",
       "      <td>11.370544</td>\n",
       "      <td>4.847795</td>\n",
       "      <td>4.392432</td>\n",
       "      <td>3.113414</td>\n",
       "      <td>7.140684</td>\n",
       "      <td>4.390954</td>\n",
       "      <td>9.180496</td>\n",
       "      <td>8.712993</td>\n",
       "      <td>-0.518369</td>\n",
       "      <td>6.244632</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>10</td>\n",
       "      <td>9.566058</td>\n",
       "      <td>5.396273</td>\n",
       "      <td>4.441339</td>\n",
       "      <td>0.899723</td>\n",
       "      <td>1.663759</td>\n",
       "      <td>2.592971</td>\n",
       "      <td>5.305820</td>\n",
       "      <td>1.212458</td>\n",
       "      <td>1.705024</td>\n",
       "      <td>6.345337</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>11</td>\n",
       "      <td>9.422722</td>\n",
       "      <td>5.814419</td>\n",
       "      <td>4.103125</td>\n",
       "      <td>1.476479</td>\n",
       "      <td>2.338355</td>\n",
       "      <td>2.783099</td>\n",
       "      <td>5.836319</td>\n",
       "      <td>0.453386</td>\n",
       "      <td>1.190403</td>\n",
       "      <td>5.976224</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>12</td>\n",
       "      <td>9.085876</td>\n",
       "      <td>5.155453</td>\n",
       "      <td>4.886012</td>\n",
       "      <td>0.073813</td>\n",
       "      <td>7.100619</td>\n",
       "      <td>6.242529</td>\n",
       "      <td>-0.049043</td>\n",
       "      <td>2.893509</td>\n",
       "      <td>9.982448</td>\n",
       "      <td>4.248672</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>13</td>\n",
       "      <td>9.709391</td>\n",
       "      <td>4.826661</td>\n",
       "      <td>5.155285</td>\n",
       "      <td>13.915672</td>\n",
       "      <td>4.762872</td>\n",
       "      <td>3.235626</td>\n",
       "      <td>2.735438</td>\n",
       "      <td>4.633454</td>\n",
       "      <td>7.020216</td>\n",
       "      <td>6.252982</td>\n",
       "      <td>10</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     timeline_index         d1        d2        d3         d4        d5  \\\n",
       "100               4   6.765463  5.742875  7.315415   4.297583  7.099874   \n",
       "101               5  10.138185  4.709245  2.165445   3.375592  0.713781   \n",
       "102               6   6.249787  3.974104  4.045012   5.929241  3.161420   \n",
       "103               7   9.160442  4.917056  7.164062  11.719777  6.225347   \n",
       "104               8   8.451878  4.867537  3.390759   8.272752  5.518309   \n",
       "105               9  11.370544  4.847795  4.392432   3.113414  7.140684   \n",
       "106              10   9.566058  5.396273  4.441339   0.899723  1.663759   \n",
       "107              11   9.422722  5.814419  4.103125   1.476479  2.338355   \n",
       "108              12   9.085876  5.155453  4.886012   0.073813  7.100619   \n",
       "109              13   9.709391  4.826661  5.155285  13.915672  4.762872   \n",
       "\n",
       "           d6        d7        d8        d9       d10  text_id language  \n",
       "100  6.667671  1.551378  6.853375  2.757002  4.997705       10       de  \n",
       "101  8.835148  2.635751  2.438843  4.611350  7.229488       10       de  \n",
       "102  4.917922  4.442468  4.858975  5.672881  2.111718       10       de  \n",
       "103  5.443470  0.693212  3.893485  7.622388  5.271125       10       de  \n",
       "104  6.608521  5.366010  5.223764  3.831670  2.555008       10       de  \n",
       "105  4.390954  9.180496  8.712993 -0.518369  6.244632       10       de  \n",
       "106  2.592971  5.305820  1.212458  1.705024  6.345337       10       de  \n",
       "107  2.783099  5.836319  0.453386  1.190403  5.976224       10       de  \n",
       "108  6.242529 -0.049043  2.893509  9.982448  4.248672       10       de  \n",
       "109  3.235626  2.735438  4.633454  7.020216  6.252982       10       de  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df_padded[dataset.df_padded[\"text_id\"]==10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "25bce380",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word                                    salbungsvolles\n",
       "language                                            de\n",
       "tokens      [s, a, l, b, u, n, g, s, v, o, l, l, e, s]\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_2.df.iloc[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0695c5ed",
   "metadata": {},
   "source": [
    "To obtain a path as a torch tensor, we use the `.get_torch_path()` method which by default keeps the time features and will remove the id and label columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "be6c23db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12000, 10, 11])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by default keeps the time features\n",
    "torch_word_path = dataset.get_torch_path()\n",
    "torch_word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e6364e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  9.3164,  4.8452,  4.3275, 12.5036,  6.3309,  9.6394,  5.7596,\n",
       "          5.4904,  8.5103,  4.9428],\n",
       "        [ 1.0000,  8.8879,  5.9607,  6.0489,  7.6250,  4.4201,  1.4468,  5.8707,\n",
       "          4.0632,  2.1491,  3.4525],\n",
       "        [ 2.0000,  9.5937,  5.0017,  4.8191,  0.7636,  8.6006,  6.8556,  0.5442,\n",
       "          2.7374,  8.7122,  4.9436],\n",
       "        [ 3.0000, 10.7384,  5.1161,  2.1005,  4.5319,  4.8176, -0.2478,  8.9974,\n",
       "          2.1034,  9.2459,  4.4463],\n",
       "        [ 4.0000,  9.9598,  4.6211,  7.9826,  2.0204,  7.1145,  8.2173,  5.9239,\n",
       "          9.3283,  4.3783,  5.2105],\n",
       "        [ 5.0000, 10.1000,  5.4727,  4.3113,  0.8878,  1.5299,  2.6565,  5.1096,\n",
       "          1.4469,  1.6553,  6.2591],\n",
       "        [ 6.0000,  8.9574,  4.8329,  7.3153, 11.5737,  6.2408,  5.4818,  0.4098,\n",
       "          3.6075,  7.7580,  5.5392],\n",
       "        [ 7.0000,  9.2132,  6.0912,  6.3231,  7.9394,  4.3163,  0.7840,  6.1789,\n",
       "          4.4194,  1.7614,  3.2507],\n",
       "        [ 8.0000,  9.1414,  5.3036,  4.4925,  1.2722,  8.9322,  6.5211,  0.4211,\n",
       "          2.2737,  8.7820,  5.8599],\n",
       "        [ 9.0000,  9.0475,  4.9001,  5.7462,  3.3931,  4.6592,  0.5551,  9.1757,\n",
       "          2.2582, 10.4953,  6.6896]], dtype=torch.float64)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_word_path[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe9d5ab",
   "metadata": {},
   "source": [
    "We can choose to ignore the time features by setting `include_time_features=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e4485ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12000, 10, 10])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ingore time features (ignore first column of timeline_index)\n",
    "torch_word_path = dataset.get_torch_path(include_time_features=False)\n",
    "torch_word_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3e26a55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.3164,  4.8452,  4.3275, 12.5036,  6.3309,  9.6394,  5.7596,  5.4904,\n",
       "          8.5103,  4.9428],\n",
       "        [ 8.8879,  5.9607,  6.0489,  7.6250,  4.4201,  1.4468,  5.8707,  4.0632,\n",
       "          2.1491,  3.4525],\n",
       "        [ 9.5937,  5.0017,  4.8191,  0.7636,  8.6006,  6.8556,  0.5442,  2.7374,\n",
       "          8.7122,  4.9436],\n",
       "        [10.7384,  5.1161,  2.1005,  4.5319,  4.8176, -0.2478,  8.9974,  2.1034,\n",
       "          9.2459,  4.4463],\n",
       "        [ 9.9598,  4.6211,  7.9826,  2.0204,  7.1145,  8.2173,  5.9239,  9.3283,\n",
       "          4.3783,  5.2105],\n",
       "        [10.1000,  5.4727,  4.3113,  0.8878,  1.5299,  2.6565,  5.1096,  1.4469,\n",
       "          1.6553,  6.2591],\n",
       "        [ 8.9574,  4.8329,  7.3153, 11.5737,  6.2408,  5.4818,  0.4098,  3.6075,\n",
       "          7.7580,  5.5392],\n",
       "        [ 9.2132,  6.0912,  6.3231,  7.9394,  4.3163,  0.7840,  6.1789,  4.4194,\n",
       "          1.7614,  3.2507],\n",
       "        [ 9.1414,  5.3036,  4.4925,  1.2722,  8.9322,  6.5211,  0.4211,  2.2737,\n",
       "          8.7820,  5.8599],\n",
       "        [ 9.0475,  4.9001,  5.7462,  3.3931,  4.6592,  0.5551,  9.1757,  2.2582,\n",
       "         10.4953,  6.6896]], dtype=torch.float64)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_word_path[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py38esig)",
   "language": "python",
   "name": "py38esig"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
