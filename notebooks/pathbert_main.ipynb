{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.path.insert(0, \"../../timeline_generation/\")  # Adds higher directory to python modules path\n",
    "import data_handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TalkLifeDataset = data_handler.TalkLifeDataset()\n",
    "annotations = TalkLifeDataset.return_annotated_timelines(load_from_pickle=False)\n",
    "annotations = annotations[annotations['content']!='nan']\n",
    "\n",
    "sample_size = annotations.shape[0]\n",
    "print(sample_size)\n",
    "annotations.head()\n",
    "#column format: \"timeline_id\"\t\"postid\"\t\"content\"\t\"label\"\t\"datetime\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "#model specifics\n",
    "model_specifics = {\"global_embedding_tp\": 'SBERT', #options: SBERT, BERT_cls , BERT_mean, BERT_max\n",
    "    \"dimensionality_reduction_tp\": 'ppapca', #options: ppapca, ppapcappa, umap\n",
    "    \"dimensionality_reduction_components\": 10, # options: any int number between 1 and embedding dimensions\n",
    "    \"time_injection_history_tp\": 'timestamp', #options: timestamp, None\n",
    "    \"time_injection_post_tp\": 'timestamp', #options: timestamp, timediff, None\n",
    "    \"signature_dimensions\": 3, #options: any int number larger than 1\n",
    "    \"post_embedding_tp\": 'sentence', #options: sentence, reduced\n",
    "    \"feature_combination_method\": 'attention', #options concatenation, attention \n",
    "    \"signature_tp\": 'log', # options: log, sig\n",
    "    \"classifier_name\": 'FFN2hidden', # options: FFN2hidden (any future classifiers added)\n",
    "    \"classes_num\": '3class', #options: 3class (5class to be added in the future)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post Embeddings, Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#post embedding\n",
    "from embeddings import Representations\n",
    "\n",
    "rep = Representations(type = model_specifics['global_embedding_tp'])\n",
    "embeddings_sentence = rep.get_embeddings()\n",
    "\n",
    "print(embeddings_sentence.shape)\n",
    "\n",
    "#dimensionality reduction\n",
    "from dimensionality_reduction import DimensionalityReduction\n",
    "\n",
    "reduction = DimensionalityReduction(method= model_specifics['dimensionality_reduction_tp'], components=model_specifics['dimensionality_reduction_components'])\n",
    "embeddings_reduced = reduction.fit_transform(embeddings_sentence)\n",
    "\n",
    "print(embeddings_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate new dataframe\n",
    "from dataset import get_modeling_dataframe\n",
    "df = get_modeling_dataframe(annotations, embeddings_sentence, embeddings_reduced)\n",
    "\n",
    "#get time features\n",
    "from timeinjection import TimeFeatures, Padding\n",
    "tf = TimeFeatures()\n",
    "df = tf.get_time_features(df)\n",
    "\n",
    "\n",
    "#padding\n",
    "pad = Padding()\n",
    "df_padded = pad.pad_timelines(df)\n",
    "df_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dyadic paths and data combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (model_specifics['time_injection_history_tp'] == 'timestamp'):\n",
    "    path = torch.from_numpy(df_padded[: , : , 2:].astype(float))\n",
    "else:\n",
    "    path = torch.from_numpy(df_padded[: , : , 3:].astype(float))\n",
    "  \n",
    "if (model_specifics['time_injection_post_tp']== 'timestamp'):\n",
    "    time_feature = torch.tensor((df[['time_encoding']].values - df['time_encoding'].mean()) / df['time_encoding'].std() )\n",
    "    post_time = True\n",
    "elif (model_specifics['time_injection_post_tp']== 'timediff'):\n",
    "    time_feature = torch.tensor( (df[['time_diff']].values - df['time_diff'].mean()) / df['time_diff'].std()  )\n",
    "    post_time = True  \n",
    "else: \n",
    "    time_feature = None\n",
    "    post_time = False\n",
    "\n",
    "if (model_specifics['post_embedding_tp'] == 'sentence'):\n",
    "    bert_embeddings = torch.tensor(df[[c for c in df.columns if re.match(\"^e\\w*[0-9]\", c)]].values)\n",
    "else:\n",
    "    bert_embeddings = None\n",
    "\n",
    "#calculate paths\n",
    "from dyadic_path import DyadicSignatures\n",
    "\n",
    "dsig = DyadicSignatures(original_size = df.shape[0], d = path.shape[2], sig_d = model_specifics['signature_dimensions'], \\\n",
    "    intervals = 1/12, k_history= None, embedding_tp = model_specifics['post_embedding_tp'], \\\n",
    "    method = model_specifics['feature_combination_method'], \\\n",
    "    history_tp = model_specifics['signature_tp'] , add_time = post_time)\n",
    "\n",
    "sig, last_index_dt_all = dsig.compute_signatures(path)\n",
    "sig_combined = dsig.combine_signatures(sig)\n",
    "x_data = dsig.create_features(path, sig_combined, last_index_dt_all, bert_embeddings, time_feature)\n",
    "\n",
    "sig.shape, last_index_dt_all.shape, sig_combined.shape, x_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K fold cross validation with random seeds in FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import random\n",
    "from datetime import date\n",
    "import math\n",
    "\n",
    "from classification_utils import Folds, set_seed, validation, training, testing\n",
    "from ffn import FeedforwardNeuralNetModel, FocalLoss\n",
    "\n",
    "# ================================\n",
    "save_results = False\n",
    "# ================================\n",
    "\n",
    "#GLOBAL MODEL PARAMETERS\n",
    "input_dim = x_data.shape[1]\n",
    "hidden_dim = 200 #200\n",
    "output_dim = 3\n",
    "dropout_rate = 0.35 #0.35 #higher dropout than 0.25 and specifically 0.35 is very promising\n",
    "num_epochs = 100\n",
    "learning_rate = 0.0003 #0.0003 #empirically optimal lr value: 0.0001\n",
    "gamma = 2 #3 #empirically optimal gamma value: 3\n",
    "BATCH_SIZE = 64\n",
    "NUM_folds = 5\n",
    "patience = 2\n",
    "weight_decay_adam = 0.0001\n",
    "RANDOM_SEED_list = [0, 1, 12, 123, 1234]\n",
    "\n",
    "classifier_params = {\"input_dim\": input_dim,\n",
    "  \"hidden_dim\": hidden_dim,\n",
    "  \"output_dim\": output_dim,\n",
    "  \"dropout_rate\": dropout_rate,\n",
    "  \"num_epochs\": num_epochs,\n",
    "  \"learning_rate\": learning_rate,\n",
    "  \"gamma\": gamma,\n",
    "  \"BATCH_SIZE\": BATCH_SIZE,\n",
    "  \"NUM_folds\": NUM_folds,\n",
    "  \"patience\": patience,\n",
    "  \"weight_decay_adam\": weight_decay_adam,\n",
    "  \"RANDOM_SEED_list\": RANDOM_SEED_list,\n",
    "}\n",
    "# ================================\n",
    "model_code_name = model_specifics[\"global_embedding_tp\"]  \\\n",
    "+ \"_\" + str(model_specifics['dimensionality_reduction_tp']) + str(model_specifics['dimensionality_reduction_components']) \\\n",
    "+ \"_\" + str(model_specifics['time_injection_history_tp']) + str(model_specifics['time_injection_post_tp']) \\\n",
    "+ \"_\" + str(model_specifics['post_embedding_tp']) + \"_\" + str(model_specifics['feature_combination_method']) \\\n",
    "+ \"_\" + str(model_specifics['signature_tp']) + \"_\" + str(model_specifics['signature_dimensions']) \\\n",
    "+ \"_\" + str(model_specifics['classifier_name']) + \"_\" + str(model_specifics['classes_num']) \n",
    "\n",
    "FOLDER_models = '/storage/ttseriotou/pathbert/models/v1/'\n",
    "FOLDER_results = '/storage/ttseriotou/pathbert/results/v1/'\n",
    "\n",
    "# ================================\n",
    "KFolds = Folds(num_folds=NUM_folds)\n",
    "y_data = KFolds.get_labels(df)\n",
    "# ================================\n",
    "#K FOLD RUNS\n",
    "\n",
    "for my_ran_seed in RANDOM_SEED_list:\n",
    "    set_seed(my_ran_seed)\n",
    "    myGenerator = torch.Generator()\n",
    "    myGenerator.manual_seed(my_ran_seed)    \n",
    "    for test_fold in range(NUM_folds):\n",
    "\n",
    "        print('Starting random seed #',my_ran_seed, ' and fold #', test_fold)\n",
    "        #get ith-fold data\n",
    "        x_test, y_test, x_valid, y_valid, x_train , y_train, test_tl_ids, test_pids = KFolds.get_splits(df, x_data, y_data, test_fold= test_fold)\n",
    "\n",
    "        #data loaders with batches\n",
    "        train = torch.utils.data.TensorDataset( x_train, y_train)\n",
    "        valid = torch.utils.data.TensorDataset( x_valid, y_valid)\n",
    "        test = torch.utils.data.TensorDataset( x_test, y_test)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train, batch_size = BATCH_SIZE, shuffle = True)\n",
    "        valid_loader = torch.utils.data.DataLoader(dataset=valid, batch_size = BATCH_SIZE, shuffle = True)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "        #calculate alpha for focal loss\n",
    "        alpha_values = torch.Tensor([math.sqrt(1/(y_train[y_train==0].shape[0]/y_train.shape[0])), math.sqrt(1/(y_train[y_train==1].shape[0]/y_train.shape[0])), math.sqrt(1/(y_train[y_train==2].shape[0]/y_train.shape[0]))])\n",
    "\n",
    "        #early stopping params\n",
    "        last_metric = 0\n",
    "        trigger_times = 0\n",
    "        best_metric = 0\n",
    "\n",
    "        #model definitions\n",
    "        model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim, dropout_rate)\n",
    "        criterion = FocalLoss(gamma = gamma, alpha = alpha_values)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay= weight_decay_adam)\n",
    "\n",
    "        #model train/validation per epoch\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            training(model, train_loader, criterion, optimizer, epoch, num_epochs)\n",
    "        \n",
    "            # Early stopping\n",
    "            \n",
    "            _ , f1_v = validation(model, valid_loader, criterion)\n",
    "            print('Current Macro F1:', f1_v)\n",
    "\n",
    "            if f1_v > best_metric :\n",
    "                best_metric = f1_v\n",
    "\n",
    "                #test and save so far best model\n",
    "                predicted_test, labels_test = testing(model, test_loader)\n",
    "\n",
    "                results = {\n",
    "                    \"model_code_name\": model_code_name, \n",
    "                    \"model_specifics\": model_specifics, \n",
    "                    \"classifier_params\": classifier_params, \n",
    "                    \"date_run\": date.today().strftime(\"%d/%m/%Y\"),\n",
    "                    \"test_tl_ids\": test_tl_ids,\n",
    "                    \"test_pids\": test_pids,\n",
    "                    \"labels\": labels_test,\n",
    "                    \"predictions\": predicted_test,\n",
    "                    \"test_fold\": test_fold,\n",
    "                    \"random_seed\": my_ran_seed,\n",
    "                    \"epoch\": epoch,\n",
    "                }\n",
    "\n",
    "                if (save_results==True):\n",
    "                    file_name_results = FOLDER_results + model_code_name + \"_\" + str(my_ran_seed) + \"seed\" + \"_\" + str(test_fold) + \"fold\" +'.pkl'\n",
    "                    file_name_model = FOLDER_models + model_code_name + \"_\" + str(my_ran_seed) + \"seed\" + \"_\" + str(test_fold) + \"fold\" +'.pkl'\n",
    "                    pickle.dump(results, open(file_name_results, 'wb'))\n",
    "                    torch.save(model.state_dict(), file_name_model)\n",
    "\n",
    "            if f1_v < last_metric:\n",
    "                trigger_times += 1\n",
    "                print('Trigger Times:', trigger_times)\n",
    "\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early stopping!')\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                print('Trigger Times: 0')\n",
    "                trigger_times = 0\n",
    "\n",
    "            last_metric = f1_v\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_utils import process_model_results\n",
    "process_model_results(model_code_name, FOLDER_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38-MoC",
   "language": "python",
   "name": "py38-moc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
